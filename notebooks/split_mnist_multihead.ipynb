{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Environment Setup](#setup)\n",
    "* [Results](#results)\n",
    "    * [S-FSVI](#res1)\n",
    "    * [S-FSVI (larger networks)](#res2)\n",
    "    * [S-FSVI (no coreset)](#res3)\n",
    "    * [VCL (random-choice coreset)](#res4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run as Colab notebook\n",
    "\n",
    "**Important: Before connecting to a kernel, select a GPU runtime. To do so, open the `Runtime` tab above, click `Change runtime type`, and select `GPU`. Run the setup cell below only after you've done this.**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pull S-FSVI repository\n",
    "!git clone https://github.com/timrudner/S-FSVI.git\n",
    "# patch required packages\n",
    "!pip install -r ./S-FSVI/colab_requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**After successfully running the cell above, you need to restart the runtime. To do so, open the “Runtime” tab above and and click “Restart runtime”. Once the runtime was restarted, run the cell below. There is no need to re-run the installation in the cell above.**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add the repo to path\n",
    "import os\n",
    "import sys\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"S-FSVI\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run as Jupyter notebook (-->skip ahead to “Results” if you are running this as a Colab notebook<--)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install conda environment `fsvi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env update -f ../environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting:\n",
    "\n",
    " - In case there is an error when installing sklearn: run `pip install Cython==0.29.23` manually and then run the above command again.\n",
    " - In case you have access to a GPU, see instructions [here](https://github.com/google/jax#pip-installation-gpu-cuda) for installing the GPU version of `jaxlib`. This will make the experiment run significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command below to install the conda environment as a kernel of the jupyter notebook. Then switch to this kernel using the Jupyter Notebook menu bar by selecting `Kernel`, `Change kernel`, and then selecting `fsvi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ipykernel install --user --name=fsvi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting: For further details, see [here](https://medium.com/@nrk25693/how-to-add-your-conda-environment-to-your-jupyter-notebook-in-just-4-steps-abeab8b8d084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# assuming os.getcwd() returns the directory containing this jupyter notebook\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results <a name=\"results\"></a>\n",
    "\n",
    "To read a model checkpoint instead of training the model from scratch, pass load_chkpt=True to the function read_config_and_run .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import sfsvi.exps.utils.load_utils as lutils\n",
    "from notebooks.nb_utils.common import read_config_and_run, show_final_average_accuracy\n",
    "\n",
    "task_sequence = \"smnist_mh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-FSVI (ours) <a name=\"res1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 1123.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on oat3.cs.ox.ac.uk\n",
      "Jax is running on gpu\n",
      "\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"command\":\"cl\",\n",
      "    \"data_training\":\"continual_learning_smnist\",\n",
      "    \"data_ood\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"model_type\":\"fsvi_mlp\",\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":[\n",
      "        256,\n",
      "        256\n",
      "    ],\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0.0\",\n",
      "    \"prior_cov\":\"0.001\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":60,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0.0,\n",
      "    \"inducing_points\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"uniform_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"equal\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":0.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":5,\n",
      "    \"tau\":1.0,\n",
      "    \"noise_std\":1.0,\n",
      "    \"ind_lim\":\"ind_-1_1\",\n",
      "    \"logging_frequency\":10,\n",
      "    \"figsize\":[\n",
      "        \"10\",\n",
      "        \"4\"\n",
      "    ],\n",
      "    \"seed\":9,\n",
      "    \"save\":false,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"not_specified\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"logroot\":\"ablation\",\n",
      "    \"subdir\":\"reproduce_main_results_2\",\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"n_inducing_inputs\":40,\n",
      "    \"n_inducing_inputs_first_task\":\"not_specified\",\n",
      "    \"n_inducing_inputs_second_task\":\"not_specified\",\n",
      "    \"inducing_inputs_bound\":[\n",
      "        0.0,\n",
      "        1.0\n",
      "    ],\n",
      "    \"fix_shuffle\":false,\n",
      "    \"use_generative_model\":false,\n",
      "    \"inducing_inputs_add_mode\":0,\n",
      "    \"inducing_input_adjustment\":false,\n",
      "    \"not_use_coreset\":false,\n",
      "    \"inducing_input_augmentation\":false,\n",
      "    \"plotting\":false,\n",
      "    \"logging\":1,\n",
      "    \"use_val_split\":false,\n",
      "    \"not_use_val_split\":true,\n",
      "    \"coreset\":\"random\",\n",
      "    \"coreset_entropy_mode\":\"soft_highest\",\n",
      "    \"coreset_entropy_offset\":\"0.0\",\n",
      "    \"coreset_kl_heuristic\":\"lowest\",\n",
      "    \"coreset_kl_offset\":\"0.0\",\n",
      "    \"coreset_elbo_heuristic\":\"lowest\",\n",
      "    \"coreset_elbo_offset\":\"0.0\",\n",
      "    \"coreset_elbo_n_samples\":5,\n",
      "    \"coreset_n_tasks\":\"not_specified\",\n",
      "    \"coreset_entropy_n_mixed\":1,\n",
      "    \"n_coreset_inputs_per_task\":\"not_specified\",\n",
      "    \"full_ntk\":false,\n",
      "    \"constant_inducing_points\":false,\n",
      "    \"n_permuted_tasks\":10,\n",
      "    \"n_omniglot_tasks\":20,\n",
      "    \"epochs_first_task\":60,\n",
      "    \"identity_cov\":false,\n",
      "    \"n_epochs_save_params\":\"not_specified\",\n",
      "    \"n_augment\":\"not_specified\",\n",
      "    \"augment_mode\":\"constant\",\n",
      "    \"n_valid\":\"same\",\n",
      "    \"learning_rate_first_task\":\"not_specified\",\n",
      "    \"save_alt\":true,\n",
      "    \"save_first_task\":false,\n",
      "    \"first_task_load_exp_path\":\"not_specified\",\n",
      "    \"only_task_id\":\"not_specified\",\n",
      "    \"loss_type\":1,\n",
      "    \"only_trainable_head\":false,\n",
      "    \"n_omniglot_coreset_chars\":2,\n",
      "    \"omniglot_randomize_test_split\":false,\n",
      "    \"omniglot_randomize_task_sequence\":false,\n",
      "    \"n_inducing_input_adjust_amount\":\"not_specified\",\n",
      "    \"save_all_params\":false,\n",
      "    \"task\":\"continual_learning_smnist\",\n",
      "    \"architecture_arg\":\"fc_256_256\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (posterior): False\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "\n",
      "Learning task 1\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------\n",
      "  epoch    mean acc    t1 acc\n",
      "-------  ----------  --------\n",
      "      0       99.86     99.86\n",
      "      1       99.95     99.95\n",
      "      2       99.95     99.95\n",
      "      3       99.95     99.95\n",
      "      4       99.95     99.95\n",
      "      5       99.95     99.95\n",
      "      6       99.95     99.95\n",
      "      7       99.95     99.95\n",
      "      8       99.95     99.95\n",
      "      9       99.95     99.95\n",
      "     10       99.95     99.95\n",
      "     11       99.95     99.95\n",
      "     12       99.95     99.95\n",
      "     13       99.95     99.95\n",
      "     14       99.95     99.95\n",
      "     15       99.95     99.95\n",
      "     16       99.95     99.95\n",
      "     17       99.95     99.95\n",
      "     18       99.95     99.95\n",
      "     19       99.95     99.95\n",
      "     20       99.95     99.95\n",
      "     21       99.95     99.95\n",
      "     22       99.95     99.95\n",
      "     23       99.95     99.95\n",
      "     24       99.95     99.95\n",
      "     25       99.95     99.95\n",
      "     26       99.95     99.95\n",
      "     27       99.95     99.95\n",
      "     28       99.95     99.95\n",
      "     29       99.95     99.95\n",
      "     30       99.95     99.95\n",
      "     31       99.95     99.95\n",
      "     32       99.95     99.95\n",
      "     33       99.95     99.95\n",
      "     34       99.95     99.95\n",
      "     35       99.95     99.95\n",
      "     36       99.95     99.95\n",
      "     37       99.95     99.95\n",
      "     38      100.00    100.00\n",
      "     39      100.00    100.00\n",
      "     40      100.00    100.00\n",
      "     41       99.95     99.95\n",
      "     42       99.95     99.95\n",
      "     43      100.00    100.00\n",
      "     44      100.00    100.00\n",
      "     45      100.00    100.00\n",
      "     46      100.00    100.00\n",
      "     47      100.00    100.00\n",
      "     48      100.00    100.00\n",
      "     49      100.00    100.00\n",
      "     50       99.95     99.95\n",
      "     51       99.95     99.95\n",
      "     52       99.95     99.95\n",
      "     53       99.95     99.95\n",
      "     54       99.95     99.95\n",
      "     55       99.95     99.95\n",
      "     56       99.95     99.95\n",
      "     57       99.95     99.95\n",
      "     58       99.95     99.95\n",
      "     59       99.95     99.95\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9995 \n",
      "Accuracies (test): [0.9995]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 2\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc\n",
      "-------  ----------  --------  --------\n",
      "      0       99.05    100.00     98.09\n",
      "      1       99.41    100.00     98.82\n",
      "      2       99.64    100.00     99.27\n",
      "      3       99.73     99.95     99.51\n",
      "      4       99.75    100.00     99.51\n",
      "      5       99.70     99.95     99.46\n",
      "      6       99.70     99.95     99.46\n",
      "      7       99.68     99.95     99.41\n",
      "      8       99.66    100.00     99.31\n",
      "      9       99.66     99.95     99.36\n",
      "     10       99.70     99.95     99.46\n",
      "     11       99.86     99.95     99.76\n",
      "     12       99.78     99.95     99.61\n",
      "     13       99.78     99.95     99.61\n",
      "     14       99.78     99.95     99.61\n",
      "     15       99.81     99.95     99.66\n",
      "     16       99.81     99.95     99.66\n",
      "     17       99.81     99.95     99.66\n",
      "     18       99.83     99.95     99.71\n",
      "     19       99.83     99.95     99.71\n",
      "     20       99.81     99.95     99.66\n",
      "     21       99.81     99.95     99.66\n",
      "     22       99.81     99.95     99.66\n",
      "     23       99.83     99.95     99.71\n",
      "     24       99.81     99.95     99.66\n",
      "     25       99.81     99.95     99.66\n",
      "     26       99.78     99.95     99.61\n",
      "     27       99.81     99.95     99.66\n",
      "     28       99.78     99.95     99.61\n",
      "     29       99.83     99.95     99.71\n",
      "     30       99.83     99.95     99.71\n",
      "     31       99.81     99.95     99.66\n",
      "     32       99.78     99.95     99.61\n",
      "     33       99.81     99.95     99.66\n",
      "     34       99.83     99.95     99.71\n",
      "     35       99.78     99.95     99.61\n",
      "     36       99.81     99.95     99.66\n",
      "     37       99.83     99.95     99.71\n",
      "     38       99.86     99.95     99.76\n",
      "     39       99.86     99.95     99.76\n",
      "     40       99.86     99.95     99.76\n",
      "     41       99.81     99.95     99.66\n",
      "     42       99.81     99.95     99.66\n",
      "     43       99.81     99.95     99.66\n",
      "     44       99.78     99.95     99.61\n",
      "     45       99.83     99.95     99.71\n",
      "     46       99.83     99.95     99.71\n",
      "     47       99.83     99.95     99.71\n",
      "     48       99.86     99.95     99.76\n",
      "     49       99.83     99.95     99.71\n",
      "     50       99.81     99.95     99.66\n",
      "     51       99.83     99.95     99.71\n",
      "     52       99.86     99.95     99.76\n",
      "     53       99.86     99.95     99.76\n",
      "     54       99.86     99.95     99.76\n",
      "     55       99.86     99.95     99.76\n",
      "     56       99.81     99.95     99.66\n",
      "     57       99.83     99.95     99.71\n",
      "     58       99.86     99.95     99.76\n",
      "     59       99.86     99.95     99.76\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9986 \n",
      "Accuracies (test): [0.9995, 0.9976]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 3\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc\n",
      "-------  ----------  --------  --------  --------\n",
      "      0       99.55     99.95     99.17     99.52\n",
      "      1       99.69     99.95     99.22     99.89\n",
      "      2       99.75     99.95     99.36     99.95\n",
      "      3       99.72     99.95     99.27     99.95\n",
      "      4       99.75     99.95     99.36     99.95\n",
      "      5       99.75     99.95     99.41     99.89\n",
      "      6       99.73     99.95     99.36     99.89\n",
      "      7       99.72     99.95     99.27     99.95\n",
      "      8       99.71     99.95     99.22     99.95\n",
      "      9       99.72     99.95     99.27     99.95\n",
      "     10       99.74     99.95     99.31     99.95\n",
      "     11       99.69     99.95     99.22     99.89\n",
      "     12       99.72     99.95     99.27     99.95\n",
      "     13       99.65     99.95     99.12     99.89\n",
      "     14       99.72     99.95     99.27     99.95\n",
      "     15       99.65     99.95     99.12     99.89\n",
      "     16       99.69     99.95     99.17     99.95\n",
      "     17       99.64     99.91     99.12     99.89\n",
      "     18       99.69     99.95     99.22     99.89\n",
      "     19       99.72     99.95     99.27     99.95\n",
      "     20       99.71     99.95     99.22     99.95\n",
      "     21       99.66     99.95     99.07     99.95\n",
      "     22       99.69     99.95     99.17     99.95\n",
      "     23       99.73     99.95     99.36     99.89\n",
      "     24       99.64     99.95     99.07     99.89\n",
      "     25       99.67     99.95     99.17     99.89\n",
      "     26       99.69     99.95     99.22     99.89\n",
      "     27       99.69     99.95     99.17     99.95\n",
      "     28       99.66     99.95     99.07     99.95\n",
      "     29       99.72     99.95     99.27     99.95\n",
      "     30       99.71     99.95     99.22     99.95\n",
      "     31       99.70     99.95     99.27     99.89\n",
      "     32       99.69     99.95     99.22     99.89\n",
      "     33       99.71     99.95     99.22     99.95\n",
      "     34       99.71     99.95     99.22     99.95\n",
      "     35       99.65     99.95     99.12     99.89\n",
      "     36       99.67     99.95     99.17     99.89\n",
      "     37       99.69     99.95     99.17     99.95\n",
      "     38       99.72     99.95     99.27     99.95\n",
      "     39       99.65     99.95     99.12     99.89\n",
      "     40       99.67     99.95     99.12     99.95\n",
      "     41       99.67     99.95     99.17     99.89\n",
      "     42       99.65     99.95     99.12     99.89\n",
      "     43       99.72     99.95     99.27     99.95\n",
      "     44       99.77     99.95     99.41     99.95\n",
      "     45       99.75     99.95     99.36     99.95\n",
      "     46       99.67     99.95     99.12     99.95\n",
      "     47       99.72     99.95     99.31     99.89\n",
      "     48       99.67     99.95     99.12     99.95\n",
      "     49       99.77     99.95     99.41     99.95\n",
      "     50       99.64     99.95     99.07     99.89\n",
      "     51       99.71     99.95     99.22     99.95\n",
      "     52       99.65     99.95     99.12     99.89\n",
      "     53       99.72     99.95     99.31     99.89\n",
      "     54       99.69     99.95     99.17     99.95\n",
      "     55       99.72     99.95     99.31     99.89\n",
      "     56       99.70     99.95     99.27     99.89\n",
      "     57       99.75     99.95     99.41     99.89\n",
      "     58       99.70     99.95     99.27     99.89\n",
      "     59       99.70     99.95     99.27     99.89\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9970 \n",
      "Accuracies (test): [0.9995, 0.9927, 0.9989]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 4\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "      0       99.54     99.91     98.92    100.00     99.35\n",
      "      1       99.64     99.91     98.92    100.00     99.75\n",
      "      2       99.66     99.91     98.92    100.00     99.80\n",
      "      3       99.65     99.91     98.87    100.00     99.80\n",
      "      4       99.65     99.91     98.87    100.00     99.80\n",
      "      5       99.66     99.91     98.87    100.00     99.85\n",
      "      6       99.66     99.91     98.87    100.00     99.85\n",
      "      7       99.67     99.91     98.92    100.00     99.85\n",
      "      8       99.64     99.91     98.82    100.00     99.85\n",
      "      9       99.68     99.91     98.97    100.00     99.85\n",
      "     10       99.67     99.91     98.87    100.00     99.90\n",
      "     11       99.67     99.91     98.87    100.00     99.90\n",
      "     12       99.66     99.91     98.82    100.00     99.90\n",
      "     13       99.66     99.91     98.87    100.00     99.85\n",
      "     14       99.66     99.91     98.92     99.95     99.85\n",
      "     15       99.67     99.91     98.97     99.95     99.85\n",
      "     16       99.67     99.91     98.92     99.95     99.90\n",
      "     17       99.67     99.91     98.97     99.95     99.85\n",
      "     18       99.69     99.91     98.97    100.00     99.90\n",
      "     19       99.64     99.91     98.82    100.00     99.85\n",
      "     20       99.68     99.91     98.92    100.00     99.90\n",
      "     21       99.61     99.91     98.63    100.00     99.90\n",
      "     22       99.65     99.91     98.78    100.00     99.90\n",
      "     23       99.66     99.91     98.87     99.95     99.90\n",
      "     24       99.68     99.91     98.92    100.00     99.90\n",
      "     25       99.61     99.91     98.63    100.00     99.90\n",
      "     26       99.68     99.91     98.92    100.00     99.90\n",
      "     27       99.67     99.91     98.92     99.95     99.90\n",
      "     28       99.62     99.91     98.73     99.95     99.90\n",
      "     29       99.67     99.91     98.87    100.00     99.90\n",
      "     30       99.65     99.91     98.78    100.00     99.90\n",
      "     31       99.66     99.91     98.87     99.95     99.90\n",
      "     32       99.68     99.91     98.92    100.00     99.90\n",
      "     33       99.70     99.91     99.02     99.95     99.90\n",
      "     34       99.67     99.91     98.92     99.95     99.90\n",
      "     35       99.66     99.91     98.92     99.95     99.85\n",
      "     36       99.66     99.91     98.92     99.95     99.85\n",
      "     37       99.65     99.91     98.87     99.95     99.85\n",
      "     38       99.68     99.91     98.97    100.00     99.85\n",
      "     39       99.67     99.91     98.92     99.95     99.90\n",
      "     40       99.64     99.91     98.82    100.00     99.85\n",
      "     41       99.63     99.91     98.82     99.95     99.85\n",
      "     42       99.67     99.91     98.97     99.95     99.85\n",
      "     43       99.69     99.91     99.07     99.95     99.85\n",
      "     44       99.67     99.91     98.92     99.95     99.90\n",
      "     45       99.66     99.91     98.92     99.95     99.85\n",
      "     46       99.68     99.91     98.97    100.00     99.85\n",
      "     47       99.63     99.91     98.82     99.95     99.85\n",
      "     48       99.66     99.91     98.87     99.95     99.90\n",
      "     49       99.64     99.91     98.82    100.00     99.85\n",
      "     50       99.66     99.91     98.87     99.95     99.90\n",
      "     51       99.67     99.91     98.92     99.95     99.90\n",
      "     52       99.65     99.91     98.78    100.00     99.90\n",
      "     53       99.67     99.91     98.92     99.95     99.90\n",
      "     54       99.69     99.91     98.97    100.00     99.90\n",
      "     55       99.66     99.91     98.92     99.89     99.90\n",
      "     56       99.67     99.91     98.92     99.95     99.90\n",
      "     57       99.68     99.91     98.97     99.95     99.90\n",
      "     58       99.66     99.91     98.87     99.95     99.90\n",
      "     59       99.64     99.91     98.82     99.95     99.90\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9964 \n",
      "Accuracies (test): [0.9991, 0.9882, 0.9995, 0.999]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 5\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "      0       99.34     99.86     98.82     99.95     99.85     98.23\n",
      "      1       99.44     99.86     98.97     99.89     99.85     98.64\n",
      "      2       99.50     99.86     98.92     99.89     99.80     99.04\n",
      "      3       99.54     99.86     99.07     99.89     99.80     99.09\n",
      "      4       99.52     99.81     98.82     99.89     99.85     99.24\n",
      "      5       99.58     99.81     98.97     99.95     99.80     99.39\n",
      "      6       99.60     99.86     98.97     99.89     99.85     99.45\n",
      "      7       99.61     99.86     98.92     99.89     99.80     99.60\n",
      "      8       99.58     99.86     98.92     99.89     99.80     99.45\n",
      "      9       99.60     99.81     98.87     99.89     99.90     99.55\n",
      "     10       99.63     99.86     98.97     99.89     99.80     99.65\n",
      "     11       99.66     99.86     98.92     99.95     99.80     99.75\n",
      "     12       99.70     99.86     99.12     99.95     99.90     99.65\n",
      "     13       99.65     99.86     98.92     99.95     99.85     99.65\n",
      "     14       99.65     99.86     98.92     99.89     99.90     99.70\n",
      "     15       99.63     99.86     98.87     99.89     99.85     99.70\n",
      "     16       99.63     99.81     98.97     99.84     99.90     99.65\n",
      "     17       99.64     99.86     98.97     99.89     99.85     99.65\n",
      "     18       99.65     99.86     98.87     99.95     99.80     99.75\n",
      "     19       99.68     99.86     98.97     99.95     99.90     99.70\n",
      "     20       99.64     99.86     98.87     99.89     99.90     99.70\n",
      "     21       99.65     99.86     98.92     99.89     99.90     99.70\n",
      "     22       99.63     99.86     98.82     99.95     99.80     99.70\n",
      "     23       99.62     99.86     98.92     99.84     99.80     99.70\n",
      "     24       99.67     99.86     98.92     99.95     99.90     99.70\n",
      "     25       99.68     99.86     99.02     99.89     99.90     99.75\n",
      "     26       99.65     99.86     98.97     99.89     99.90     99.65\n",
      "     27       99.60     99.76     98.87     99.84     99.85     99.70\n",
      "     28       99.69     99.86     99.07     99.95     99.80     99.75\n",
      "     29       99.65     99.86     98.87     99.89     99.90     99.75\n",
      "     30       99.61     99.86     98.92     99.89     99.80     99.60\n",
      "     31       99.65     99.81     99.02     99.89     99.85     99.70\n",
      "     32       99.68     99.86     99.02     99.95     99.90     99.65\n",
      "     33       99.64     99.76     98.92     99.89     99.90     99.75\n",
      "     34       99.70     99.86     99.02     99.95     99.90     99.75\n",
      "     35       99.62     99.81     98.87     99.89     99.80     99.75\n",
      "     36       99.60     99.81     98.87     99.84     99.80     99.70\n",
      "     37       99.64     99.81     98.97     99.89     99.85     99.70\n",
      "     38       99.65     99.86     98.82     99.95     99.90     99.70\n",
      "     39       99.65     99.81     98.87     99.95     99.85     99.75\n",
      "     40       99.64     99.86     98.97     99.79     99.90     99.70\n",
      "     41       99.68     99.86     98.97     99.95     99.90     99.70\n",
      "     42       99.67     99.86     99.02     99.95     99.80     99.70\n",
      "     43       99.66     99.86     99.02     99.84     99.90     99.70\n",
      "     44       99.62     99.86     98.92     99.84     99.90     99.60\n",
      "     45       99.60     99.86     98.87     99.79     99.80     99.70\n",
      "     46       99.68     99.86     99.02     99.95     99.85     99.70\n",
      "     47       99.63     99.86     98.97     99.84     99.80     99.70\n",
      "     48       99.64     99.86     98.87     99.89     99.90     99.70\n",
      "     49       99.58     99.86     98.87     99.73     99.90     99.55\n",
      "     50       99.66     99.86     98.92     99.89     99.90     99.75\n",
      "     51       99.58     99.76     98.97     99.84     99.85     99.50\n",
      "     52       99.65     99.86     98.92     99.89     99.85     99.75\n",
      "     53       99.65     99.81     98.87     99.95     99.90     99.70\n",
      "     54       99.65     99.86     98.92     99.89     99.85     99.75\n",
      "     55       99.62     99.81     98.97     99.84     99.80     99.70\n",
      "     56       99.61     99.81     98.92     99.89     99.80     99.65\n",
      "     57       99.66     99.86     99.02     99.84     99.85     99.75\n",
      "     58       99.62     99.81     98.87     99.89     99.90     99.65\n",
      "     59       99.62     99.86     98.97     99.79     99.80     99.70\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9962 \n",
      "Accuracies (test): [0.9986, 0.9897, 0.9979, 0.998, 0.997]\n",
      "---\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n",
      "\n",
      "0.99624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fsvi_match.pkl\", task_sequence)\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-FSVI (larger networks) <a name=\"res2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 1159.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on oat2.cs.ox.ac.uk\n",
      "Jax is running on gpu\n",
      "\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"command\":\"cl\",\n",
      "    \"data_training\":\"continual_learning_smnist\",\n",
      "    \"data_ood\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"model_type\":\"fsvi_mlp\",\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":[\n",
      "        400\n",
      "    ],\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0.0\",\n",
      "    \"prior_cov\":\"0.001\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":60,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0.0,\n",
      "    \"inducing_points\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"uniform_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"equal\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":0.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":5,\n",
      "    \"tau\":1.0,\n",
      "    \"noise_std\":1.0,\n",
      "    \"ind_lim\":\"ind_-1_1\",\n",
      "    \"logging_frequency\":10,\n",
      "    \"figsize\":[\n",
      "        \"10\",\n",
      "        \"4\"\n",
      "    ],\n",
      "    \"seed\":5,\n",
      "    \"save\":false,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"not_specified\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"logroot\":\"ablation\",\n",
      "    \"subdir\":\"reproduce_main_results_2\",\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"n_inducing_inputs\":40,\n",
      "    \"n_inducing_inputs_first_task\":\"not_specified\",\n",
      "    \"n_inducing_inputs_second_task\":\"not_specified\",\n",
      "    \"inducing_inputs_bound\":[\n",
      "        0.0,\n",
      "        1.0\n",
      "    ],\n",
      "    \"fix_shuffle\":false,\n",
      "    \"use_generative_model\":false,\n",
      "    \"inducing_inputs_add_mode\":0,\n",
      "    \"inducing_input_adjustment\":false,\n",
      "    \"not_use_coreset\":false,\n",
      "    \"inducing_input_augmentation\":false,\n",
      "    \"plotting\":false,\n",
      "    \"logging\":1,\n",
      "    \"use_val_split\":false,\n",
      "    \"not_use_val_split\":true,\n",
      "    \"coreset\":\"random\",\n",
      "    \"coreset_entropy_mode\":\"soft_highest\",\n",
      "    \"coreset_entropy_offset\":\"0.0\",\n",
      "    \"coreset_kl_heuristic\":\"lowest\",\n",
      "    \"coreset_kl_offset\":\"0.0\",\n",
      "    \"coreset_elbo_heuristic\":\"lowest\",\n",
      "    \"coreset_elbo_offset\":\"0.0\",\n",
      "    \"coreset_elbo_n_samples\":5,\n",
      "    \"coreset_n_tasks\":\"not_specified\",\n",
      "    \"coreset_entropy_n_mixed\":1,\n",
      "    \"n_coreset_inputs_per_task\":\"not_specified\",\n",
      "    \"full_ntk\":false,\n",
      "    \"constant_inducing_points\":false,\n",
      "    \"n_permuted_tasks\":10,\n",
      "    \"n_omniglot_tasks\":20,\n",
      "    \"epochs_first_task\":60,\n",
      "    \"identity_cov\":false,\n",
      "    \"n_epochs_save_params\":\"not_specified\",\n",
      "    \"n_augment\":\"not_specified\",\n",
      "    \"augment_mode\":\"constant\",\n",
      "    \"n_valid\":\"same\",\n",
      "    \"learning_rate_first_task\":\"not_specified\",\n",
      "    \"save_alt\":true,\n",
      "    \"save_first_task\":false,\n",
      "    \"first_task_load_exp_path\":\"not_specified\",\n",
      "    \"only_task_id\":\"not_specified\",\n",
      "    \"loss_type\":1,\n",
      "    \"only_trainable_head\":false,\n",
      "    \"n_omniglot_coreset_chars\":2,\n",
      "    \"omniglot_randomize_test_split\":false,\n",
      "    \"omniglot_randomize_task_sequence\":false,\n",
      "    \"n_inducing_input_adjust_amount\":\"not_specified\",\n",
      "    \"save_all_params\":false,\n",
      "    \"task\":\"continual_learning_smnist\",\n",
      "    \"architecture_arg\":\"fc_400\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (posterior): False\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "\n",
      "Learning task 1\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------\n",
      "  epoch    mean acc    t1 acc\n",
      "-------  ----------  --------\n",
      "      0       99.53     99.53\n",
      "      1       99.86     99.86\n",
      "      2       99.95     99.95\n",
      "      3       99.95     99.95\n",
      "      4       99.95     99.95\n",
      "      5       99.95     99.95\n",
      "      6       99.95     99.95\n",
      "      7       99.95     99.95\n",
      "      8       99.95     99.95\n",
      "      9       99.95     99.95\n",
      "     10       99.95     99.95\n",
      "     11       99.95     99.95\n",
      "     12       99.95     99.95\n",
      "     13       99.95     99.95\n",
      "     14       99.95     99.95\n",
      "     15       99.95     99.95\n",
      "     16       99.95     99.95\n",
      "     17       99.91     99.91\n",
      "     18       99.91     99.91\n",
      "     19       99.91     99.91\n",
      "     20       99.91     99.91\n",
      "     21       99.91     99.91\n",
      "     22       99.91     99.91\n",
      "     23       99.91     99.91\n",
      "     24       99.91     99.91\n",
      "     25       99.91     99.91\n",
      "     26       99.91     99.91\n",
      "     27       99.91     99.91\n",
      "     28       99.91     99.91\n",
      "     29       99.91     99.91\n",
      "     30       99.91     99.91\n",
      "     31       99.91     99.91\n",
      "     32       99.91     99.91\n",
      "     33       99.91     99.91\n",
      "     34       99.95     99.95\n",
      "     35       99.95     99.95\n",
      "     36       99.95     99.95\n",
      "     37       99.95     99.95\n",
      "     38       99.95     99.95\n",
      "     39       99.95     99.95\n",
      "     40       99.95     99.95\n",
      "     41       99.95     99.95\n",
      "     42       99.95     99.95\n",
      "     43       99.95     99.95\n",
      "     44       99.95     99.95\n",
      "     45       99.95     99.95\n",
      "     46       99.95     99.95\n",
      "     47       99.95     99.95\n",
      "     48       99.95     99.95\n",
      "     49       99.95     99.95\n",
      "     50       99.95     99.95\n",
      "     51       99.95     99.95\n",
      "     52       99.95     99.95\n",
      "     53       99.95     99.95\n",
      "     54       99.95     99.95\n",
      "     55       99.95     99.95\n",
      "     56       99.95     99.95\n",
      "     57       99.95     99.95\n",
      "     58       99.95     99.95\n",
      "     59       99.95     99.95\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9995 \n",
      "Accuracies (test): [0.9995]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 2\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc\n",
      "-------  ----------  --------  --------\n",
      "      0       98.85     99.95     97.75\n",
      "      1       99.34     99.95     98.73\n",
      "      2       99.53     99.95     99.12\n",
      "      3       99.59     99.95     99.22\n",
      "      4       99.61     99.95     99.27\n",
      "      5       99.66     99.95     99.36\n",
      "      6       99.70     99.95     99.46\n",
      "      7       99.76     99.95     99.56\n",
      "      8       99.73     99.95     99.51\n",
      "      9       99.70     99.95     99.46\n",
      "     10       99.76     99.95     99.56\n",
      "     11       99.83     99.95     99.71\n",
      "     12       99.78     99.95     99.61\n",
      "     13       99.78     99.95     99.61\n",
      "     14       99.81     99.91     99.71\n",
      "     15       99.81     99.95     99.66\n",
      "     16       99.81     99.95     99.66\n",
      "     17       99.81     99.95     99.66\n",
      "     18       99.83    100.00     99.66\n",
      "     19       99.83     99.95     99.71\n",
      "     20       99.83     99.95     99.71\n",
      "     21       99.76     99.91     99.61\n",
      "     22       99.81     99.95     99.66\n",
      "     23       99.78     99.95     99.61\n",
      "     24       99.76     99.91     99.61\n",
      "     25       99.80    100.00     99.61\n",
      "     26       99.76     99.95     99.56\n",
      "     27       99.73     99.91     99.56\n",
      "     28       99.76     99.95     99.56\n",
      "     29       99.73     99.91     99.56\n",
      "     30       99.78     99.95     99.61\n",
      "     31       99.76     99.95     99.56\n",
      "     32       99.86    100.00     99.71\n",
      "     33       99.76     99.91     99.61\n",
      "     34       99.80    100.00     99.61\n",
      "     35       99.81     99.95     99.66\n",
      "     36       99.78     99.91     99.66\n",
      "     37       99.83     99.95     99.71\n",
      "     38       99.81     99.95     99.66\n",
      "     39       99.78     99.95     99.61\n",
      "     40       99.83    100.00     99.66\n",
      "     41       99.83    100.00     99.66\n",
      "     42       99.80    100.00     99.61\n",
      "     43       99.83    100.00     99.66\n",
      "     44       99.81     99.95     99.66\n",
      "     45       99.88    100.00     99.76\n",
      "     46       99.81     99.95     99.66\n",
      "     47       99.86     99.95     99.76\n",
      "     48       99.88     99.95     99.80\n",
      "     49       99.81     99.95     99.66\n",
      "     50       99.86     99.95     99.76\n",
      "     51       99.83     99.95     99.71\n",
      "     52       99.83     99.95     99.71\n",
      "     53       99.83     99.95     99.71\n",
      "     54       99.83     99.95     99.71\n",
      "     55       99.83     99.95     99.71\n",
      "     56       99.81     99.95     99.66\n",
      "     57       99.81     99.95     99.66\n",
      "     58       99.83     99.95     99.71\n",
      "     59       99.81     99.95     99.66\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9981 \n",
      "Accuracies (test): [0.9995, 0.9966]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 3\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc\n",
      "-------  ----------  --------  --------  --------\n",
      "      0       99.73     99.95     99.71     99.52\n",
      "      1       99.83     99.95     99.71     99.84\n",
      "      2       99.85     99.95     99.71     99.89\n",
      "      3       99.85     99.95     99.71     99.89\n",
      "      4       99.87     99.95     99.76     99.89\n",
      "      5       99.85     99.95     99.71     99.89\n",
      "      6       99.85     99.95     99.71     99.89\n",
      "      7       99.87     99.95     99.76     99.89\n",
      "      8       99.87     99.95     99.76     99.89\n",
      "      9       99.85     99.95     99.71     99.89\n",
      "     10       99.85     99.91     99.76     99.89\n",
      "     11       99.87     99.95     99.76     99.89\n",
      "     12       99.87     99.95     99.76     99.89\n",
      "     13       99.85     99.95     99.76     99.84\n",
      "     14       99.83     99.95     99.71     99.84\n",
      "     15       99.87     99.95     99.76     99.89\n",
      "     16       99.85     99.95     99.71     99.89\n",
      "     17       99.87     99.95     99.76     99.89\n",
      "     18       99.85     99.95     99.71     99.89\n",
      "     19       99.87     99.95     99.76     99.89\n",
      "     20       99.85     99.95     99.71     99.89\n",
      "     21       99.84     99.91     99.71     99.89\n",
      "     22       99.85     99.95     99.71     99.89\n",
      "     23       99.83     99.95     99.71     99.84\n",
      "     24       99.85     99.95     99.71     99.89\n",
      "     25       99.87     99.95     99.76     99.89\n",
      "     26       99.87     99.95     99.76     99.89\n",
      "     27       99.85     99.95     99.71     99.89\n",
      "     28       99.87     99.95     99.76     99.89\n",
      "     29       99.87     99.95     99.76     99.89\n",
      "     30       99.87     99.95     99.76     99.89\n",
      "     31       99.85     99.95     99.71     99.89\n",
      "     32       99.85     99.95     99.71     99.89\n",
      "     33       99.83     99.95     99.71     99.84\n",
      "     34       99.84     99.91     99.71     99.89\n",
      "     35       99.85     99.95     99.71     99.89\n",
      "     36       99.83     99.95     99.71     99.84\n",
      "     37       99.87     99.95     99.76     99.89\n",
      "     38       99.83     99.95     99.71     99.84\n",
      "     39       99.85     99.95     99.71     99.89\n",
      "     40       99.85     99.95     99.71     99.89\n",
      "     41       99.83     99.95     99.71     99.84\n",
      "     42       99.85     99.95     99.71     99.89\n",
      "     43       99.85     99.95     99.71     99.89\n",
      "     44       99.85     99.95     99.71     99.89\n",
      "     45       99.85     99.95     99.71     99.89\n",
      "     46       99.83     99.95     99.71     99.84\n",
      "     47       99.85     99.95     99.71     99.89\n",
      "     48       99.83     99.95     99.71     99.84\n",
      "     49       99.83     99.95     99.71     99.84\n",
      "     50       99.85     99.95     99.71     99.89\n",
      "     51       99.83     99.95     99.71     99.84\n",
      "     52       99.83     99.95     99.71     99.84\n",
      "     53       99.83     99.95     99.71     99.84\n",
      "     54       99.85     99.95     99.76     99.84\n",
      "     55       99.83     99.95     99.71     99.84\n",
      "     56       99.82     99.91     99.71     99.84\n",
      "     57       99.87     99.95     99.76     99.89\n",
      "     58       99.85     99.95     99.76     99.84\n",
      "     59       99.83     99.95     99.71     99.84\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9983 \n",
      "Accuracies (test): [0.9995, 0.9971, 0.9984]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 4\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "      0       99.80     99.95     99.76     99.84     99.65\n",
      "      1       99.85     99.95     99.80     99.84     99.80\n",
      "      2       99.86     99.95     99.76     99.84     99.90\n",
      "      3       99.85     99.91     99.76     99.84     99.90\n",
      "      4       99.87     99.95     99.80     99.84     99.90\n",
      "      5       99.86     99.95     99.76     99.84     99.90\n",
      "      6       99.87     99.95     99.80     99.84     99.90\n",
      "      7       99.85     99.91     99.76     99.84     99.90\n",
      "      8       99.86     99.91     99.80     99.84     99.90\n",
      "      9       99.86     99.91     99.80     99.84     99.90\n",
      "     10       99.85     99.91     99.76     99.84     99.90\n",
      "     11       99.87     99.95     99.80     99.84     99.90\n",
      "     12       99.85     99.91     99.76     99.84     99.90\n",
      "     13       99.85     99.95     99.71     99.84     99.90\n",
      "     14       99.86     99.95     99.76     99.84     99.90\n",
      "     15       99.85     99.91     99.76     99.84     99.90\n",
      "     16       99.85     99.91     99.76     99.84     99.90\n",
      "     17       99.86     99.95     99.76     99.84     99.90\n",
      "     18       99.87     99.95     99.80     99.84     99.90\n",
      "     19       99.85     99.95     99.71     99.84     99.90\n",
      "     20       99.86     99.95     99.76     99.84     99.90\n",
      "     21       99.86     99.95     99.76     99.84     99.90\n",
      "     22       99.85     99.95     99.71     99.84     99.90\n",
      "     23       99.86     99.91     99.80     99.84     99.90\n",
      "     24       99.86     99.95     99.76     99.84     99.90\n",
      "     25       99.85     99.95     99.71     99.84     99.90\n",
      "     26       99.85     99.91     99.76     99.84     99.90\n",
      "     27       99.86     99.95     99.76     99.84     99.90\n",
      "     28       99.85     99.91     99.76     99.84     99.90\n",
      "     29       99.85     99.91     99.76     99.84     99.90\n",
      "     30       99.84     99.91     99.71     99.84     99.90\n",
      "     31       99.87     99.95     99.80     99.84     99.90\n",
      "     32       99.86     99.95     99.76     99.84     99.90\n",
      "     33       99.87     99.95     99.80     99.84     99.90\n",
      "     34       99.86     99.95     99.76     99.84     99.90\n",
      "     35       99.85     99.91     99.76     99.84     99.90\n",
      "     36       99.85     99.91     99.76     99.84     99.90\n",
      "     37       99.85     99.91     99.76     99.84     99.90\n",
      "     38       99.86     99.95     99.76     99.84     99.90\n",
      "     39       99.86     99.95     99.76     99.84     99.90\n",
      "     40       99.86     99.91     99.80     99.84     99.90\n",
      "     41       99.86     99.95     99.76     99.84     99.90\n",
      "     42       99.86     99.95     99.76     99.84     99.90\n",
      "     43       99.85     99.91     99.76     99.84     99.90\n",
      "     44       99.86     99.95     99.76     99.84     99.90\n",
      "     45       99.86     99.95     99.76     99.84     99.90\n",
      "     46       99.85     99.91     99.76     99.84     99.90\n",
      "     47       99.86     99.91     99.80     99.84     99.90\n",
      "     48       99.86     99.91     99.80     99.84     99.90\n",
      "     49       99.85     99.91     99.76     99.84     99.90\n",
      "     50       99.84     99.91     99.71     99.84     99.90\n",
      "     51       99.86     99.91     99.80     99.84     99.90\n",
      "     52       99.86     99.95     99.76     99.84     99.90\n",
      "     53       99.86     99.95     99.76     99.84     99.90\n",
      "     54       99.86     99.95     99.76     99.84     99.90\n",
      "     55       99.86     99.95     99.76     99.84     99.90\n",
      "     56       99.86     99.95     99.76     99.84     99.90\n",
      "     57       99.86     99.95     99.76     99.84     99.90\n",
      "     58       99.86     99.95     99.76     99.84     99.90\n",
      "     59       99.87     99.95     99.80     99.84     99.90\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9987 \n",
      "Accuracies (test): [0.9995, 0.998, 0.9984, 0.999]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 5\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "      0       99.50     99.91     99.80     99.84     99.90     98.03\n",
      "      1       99.60     99.91     99.66     99.84     99.90     98.69\n",
      "      2       99.61     99.91     99.66     99.84     99.90     98.74\n",
      "      3       99.65     99.91     99.61     99.84     99.90     98.99\n",
      "      4       99.69     99.91     99.61     99.84     99.90     99.19\n",
      "      5       99.72     99.91     99.61     99.84     99.90     99.34\n",
      "      6       99.73     99.91     99.61     99.84     99.90     99.39\n",
      "      7       99.76     99.91     99.66     99.84     99.90     99.50\n",
      "      8       99.75     99.91     99.61     99.84     99.90     99.50\n",
      "      9       99.74     99.91     99.61     99.84     99.90     99.45\n",
      "     10       99.76     99.91     99.56     99.84     99.90     99.60\n",
      "     11       99.79     99.91     99.61     99.84     99.90     99.70\n",
      "     12       99.79     99.91     99.61     99.84     99.90     99.70\n",
      "     13       99.78     99.91     99.61     99.84     99.90     99.65\n",
      "     14       99.80     99.91     99.71     99.84     99.90     99.65\n",
      "     15       99.78     99.91     99.56     99.84     99.90     99.70\n",
      "     16       99.78     99.91     99.61     99.84     99.90     99.65\n",
      "     17       99.77     99.91     99.56     99.84     99.90     99.65\n",
      "     18       99.78     99.91     99.56     99.84     99.90     99.70\n",
      "     19       99.78     99.91     99.51     99.84     99.90     99.75\n",
      "     20       99.78     99.91     99.51     99.84     99.90     99.75\n",
      "     21       99.78     99.91     99.56     99.84     99.90     99.70\n",
      "     22       99.81     99.91     99.66     99.84     99.90     99.75\n",
      "     23       99.79     99.91     99.61     99.84     99.90     99.70\n",
      "     24       99.77     99.91     99.56     99.84     99.90     99.65\n",
      "     25       99.79     99.91     99.61     99.84     99.90     99.70\n",
      "     26       99.78     99.91     99.56     99.84     99.90     99.70\n",
      "     27       99.79     99.91     99.61     99.84     99.90     99.70\n",
      "     28       99.80     99.91     99.66     99.84     99.90     99.70\n",
      "     29       99.80     99.91     99.61     99.84     99.90     99.75\n",
      "     30       99.78     99.91     99.56     99.84     99.90     99.70\n",
      "     31       99.80     99.91     99.61     99.84     99.90     99.75\n",
      "     32       99.80     99.91     99.61     99.84     99.90     99.75\n",
      "     33       99.79     99.91     99.56     99.84     99.90     99.75\n",
      "     34       99.78     99.91     99.51     99.84     99.90     99.75\n",
      "     35       99.75     99.91     99.51     99.84     99.85     99.65\n",
      "     36       99.79     99.91     99.61     99.84     99.90     99.70\n",
      "     37       99.77     99.91     99.56     99.84     99.90     99.65\n",
      "     38       99.79     99.91     99.56     99.84     99.90     99.75\n",
      "     39       99.80     99.91     99.61     99.84     99.90     99.75\n",
      "     40       99.76     99.91     99.56     99.84     99.90     99.60\n",
      "     41       99.77     99.91     99.51     99.84     99.90     99.70\n",
      "     42       99.79     99.91     99.66     99.84     99.90     99.65\n",
      "     43       99.78     99.91     99.56     99.84     99.85     99.75\n",
      "     44       99.80     99.91     99.61     99.84     99.90     99.75\n",
      "     45       99.78     99.91     99.61     99.84     99.90     99.65\n",
      "     46       99.78     99.91     99.51     99.84     99.90     99.75\n",
      "     47       99.80     99.91     99.71     99.84     99.90     99.65\n",
      "     48       99.78     99.91     99.51     99.84     99.90     99.75\n",
      "     49       99.78     99.91     99.51     99.84     99.90     99.75\n",
      "     50       99.76     99.91     99.46     99.84     99.90     99.70\n",
      "     51       99.76     99.91     99.51     99.84     99.90     99.65\n",
      "     52       99.77     99.91     99.56     99.84     99.85     99.70\n",
      "     53       99.79     99.91     99.61     99.84     99.90     99.70\n",
      "     54       99.75     99.91     99.51     99.84     99.90     99.60\n",
      "     55       99.78     99.91     99.61     99.84     99.85     99.70\n",
      "     56       99.77     99.91     99.51     99.84     99.90     99.70\n",
      "     57       99.77     99.91     99.51     99.84     99.90     99.70\n",
      "     58       99.75     99.91     99.46     99.84     99.90     99.65\n",
      "     59       99.80     99.91     99.61     99.84     99.90     99.75\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9980 \n",
      "Accuracies (test): [0.9991, 0.9961, 0.9984, 0.999, 0.9975]\n",
      "---\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n",
      "\n",
      "0.99802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fsvi_optimized.pkl\", task_sequence)\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-FSVI (no coreset) <a name=\"res3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 1155.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on oat3.cs.ox.ac.uk\n",
      "Jax is running on gpu\n",
      "\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"command\":\"cl\",\n",
      "    \"data_training\":\"continual_learning_smnist\",\n",
      "    \"data_ood\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"model_type\":\"fsvi_mlp\",\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":[\n",
      "        256,\n",
      "        256\n",
      "    ],\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0.0\",\n",
      "    \"prior_cov\":\"100.0\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":20,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0.0,\n",
      "    \"inducing_points\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"train_pixel_rand_1.0\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"equal\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":0.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":5,\n",
      "    \"tau\":1.0,\n",
      "    \"noise_std\":1.0,\n",
      "    \"ind_lim\":\"ind_-1_1\",\n",
      "    \"logging_frequency\":10,\n",
      "    \"figsize\":[\n",
      "        \"10\",\n",
      "        \"4\"\n",
      "    ],\n",
      "    \"seed\":6,\n",
      "    \"save\":false,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"not_specified\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"logroot\":\"ablation\",\n",
      "    \"subdir\":\"reproduce_main_results_2\",\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"n_inducing_inputs\":40,\n",
      "    \"n_inducing_inputs_first_task\":\"not_specified\",\n",
      "    \"n_inducing_inputs_second_task\":\"not_specified\",\n",
      "    \"inducing_inputs_bound\":[\n",
      "        0.0,\n",
      "        1.0\n",
      "    ],\n",
      "    \"fix_shuffle\":false,\n",
      "    \"use_generative_model\":false,\n",
      "    \"inducing_inputs_add_mode\":0,\n",
      "    \"inducing_input_adjustment\":false,\n",
      "    \"not_use_coreset\":true,\n",
      "    \"inducing_input_augmentation\":false,\n",
      "    \"plotting\":false,\n",
      "    \"logging\":1,\n",
      "    \"use_val_split\":false,\n",
      "    \"not_use_val_split\":true,\n",
      "    \"coreset\":\"random\",\n",
      "    \"coreset_entropy_mode\":\"soft_highest\",\n",
      "    \"coreset_entropy_offset\":\"0.0\",\n",
      "    \"coreset_kl_heuristic\":\"lowest\",\n",
      "    \"coreset_kl_offset\":\"0.0\",\n",
      "    \"coreset_elbo_heuristic\":\"lowest\",\n",
      "    \"coreset_elbo_offset\":\"0.0\",\n",
      "    \"coreset_elbo_n_samples\":5,\n",
      "    \"coreset_n_tasks\":\"not_specified\",\n",
      "    \"coreset_entropy_n_mixed\":1,\n",
      "    \"n_coreset_inputs_per_task\":\"not_specified\",\n",
      "    \"full_ntk\":false,\n",
      "    \"constant_inducing_points\":true,\n",
      "    \"n_permuted_tasks\":10,\n",
      "    \"n_omniglot_tasks\":20,\n",
      "    \"epochs_first_task\":20,\n",
      "    \"identity_cov\":false,\n",
      "    \"n_epochs_save_params\":\"not_specified\",\n",
      "    \"n_augment\":\"not_specified\",\n",
      "    \"augment_mode\":\"constant\",\n",
      "    \"n_valid\":\"same\",\n",
      "    \"learning_rate_first_task\":\"not_specified\",\n",
      "    \"save_alt\":true,\n",
      "    \"save_first_task\":false,\n",
      "    \"first_task_load_exp_path\":\"not_specified\",\n",
      "    \"only_task_id\":\"not_specified\",\n",
      "    \"loss_type\":1,\n",
      "    \"only_trainable_head\":false,\n",
      "    \"n_omniglot_coreset_chars\":2,\n",
      "    \"omniglot_randomize_test_split\":false,\n",
      "    \"omniglot_randomize_task_sequence\":false,\n",
      "    \"n_inducing_input_adjust_amount\":\"not_specified\",\n",
      "    \"save_all_params\":false,\n",
      "    \"task\":\"continual_learning_smnist\",\n",
      "    \"architecture_arg\":\"fc_256_256\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (posterior): False\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "\n",
      "Learning task 1\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------\n",
      "  epoch    mean acc    t1 acc\n",
      "-------  ----------  --------\n",
      "      0       99.48     99.48\n",
      "      1       99.95     99.95\n",
      "      2       99.86     99.86\n",
      "      3       99.81     99.81\n",
      "      4       99.39     99.39\n",
      "      5       98.96     98.96\n",
      "      6       99.91     99.91\n",
      "      7       99.86     99.86\n",
      "      8       99.95     99.95\n",
      "      9       99.81     99.81\n",
      "     10       99.91     99.91\n",
      "     11       99.95     99.95\n",
      "     12       99.95     99.95\n",
      "     13       99.86     99.86\n",
      "     14       99.76     99.76\n",
      "     15       99.95     99.95\n",
      "     16       99.86     99.86\n",
      "     17       99.95     99.95\n",
      "     18       99.81     99.81\n",
      "     19       99.72     99.72\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9972 \n",
      "Accuracies (test): [0.9972]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 2\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc\n",
      "-------  ----------  --------  --------\n",
      "      0       96.89    100.00     93.78\n",
      "      1       98.39    100.00     96.77\n",
      "      2       98.72     99.95     97.50\n",
      "      3       98.97     99.95     97.99\n",
      "      4       99.19     99.95     98.43\n",
      "      5       99.17     99.95     98.38\n",
      "      6       99.25     99.91     98.58\n",
      "      7       99.31     99.95     98.68\n",
      "      8       99.46     99.95     98.97\n",
      "      9       99.34     99.95     98.73\n",
      "     10       99.00     99.91     98.09\n",
      "     11       99.59     99.95     99.22\n",
      "     12       99.54     99.91     99.17\n",
      "     13       99.63     99.95     99.31\n",
      "     14       99.52     99.86     99.17\n",
      "     15       99.59     99.95     99.22\n",
      "     16       99.70    100.00     99.41\n",
      "     17       99.37     99.86     98.87\n",
      "     18       99.71     99.91     99.51\n",
      "     19       99.73     99.95     99.51\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9973 \n",
      "Accuracies (test): [0.9995, 0.9951]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 3\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc\n",
      "-------  ----------  --------  --------  --------\n",
      "      0       98.85     99.95     99.22     97.39\n",
      "      1       99.61     99.91     99.56     99.36\n",
      "      2       99.47     99.91     99.41     99.09\n",
      "      3       99.70    100.00     99.36     99.73\n",
      "      4       99.70     99.95     99.46     99.68\n",
      "      5       99.65     99.91     99.41     99.63\n",
      "      6       99.57     99.86     99.17     99.68\n",
      "      7       99.67     99.86     99.36     99.79\n",
      "      8       99.34     99.95     98.33     99.73\n",
      "      9       99.70     99.91     99.41     99.79\n",
      "     10       99.70     99.86     99.46     99.79\n",
      "     11       99.55     99.91     99.12     99.63\n",
      "     12       99.71     99.95     99.51     99.68\n",
      "     13       99.69     99.72     99.51     99.84\n",
      "     14       99.63     99.86     99.41     99.63\n",
      "     15       99.75     99.91     99.51     99.84\n",
      "     16       99.44     99.91     98.58     99.84\n",
      "     17       99.45     99.95     98.68     99.73\n",
      "     18       99.75     99.95     99.56     99.73\n",
      "     19       99.77    100.00     99.41     99.89\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9977 \n",
      "Accuracies (test): [1.0, 0.9941, 0.9989]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 4\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "      0       99.47     99.86     99.36     99.36     99.30\n",
      "      1       99.59     99.95     99.51     99.79     99.09\n",
      "      2       99.59     99.95     99.27     99.79     99.35\n",
      "      3       99.58     99.91     98.97     99.79     99.65\n",
      "      4       99.62    100.00     99.12     99.68     99.70\n",
      "      5       99.71     99.95     99.56     99.68     99.65\n",
      "      6       99.75     99.95     99.41     99.79     99.85\n",
      "      7       99.74     99.91     99.31     99.89     99.85\n",
      "      8       99.77    100.00     99.56     99.73     99.80\n",
      "      9       99.72     99.72     99.61     99.79     99.75\n",
      "     10       99.78     99.91     99.46     99.89     99.85\n",
      "     11       99.79     99.95     99.56     99.79     99.85\n",
      "     12       99.80     99.95     99.56     99.73     99.95\n",
      "     13       99.80     99.95     99.61     99.68     99.95\n",
      "     14       99.80     99.91     99.51     99.84     99.95\n",
      "     15       99.73     99.95     99.51     99.52     99.95\n",
      "     16       99.69     99.95     99.22     99.73     99.85\n",
      "     17       99.69     99.86     99.27     99.68     99.95\n",
      "     18       99.70     99.95     99.22     99.79     99.85\n",
      "     19       99.80    100.00     99.51     99.73     99.95\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9980 \n",
      "Accuracies (test): [1.0, 0.9951, 0.9973, 0.9995]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 5\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "      0       99.20     99.95     99.12     99.79     99.55     97.58\n",
      "      1       99.38    100.00     99.51     99.47     99.90     98.03\n",
      "      2       99.36     99.91     99.46     99.63     99.75     98.03\n",
      "      3       99.36     99.95     98.92     99.73     99.80     98.39\n",
      "      4       99.56     99.91     99.51     99.89     99.85     98.64\n",
      "      5       99.54     99.95     99.41     99.79     99.90     98.64\n",
      "      6       99.59     99.91     99.56     99.79     99.90     98.79\n",
      "      7       99.61    100.00     99.31     99.89     99.90     98.94\n",
      "      8       99.63     99.95     99.41     99.84     99.90     99.04\n",
      "      9       99.50     99.95     99.02     99.63     99.90     98.99\n",
      "     10       99.41     99.53     99.22     99.63     99.75     98.94\n",
      "     11       99.62    100.00     99.27     99.68     99.80     99.34\n",
      "     12       99.65     99.95     99.51     99.63     99.80     99.34\n",
      "     13       99.64     99.86     99.36     99.68     99.80     99.50\n",
      "     14       99.57     99.72     99.22     99.63     99.85     99.45\n",
      "     15       99.51     99.91     98.82     99.73     99.90     99.19\n",
      "     16       99.65     99.95     99.36     99.84     99.80     99.29\n",
      "     17       99.54     99.91     98.97     99.63     99.85     99.34\n",
      "     18       99.59     99.91     99.02     99.73     99.85     99.45\n",
      "     19       99.52     99.95     99.22     99.52     99.90     98.99\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9952 \n",
      "Accuracies (test): [0.9995, 0.9922, 0.9952, 0.999, 0.9899]\n",
      "---\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n",
      "\n",
      "0.9951599999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fsvi_no_coreset.pkl\", task_sequence)\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VCL (random-choice coreset) <a name=\"res4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 5023.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on oat1.cs.ox.ac.uk\n",
      "Running with: python /auto/users/timner/qixuan/function-space-variational-inference/fsvi_cl/baselines/vcl/run_vcl.py --dataset smnist --n_epochs 100 --batch_size not_specified --hidden_size 256 --n_layers 2 --seed 5 --select_method random_choice --n_permuted_tasks 10 --logroot ablation --subdir reproduce_main_results_3 --n_coreset_inputs_per_task 40\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('Epoch:', '0001', 'cost=', '2.529914856')\n",
      "('Epoch:', '0006', 'cost=', '0.145845681')\n",
      "('Epoch:', '0011', 'cost=', '0.023763664')\n",
      "('Epoch:', '0016', 'cost=', '0.012767947')\n",
      "('Epoch:', '0021', 'cost=', '0.007185564')\n",
      "('Epoch:', '0026', 'cost=', '0.006027640')\n",
      "('Epoch:', '0031', 'cost=', '0.004565736')\n",
      "('Epoch:', '0036', 'cost=', '0.003384513')\n",
      "('Epoch:', '0041', 'cost=', '0.002716686')\n",
      "('Epoch:', '0046', 'cost=', '0.002201111')\n",
      "('Epoch:', '0051', 'cost=', '0.001794693')\n",
      "('Epoch:', '0056', 'cost=', '0.001563322')\n",
      "('Epoch:', '0061', 'cost=', '0.001345129')\n",
      "('Epoch:', '0066', 'cost=', '0.001173626')\n",
      "('Epoch:', '0071', 'cost=', '0.001033420')\n",
      "('Epoch:', '0076', 'cost=', '0.000899654')\n",
      "('Epoch:', '0081', 'cost=', '0.000785245')\n",
      "('Epoch:', '0086', 'cost=', '0.000681824')\n",
      "('Epoch:', '0091', 'cost=', '0.000589146')\n",
      "('Epoch:', '0096', 'cost=', '0.000504430')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '53.443759918')\n",
      "('Epoch:', '0006', 'cost=', '53.386100769')\n",
      "('Epoch:', '0011', 'cost=', '53.327640533')\n",
      "('Epoch:', '0016', 'cost=', '53.269569397')\n",
      "('Epoch:', '0021', 'cost=', '53.212066650')\n",
      "('Epoch:', '0026', 'cost=', '53.154918671')\n",
      "('Epoch:', '0031', 'cost=', '53.098121643')\n",
      "('Epoch:', '0036', 'cost=', '53.041217804')\n",
      "('Epoch:', '0041', 'cost=', '52.984592438')\n",
      "('Epoch:', '0046', 'cost=', '52.928501129')\n",
      "('Epoch:', '0051', 'cost=', '52.872444153')\n",
      "('Epoch:', '0056', 'cost=', '52.816612244')\n",
      "('Epoch:', '0061', 'cost=', '52.760875702')\n",
      "('Epoch:', '0066', 'cost=', '52.705486298')\n",
      "('Epoch:', '0071', 'cost=', '52.650192261')\n",
      "('Epoch:', '0076', 'cost=', '52.595092773')\n",
      "('Epoch:', '0081', 'cost=', '52.540122986')\n",
      "('Epoch:', '0086', 'cost=', '52.485290527')\n",
      "('Epoch:', '0091', 'cost=', '52.430820465')\n",
      "('Epoch:', '0096', 'cost=', '52.376075745')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '160.947631836')\n",
      "('Epoch:', '0006', 'cost=', '160.932495117')\n",
      "('Epoch:', '0011', 'cost=', '160.653579712')\n",
      "('Epoch:', '0016', 'cost=', '160.442047119')\n",
      "('Epoch:', '0021', 'cost=', '160.252151489')\n",
      "('Epoch:', '0026', 'cost=', '160.067642212')\n",
      "('Epoch:', '0031', 'cost=', '159.886764526')\n",
      "('Epoch:', '0036', 'cost=', '159.710281372')\n",
      "('Epoch:', '0041', 'cost=', '159.536392212')\n",
      "('Epoch:', '0046', 'cost=', '159.364227295')\n",
      "('Epoch:', '0051', 'cost=', '159.193786621')\n",
      "('Epoch:', '0056', 'cost=', '159.024490356')\n",
      "('Epoch:', '0061', 'cost=', '158.856262207')\n",
      "('Epoch:', '0066', 'cost=', '158.688674927')\n",
      "('Epoch:', '0071', 'cost=', '158.522064209')\n",
      "('Epoch:', '0076', 'cost=', '158.356079102')\n",
      "('Epoch:', '0081', 'cost=', '158.190551758')\n",
      "('Epoch:', '0086', 'cost=', '158.025619507')\n",
      "('Epoch:', '0091', 'cost=', '157.860931396')\n",
      "('Epoch:', '0096', 'cost=', '157.696868896')\n",
      "Optimization Finished!\n",
      "Accuracy at task 0: [0.9995271867612293]\n",
      "('Epoch:', '0001', 'cost=', '4.039278984')\n",
      "('Epoch:', '0006', 'cost=', '1.225511432')\n",
      "('Epoch:', '0011', 'cost=', '0.888721347')\n",
      "('Epoch:', '0016', 'cost=', '0.853913963')\n",
      "('Epoch:', '0021', 'cost=', '0.878460884')\n",
      "('Epoch:', '0026', 'cost=', '0.807475388')\n",
      "('Epoch:', '0031', 'cost=', '0.795953929')\n",
      "('Epoch:', '0036', 'cost=', '0.785671294')\n",
      "('Epoch:', '0041', 'cost=', '0.787793100')\n",
      "('Epoch:', '0046', 'cost=', '0.749795675')\n",
      "('Epoch:', '0051', 'cost=', '0.740599573')\n",
      "('Epoch:', '0056', 'cost=', '0.741541624')\n",
      "('Epoch:', '0061', 'cost=', '0.727434754')\n",
      "('Epoch:', '0066', 'cost=', '0.738184035')\n",
      "('Epoch:', '0071', 'cost=', '0.714919269')\n",
      "('Epoch:', '0076', 'cost=', '0.702434063')\n",
      "('Epoch:', '0081', 'cost=', '0.709636092')\n",
      "('Epoch:', '0086', 'cost=', '0.708732843')\n",
      "('Epoch:', '0091', 'cost=', '0.707388997')\n",
      "('Epoch:', '0096', 'cost=', '0.720806062')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '160.946380615')\n",
      "('Epoch:', '0006', 'cost=', '160.935668945')\n",
      "('Epoch:', '0011', 'cost=', '160.671051025')\n",
      "('Epoch:', '0016', 'cost=', '160.459884644')\n",
      "('Epoch:', '0021', 'cost=', '160.259552002')\n",
      "('Epoch:', '0026', 'cost=', '160.067886353')\n",
      "('Epoch:', '0031', 'cost=', '159.884765625')\n",
      "('Epoch:', '0036', 'cost=', '159.707565308')\n",
      "('Epoch:', '0041', 'cost=', '159.534210205')\n",
      "('Epoch:', '0046', 'cost=', '159.362213135')\n",
      "('Epoch:', '0051', 'cost=', '159.191726685')\n",
      "('Epoch:', '0056', 'cost=', '159.022399902')\n",
      "('Epoch:', '0061', 'cost=', '158.853775024')\n",
      "('Epoch:', '0066', 'cost=', '158.686645508')\n",
      "('Epoch:', '0071', 'cost=', '158.519989014')\n",
      "('Epoch:', '0076', 'cost=', '158.353866577')\n",
      "('Epoch:', '0081', 'cost=', '158.188430786')\n",
      "('Epoch:', '0086', 'cost=', '158.023590088')\n",
      "('Epoch:', '0091', 'cost=', '157.859069824')\n",
      "('Epoch:', '0096', 'cost=', '157.694992065')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '161.032089233')\n",
      "('Epoch:', '0006', 'cost=', '161.191329956')\n",
      "('Epoch:', '0011', 'cost=', '160.868301392')\n",
      "('Epoch:', '0016', 'cost=', '160.576690674')\n",
      "('Epoch:', '0021', 'cost=', '160.370727539')\n",
      "('Epoch:', '0026', 'cost=', '160.181549072')\n",
      "('Epoch:', '0031', 'cost=', '159.968994141')\n",
      "('Epoch:', '0036', 'cost=', '159.784454346')\n",
      "('Epoch:', '0041', 'cost=', '159.626663208')\n",
      "('Epoch:', '0046', 'cost=', '159.469421387')\n",
      "('Epoch:', '0051', 'cost=', '159.298507690')\n",
      "('Epoch:', '0056', 'cost=', '159.156311035')\n",
      "('Epoch:', '0061', 'cost=', '158.956039429')\n",
      "('Epoch:', '0066', 'cost=', '158.800827026')\n",
      "('Epoch:', '0071', 'cost=', '158.609024048')\n",
      "('Epoch:', '0076', 'cost=', '158.470504761')\n",
      "('Epoch:', '0081', 'cost=', '158.292877197')\n",
      "('Epoch:', '0086', 'cost=', '158.103317261')\n",
      "('Epoch:', '0091', 'cost=', '157.945205688')\n",
      "('Epoch:', '0096', 'cost=', '157.788192749')\n",
      "Optimization Finished!\n",
      "Accuracy at task 1: [0.9995271867612293, 0.9764936336924583]\n",
      "('Epoch:', '0001', 'cost=', '3.047828197')\n",
      "('Epoch:', '0006', 'cost=', '0.969359279')\n",
      "('Epoch:', '0011', 'cost=', '0.830238402')\n",
      "('Epoch:', '0016', 'cost=', '0.846189618')\n",
      "('Epoch:', '0021', 'cost=', '0.780759037')\n",
      "('Epoch:', '0026', 'cost=', '0.782975674')\n",
      "('Epoch:', '0031', 'cost=', '0.761332035')\n",
      "('Epoch:', '0036', 'cost=', '0.763055503')\n",
      "('Epoch:', '0041', 'cost=', '0.737112999')\n",
      "('Epoch:', '0046', 'cost=', '0.722155809')\n",
      "('Epoch:', '0051', 'cost=', '0.704289854')\n",
      "('Epoch:', '0056', 'cost=', '0.703847289')\n",
      "('Epoch:', '0061', 'cost=', '0.693237007')\n",
      "('Epoch:', '0066', 'cost=', '0.698464811')\n",
      "('Epoch:', '0071', 'cost=', '0.683710098')\n",
      "('Epoch:', '0076', 'cost=', '0.685621619')\n",
      "('Epoch:', '0081', 'cost=', '0.676908314')\n",
      "('Epoch:', '0086', 'cost=', '0.689052582')\n",
      "('Epoch:', '0091', 'cost=', '0.680220544')\n",
      "('Epoch:', '0096', 'cost=', '0.679293156')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '160.947326660')\n",
      "('Epoch:', '0006', 'cost=', '160.988632202')\n",
      "('Epoch:', '0011', 'cost=', '160.713516235')\n",
      "('Epoch:', '0016', 'cost=', '160.485671997')\n",
      "('Epoch:', '0021', 'cost=', '160.269699097')\n",
      "('Epoch:', '0026', 'cost=', '160.068862915')\n",
      "('Epoch:', '0031', 'cost=', '159.884109497')\n",
      "('Epoch:', '0036', 'cost=', '159.707748413')\n",
      "('Epoch:', '0041', 'cost=', '159.535079956')\n",
      "('Epoch:', '0046', 'cost=', '159.363433838')\n",
      "('Epoch:', '0051', 'cost=', '159.192474365')\n",
      "('Epoch:', '0056', 'cost=', '159.023330688')\n",
      "('Epoch:', '0061', 'cost=', '158.854583740')\n",
      "('Epoch:', '0066', 'cost=', '158.688110352')\n",
      "('Epoch:', '0071', 'cost=', '158.520889282')\n",
      "('Epoch:', '0076', 'cost=', '158.354690552')\n",
      "('Epoch:', '0081', 'cost=', '158.189697266')\n",
      "('Epoch:', '0086', 'cost=', '158.025299072')\n",
      "('Epoch:', '0091', 'cost=', '157.861892700')\n",
      "('Epoch:', '0096', 'cost=', '157.696029663')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '161.090194702')\n",
      "('Epoch:', '0006', 'cost=', '161.245986938')\n",
      "('Epoch:', '0011', 'cost=', '160.928146362')\n",
      "('Epoch:', '0016', 'cost=', '160.640319824')\n",
      "('Epoch:', '0021', 'cost=', '160.397644043')\n",
      "('Epoch:', '0026', 'cost=', '160.183227539')\n",
      "('Epoch:', '0031', 'cost=', '159.986648560')\n",
      "('Epoch:', '0036', 'cost=', '159.829483032')\n",
      "('Epoch:', '0041', 'cost=', '159.645965576')\n",
      "('Epoch:', '0046', 'cost=', '159.465347290')\n",
      "('Epoch:', '0051', 'cost=', '159.319946289')\n",
      "('Epoch:', '0056', 'cost=', '159.132507324')\n",
      "('Epoch:', '0061', 'cost=', '158.976028442')\n",
      "('Epoch:', '0066', 'cost=', '158.812667847')\n",
      "('Epoch:', '0071', 'cost=', '158.637969971')\n",
      "('Epoch:', '0076', 'cost=', '158.468307495')\n",
      "('Epoch:', '0081', 'cost=', '158.301345825')\n",
      "('Epoch:', '0086', 'cost=', '158.135208130')\n",
      "('Epoch:', '0091', 'cost=', '157.968856812')\n",
      "('Epoch:', '0096', 'cost=', '157.793151855')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '160.974624634')\n",
      "('Epoch:', '0006', 'cost=', '161.094818115')\n",
      "('Epoch:', '0011', 'cost=', '160.802688599')\n",
      "('Epoch:', '0016', 'cost=', '160.531845093')\n",
      "('Epoch:', '0021', 'cost=', '160.300369263')\n",
      "('Epoch:', '0026', 'cost=', '160.095825195')\n",
      "('Epoch:', '0031', 'cost=', '159.906341553')\n",
      "('Epoch:', '0036', 'cost=', '159.733276367')\n",
      "('Epoch:', '0041', 'cost=', '159.554168701')\n",
      "('Epoch:', '0046', 'cost=', '159.389022827')\n",
      "('Epoch:', '0051', 'cost=', '159.214584351')\n",
      "('Epoch:', '0056', 'cost=', '159.044509888')\n",
      "('Epoch:', '0061', 'cost=', '158.877410889')\n",
      "('Epoch:', '0066', 'cost=', '158.715133667')\n",
      "('Epoch:', '0071', 'cost=', '158.544418335')\n",
      "('Epoch:', '0076', 'cost=', '158.386322021')\n",
      "('Epoch:', '0081', 'cost=', '158.214279175')\n",
      "('Epoch:', '0086', 'cost=', '158.053146362')\n",
      "('Epoch:', '0091', 'cost=', '157.882308960')\n",
      "('Epoch:', '0096', 'cost=', '157.718185425')\n",
      "Optimization Finished!\n",
      "Accuracy at task 2: [0.9990543735224586, 0.9657198824681684, 0.9935965848452508]\n",
      "('Epoch:', '0001', 'cost=', '4.049575806')\n",
      "('Epoch:', '0006', 'cost=', '0.866352439')\n",
      "('Epoch:', '0011', 'cost=', '0.704030275')\n",
      "('Epoch:', '0016', 'cost=', '0.690023959')\n",
      "('Epoch:', '0021', 'cost=', '0.681744337')\n",
      "('Epoch:', '0026', 'cost=', '0.688823044')\n",
      "('Epoch:', '0031', 'cost=', '0.676604211')\n",
      "('Epoch:', '0036', 'cost=', '0.664943933')\n",
      "('Epoch:', '0041', 'cost=', '0.654449642')\n",
      "('Epoch:', '0046', 'cost=', '0.640503407')\n",
      "('Epoch:', '0051', 'cost=', '0.631652296')\n",
      "('Epoch:', '0056', 'cost=', '0.625036478')\n",
      "('Epoch:', '0061', 'cost=', '0.620218933')\n",
      "('Epoch:', '0066', 'cost=', '0.609363258')\n",
      "('Epoch:', '0071', 'cost=', '0.604276061')\n",
      "('Epoch:', '0076', 'cost=', '0.600609124')\n",
      "('Epoch:', '0081', 'cost=', '0.598996460')\n",
      "('Epoch:', '0086', 'cost=', '0.598244429')\n",
      "('Epoch:', '0091', 'cost=', '0.593895733')\n",
      "('Epoch:', '0096', 'cost=', '0.589362383')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '160.944442749')\n",
      "('Epoch:', '0006', 'cost=', '160.993225098')\n",
      "('Epoch:', '0011', 'cost=', '160.716232300')\n",
      "('Epoch:', '0016', 'cost=', '160.487701416')\n",
      "('Epoch:', '0021', 'cost=', '160.267257690')\n",
      "('Epoch:', '0026', 'cost=', '160.066406250')\n",
      "('Epoch:', '0031', 'cost=', '159.883041382')\n",
      "('Epoch:', '0036', 'cost=', '159.706726074')\n",
      "('Epoch:', '0041', 'cost=', '159.533370972')\n",
      "('Epoch:', '0046', 'cost=', '159.362792969')\n",
      "('Epoch:', '0051', 'cost=', '159.192718506')\n",
      "('Epoch:', '0056', 'cost=', '159.023208618')\n",
      "('Epoch:', '0061', 'cost=', '158.854644775')\n",
      "('Epoch:', '0066', 'cost=', '158.687362671')\n",
      "('Epoch:', '0071', 'cost=', '158.522033691')\n",
      "('Epoch:', '0076', 'cost=', '158.355895996')\n",
      "('Epoch:', '0081', 'cost=', '158.190216064')\n",
      "('Epoch:', '0086', 'cost=', '158.026138306')\n",
      "('Epoch:', '0091', 'cost=', '157.861145020')\n",
      "('Epoch:', '0096', 'cost=', '157.698608398')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '161.084274292')\n",
      "('Epoch:', '0006', 'cost=', '161.203720093')\n",
      "('Epoch:', '0011', 'cost=', '160.873641968')\n",
      "('Epoch:', '0016', 'cost=', '160.651443481')\n",
      "('Epoch:', '0021', 'cost=', '160.388931274')\n",
      "('Epoch:', '0026', 'cost=', '160.191162109')\n",
      "('Epoch:', '0031', 'cost=', '160.048538208')\n",
      "('Epoch:', '0036', 'cost=', '159.845108032')\n",
      "('Epoch:', '0041', 'cost=', '159.687744141')\n",
      "('Epoch:', '0046', 'cost=', '159.511291504')\n",
      "('Epoch:', '0051', 'cost=', '159.312225342')\n",
      "('Epoch:', '0056', 'cost=', '159.152969360')\n",
      "('Epoch:', '0061', 'cost=', '158.992095947')\n",
      "('Epoch:', '0066', 'cost=', '158.791137695')\n",
      "('Epoch:', '0071', 'cost=', '158.666687012')\n",
      "('Epoch:', '0076', 'cost=', '158.520690918')\n",
      "('Epoch:', '0081', 'cost=', '158.313980103')\n",
      "('Epoch:', '0086', 'cost=', '158.180313110')\n",
      "('Epoch:', '0091', 'cost=', '157.957717896')\n",
      "('Epoch:', '0096', 'cost=', '157.806884766')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '160.979431152')\n",
      "('Epoch:', '0006', 'cost=', '161.099319458')\n",
      "('Epoch:', '0011', 'cost=', '160.800796509')\n",
      "('Epoch:', '0016', 'cost=', '160.570388794')\n",
      "('Epoch:', '0021', 'cost=', '160.322738647')\n",
      "('Epoch:', '0026', 'cost=', '160.111007690')\n",
      "('Epoch:', '0031', 'cost=', '159.915634155')\n",
      "('Epoch:', '0036', 'cost=', '159.747802734')\n",
      "('Epoch:', '0041', 'cost=', '159.576950073')\n",
      "('Epoch:', '0046', 'cost=', '159.397415161')\n",
      "('Epoch:', '0051', 'cost=', '159.232070923')\n",
      "('Epoch:', '0056', 'cost=', '159.060943604')\n",
      "('Epoch:', '0061', 'cost=', '158.915527344')\n",
      "('Epoch:', '0066', 'cost=', '158.731582642')\n",
      "('Epoch:', '0071', 'cost=', '158.557800293')\n",
      "('Epoch:', '0076', 'cost=', '158.393188477')\n",
      "('Epoch:', '0081', 'cost=', '158.230911255')\n",
      "('Epoch:', '0086', 'cost=', '158.069198608')\n",
      "('Epoch:', '0091', 'cost=', '157.888992310')\n",
      "('Epoch:', '0096', 'cost=', '157.728851318')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '161.002502441')\n",
      "('Epoch:', '0006', 'cost=', '161.122589111')\n",
      "('Epoch:', '0011', 'cost=', '160.804336548')\n",
      "('Epoch:', '0016', 'cost=', '160.572082520')\n",
      "('Epoch:', '0021', 'cost=', '160.333602905')\n",
      "('Epoch:', '0026', 'cost=', '160.100051880')\n",
      "('Epoch:', '0031', 'cost=', '159.919494629')\n",
      "('Epoch:', '0036', 'cost=', '159.742691040')\n",
      "('Epoch:', '0041', 'cost=', '159.580047607')\n",
      "('Epoch:', '0046', 'cost=', '159.406188965')\n",
      "('Epoch:', '0051', 'cost=', '159.229156494')\n",
      "('Epoch:', '0056', 'cost=', '159.059417725')\n",
      "('Epoch:', '0061', 'cost=', '158.892318726')\n",
      "('Epoch:', '0066', 'cost=', '158.724929810')\n",
      "('Epoch:', '0071', 'cost=', '158.557723999')\n",
      "('Epoch:', '0076', 'cost=', '158.397277832')\n",
      "('Epoch:', '0081', 'cost=', '158.229843140')\n",
      "('Epoch:', '0086', 'cost=', '158.059921265')\n",
      "('Epoch:', '0091', 'cost=', '157.893920898')\n",
      "('Epoch:', '0096', 'cost=', '157.729553223')\n",
      "Optimization Finished!\n",
      "Accuracy at task 3: [0.9990543735224586, 0.9666993143976493, 0.9941302027748132, 0.9939577039274925]\n",
      "('Epoch:', '0001', 'cost=', '5.565063953')\n",
      "('Epoch:', '0006', 'cost=', '1.228509068')\n",
      "('Epoch:', '0011', 'cost=', '0.902415454')\n",
      "('Epoch:', '0016', 'cost=', '0.874587834')\n",
      "('Epoch:', '0021', 'cost=', '0.856419683')\n",
      "('Epoch:', '0026', 'cost=', '0.817785501')\n",
      "('Epoch:', '0031', 'cost=', '0.821552157')\n",
      "('Epoch:', '0036', 'cost=', '0.805604100')\n",
      "('Epoch:', '0041', 'cost=', '0.790104032')\n",
      "('Epoch:', '0046', 'cost=', '0.781013310')\n",
      "('Epoch:', '0051', 'cost=', '0.764377236')\n",
      "('Epoch:', '0056', 'cost=', '0.788177550')\n",
      "('Epoch:', '0061', 'cost=', '0.740444899')\n",
      "('Epoch:', '0066', 'cost=', '0.725876629')\n",
      "('Epoch:', '0071', 'cost=', '0.733733237')\n",
      "('Epoch:', '0076', 'cost=', '0.730919957')\n",
      "('Epoch:', '0081', 'cost=', '0.721597850')\n",
      "('Epoch:', '0086', 'cost=', '0.708596528')\n",
      "('Epoch:', '0091', 'cost=', '0.728587270')\n",
      "('Epoch:', '0096', 'cost=', '0.711938977')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '160.955169678')\n",
      "('Epoch:', '0006', 'cost=', '161.073883057')\n",
      "('Epoch:', '0011', 'cost=', '160.772888184')\n",
      "('Epoch:', '0016', 'cost=', '160.514160156')\n",
      "('Epoch:', '0021', 'cost=', '160.291259766')\n",
      "('Epoch:', '0026', 'cost=', '160.072341919')\n",
      "('Epoch:', '0031', 'cost=', '159.887634277')\n",
      "('Epoch:', '0036', 'cost=', '159.712753296')\n",
      "('Epoch:', '0041', 'cost=', '159.553665161')\n",
      "('Epoch:', '0046', 'cost=', '159.374633789')\n",
      "('Epoch:', '0051', 'cost=', '159.200195312')\n",
      "('Epoch:', '0056', 'cost=', '159.033233643')\n",
      "('Epoch:', '0061', 'cost=', '158.867980957')\n",
      "('Epoch:', '0066', 'cost=', '158.693511963')\n",
      "('Epoch:', '0071', 'cost=', '158.531250000')\n",
      "('Epoch:', '0076', 'cost=', '158.364913940')\n",
      "('Epoch:', '0081', 'cost=', '158.198242188')\n",
      "('Epoch:', '0086', 'cost=', '158.032302856')\n",
      "('Epoch:', '0091', 'cost=', '157.870513916')\n",
      "('Epoch:', '0096', 'cost=', '157.701934814')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '161.130599976')\n",
      "('Epoch:', '0006', 'cost=', '161.269012451')\n",
      "('Epoch:', '0011', 'cost=', '160.957260132')\n",
      "('Epoch:', '0016', 'cost=', '160.679977417')\n",
      "('Epoch:', '0021', 'cost=', '160.425521851')\n",
      "('Epoch:', '0026', 'cost=', '160.228179932')\n",
      "('Epoch:', '0031', 'cost=', '160.046005249')\n",
      "('Epoch:', '0036', 'cost=', '159.908386230')\n",
      "('Epoch:', '0041', 'cost=', '159.703094482')\n",
      "('Epoch:', '0046', 'cost=', '159.520477295')\n",
      "('Epoch:', '0051', 'cost=', '159.343826294')\n",
      "('Epoch:', '0056', 'cost=', '159.208160400')\n",
      "('Epoch:', '0061', 'cost=', '159.011962891')\n",
      "('Epoch:', '0066', 'cost=', '158.871826172')\n",
      "('Epoch:', '0071', 'cost=', '158.680252075')\n",
      "('Epoch:', '0076', 'cost=', '158.536804199')\n",
      "('Epoch:', '0081', 'cost=', '158.364440918')\n",
      "('Epoch:', '0086', 'cost=', '158.200531006')\n",
      "('Epoch:', '0091', 'cost=', '158.059524536')\n",
      "('Epoch:', '0096', 'cost=', '157.854583740')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '161.394775391')\n",
      "('Epoch:', '0006', 'cost=', '161.262802124')\n",
      "('Epoch:', '0011', 'cost=', '160.924911499')\n",
      "('Epoch:', '0016', 'cost=', '160.688415527')\n",
      "('Epoch:', '0021', 'cost=', '160.429031372')\n",
      "('Epoch:', '0026', 'cost=', '160.209609985')\n",
      "('Epoch:', '0031', 'cost=', '160.049087524')\n",
      "('Epoch:', '0036', 'cost=', '159.867172241')\n",
      "('Epoch:', '0041', 'cost=', '159.704330444')\n",
      "('Epoch:', '0046', 'cost=', '159.608551025')\n",
      "('Epoch:', '0051', 'cost=', '159.358459473')\n",
      "('Epoch:', '0056', 'cost=', '159.206893921')\n",
      "('Epoch:', '0061', 'cost=', '159.013549805')\n",
      "('Epoch:', '0066', 'cost=', '158.837936401')\n",
      "('Epoch:', '0071', 'cost=', '158.671707153')\n",
      "('Epoch:', '0076', 'cost=', '158.544754028')\n",
      "('Epoch:', '0081', 'cost=', '158.387985229')\n",
      "('Epoch:', '0086', 'cost=', '158.182586670')\n",
      "('Epoch:', '0091', 'cost=', '158.002029419')\n",
      "('Epoch:', '0096', 'cost=', '157.875732422')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '161.015411377')\n",
      "('Epoch:', '0006', 'cost=', '161.116622925')\n",
      "('Epoch:', '0011', 'cost=', '160.819671631')\n",
      "('Epoch:', '0016', 'cost=', '160.578079224')\n",
      "('Epoch:', '0021', 'cost=', '160.332839966')\n",
      "('Epoch:', '0026', 'cost=', '160.121368408')\n",
      "('Epoch:', '0031', 'cost=', '159.947021484')\n",
      "('Epoch:', '0036', 'cost=', '159.767791748')\n",
      "('Epoch:', '0041', 'cost=', '159.590515137')\n",
      "('Epoch:', '0046', 'cost=', '159.429595947')\n",
      "('Epoch:', '0051', 'cost=', '159.253402710')\n",
      "('Epoch:', '0056', 'cost=', '159.081192017')\n",
      "('Epoch:', '0061', 'cost=', '158.909591675')\n",
      "('Epoch:', '0066', 'cost=', '158.748977661')\n",
      "('Epoch:', '0071', 'cost=', '158.570831299')\n",
      "('Epoch:', '0076', 'cost=', '158.409912109')\n",
      "('Epoch:', '0081', 'cost=', '158.248474121')\n",
      "('Epoch:', '0086', 'cost=', '158.088470459')\n",
      "('Epoch:', '0091', 'cost=', '157.909057617')\n",
      "('Epoch:', '0096', 'cost=', '157.754089355')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '160.989044189')\n",
      "('Epoch:', '0006', 'cost=', '161.128601074')\n",
      "('Epoch:', '0011', 'cost=', '160.819519043')\n",
      "('Epoch:', '0016', 'cost=', '160.568084717')\n",
      "('Epoch:', '0021', 'cost=', '160.325607300')\n",
      "('Epoch:', '0026', 'cost=', '160.137023926')\n",
      "('Epoch:', '0031', 'cost=', '159.940887451')\n",
      "('Epoch:', '0036', 'cost=', '159.759674072')\n",
      "('Epoch:', '0041', 'cost=', '159.594604492')\n",
      "('Epoch:', '0046', 'cost=', '159.416534424')\n",
      "('Epoch:', '0051', 'cost=', '159.261688232')\n",
      "('Epoch:', '0056', 'cost=', '159.074554443')\n",
      "('Epoch:', '0061', 'cost=', '158.912460327')\n",
      "('Epoch:', '0066', 'cost=', '158.718643188')\n",
      "('Epoch:', '0071', 'cost=', '158.555038452')\n",
      "('Epoch:', '0076', 'cost=', '158.401214600')\n",
      "('Epoch:', '0081', 'cost=', '158.247573853')\n",
      "('Epoch:', '0086', 'cost=', '158.063842773')\n",
      "('Epoch:', '0091', 'cost=', '157.901977539')\n",
      "('Epoch:', '0096', 'cost=', '157.740280151')\n",
      "Optimization Finished!\n",
      "Accuracy at task 4: [0.9985815602836879, 0.9662095984329089, 0.9807897545357525, 0.9919436052366566, 0.9768028240040343]\n",
      "[[0.99952719        nan        nan        nan        nan]\n",
      " [0.99952719 0.97649363        nan        nan        nan]\n",
      " [0.99905437 0.96571988 0.99359658        nan        nan]\n",
      " [0.99905437 0.96669931 0.9941302  0.9939577         nan]\n",
      " [0.99858156 0.9662096  0.98078975 0.99194361 0.97680282]]\n",
      "\n",
      "0.9828654684986079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"vcl_random_coreset.pkl\", task_sequence, \"vcl\")\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp, runner=\"vcl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsvi",
   "language": "python",
   "name": "fsvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}