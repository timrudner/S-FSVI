{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Environment Setup](#setup)\n",
    "* [Results](#results)\n",
    "    * [S-FSVI](#res1)\n",
    "    * [S-FSVI (larger networks)](#res2)\n",
    "    * [S-FSVI (no coreset)](#res3)\n",
    "    * [S-FSVI (minimal coreset)](#res4)\n",
    "    * [FROMP (with lambda-descend coreset)](#res5)\n",
    "    * [VCL (random-choice coreset)](#res6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run as Colab notebook\n",
    "\n",
    "**Important: Before connecting to a kernel, select a GPU runtime. To do so, open the `Runtime` tab above, click `Change runtime type`, and select `GPU`. Run the setup cell below only after you've done this.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pull S-FSVI repository\n",
    "!git clone https://github.com/timrudner/S-FSVI.git\n",
    "# patch required packages\n",
    "!pip install -r ./S-FSVI/colab_requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**After successfully running the cell above, you need to restart the runtime. To do so, open the “Runtime” tab above and and click “Restart runtime”. Once the runtime was restarted, run the cell below. There is no need to re-run the installation in the cell above.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add the repo to path\n",
    "import os\n",
    "import sys\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"S-FSVI\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run as Jupyter notebook (-->skip ahead to “Results” if you are running this as a Colab notebook<--)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install conda environment `fsvi`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!conda env update -f ../environment.yml"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Troubleshooting:\n",
    "\n",
    " - In case there is an error when installing sklearn: run `pip install Cython==0.29.23` manually and then run the above command again.\n",
    " - In case you have access to a GPU, see instructions [here](https://github.com/google/jax#pip-installation-gpu-cuda) for installing the GPU version of `jaxlib`. This will make the experiment run significantly faster."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the command below to install the conda environment as a kernel of the jupyter notebook. Then switch to this kernel using the Jupyter Notebook menu bar by selecting `Kernel`, `Change kernel`, and then selecting `fsvi`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m ipykernel install --user --name=fsvi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Troubleshooting: For further details, see [here](https://medium.com/@nrk25693/how-to-add-your-conda-environment-to-your-jupyter-notebook-in-just-4-steps-abeab8b8d084)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# assuming os.getcwd() returns the directory containing this jupyter notebook\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results <a name=\"results\"></a>\n",
    "\n",
    "To read a model checkpoint instead of training the model from scratch, pass load_chkpt=True to the function read_config_and_run .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scratch/timner/applications/anaconda3/envs/fsvi/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/scratch/timner/applications/anaconda3/envs/fsvi/lib/python3.8/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: TensorFlow is set to only use CPU.\n",
      "Jax is running on gpu\n",
      "WARNING: TensorFlow is set to only use CPU.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False\n",
    "import os\n",
    "import sys\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n",
    "\n",
    "import sfsvi.exps.utils.load_utils as lutils\n",
    "from notebooks.nb_utils.common import read_config_and_run, show_final_average_accuracy\n",
    "\n",
    "task_sequence = \"smnist_sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-FSVI (ours) <a name=\"res1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 1006.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on oat1.cs.ox.ac.uk\n",
      "Jax is running on gpu\n",
      "\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"command\":\"cl\",\n",
      "    \"data_training\":\"continual_learning_smnist_sh\",\n",
      "    \"data_ood\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"model_type\":\"fsvi_mlp\",\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":[\n",
      "        256,\n",
      "        256\n",
      "    ],\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0.0\",\n",
      "    \"prior_cov\":\"0.001\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":80,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0.0,\n",
      "    \"inducing_points\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"uniform_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"equal\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":0.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":5,\n",
      "    \"tau\":1.0,\n",
      "    \"noise_std\":1.0,\n",
      "    \"ind_lim\":\"ind_-1_1\",\n",
      "    \"logging_frequency\":10,\n",
      "    \"figsize\":[\n",
      "        \"10\",\n",
      "        \"4\"\n",
      "    ],\n",
      "    \"seed\":7,\n",
      "    \"save\":false,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"not_specified\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"logroot\":\"ablation\",\n",
      "    \"subdir\":\"reproduce_main_results_2\",\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"n_inducing_inputs\":40,\n",
      "    \"n_inducing_inputs_first_task\":\"not_specified\",\n",
      "    \"n_inducing_inputs_second_task\":\"not_specified\",\n",
      "    \"inducing_inputs_bound\":[\n",
      "        0.0,\n",
      "        1.0\n",
      "    ],\n",
      "    \"fix_shuffle\":false,\n",
      "    \"use_generative_model\":false,\n",
      "    \"inducing_inputs_add_mode\":0,\n",
      "    \"inducing_input_adjustment\":false,\n",
      "    \"not_use_coreset\":false,\n",
      "    \"inducing_input_augmentation\":false,\n",
      "    \"plotting\":false,\n",
      "    \"logging\":1,\n",
      "    \"use_val_split\":false,\n",
      "    \"not_use_val_split\":true,\n",
      "    \"coreset\":\"random\",\n",
      "    \"coreset_entropy_mode\":\"soft_highest\",\n",
      "    \"coreset_entropy_offset\":\"0.0\",\n",
      "    \"coreset_kl_heuristic\":\"lowest\",\n",
      "    \"coreset_kl_offset\":\"0.0\",\n",
      "    \"coreset_elbo_heuristic\":\"lowest\",\n",
      "    \"coreset_elbo_offset\":\"0.0\",\n",
      "    \"coreset_elbo_n_samples\":5,\n",
      "    \"coreset_n_tasks\":\"not_specified\",\n",
      "    \"coreset_entropy_n_mixed\":1,\n",
      "    \"n_coreset_inputs_per_task\":\"not_specified\",\n",
      "    \"full_ntk\":false,\n",
      "    \"constant_inducing_points\":false,\n",
      "    \"n_permuted_tasks\":10,\n",
      "    \"n_omniglot_tasks\":20,\n",
      "    \"epochs_first_task\":80,\n",
      "    \"identity_cov\":false,\n",
      "    \"n_epochs_save_params\":\"not_specified\",\n",
      "    \"n_augment\":\"not_specified\",\n",
      "    \"augment_mode\":\"constant\",\n",
      "    \"n_valid\":\"same\",\n",
      "    \"learning_rate_first_task\":\"not_specified\",\n",
      "    \"save_alt\":true,\n",
      "    \"save_first_task\":false,\n",
      "    \"first_task_load_exp_path\":\"not_specified\",\n",
      "    \"only_task_id\":\"not_specified\",\n",
      "    \"loss_type\":1,\n",
      "    \"only_trainable_head\":false,\n",
      "    \"n_omniglot_coreset_chars\":2,\n",
      "    \"omniglot_randomize_test_split\":false,\n",
      "    \"omniglot_randomize_task_sequence\":false,\n",
      "    \"n_inducing_input_adjust_amount\":\"not_specified\",\n",
      "    \"save_all_params\":false,\n",
      "    \"task\":\"continual_learning_smnist_sh\",\n",
      "    \"architecture_arg\":\"fc_256_256\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (posterior): False\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "\n",
      "Learning task 1\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------\n",
      "  epoch    mean acc    t1 acc\n",
      "-------  ----------  --------\n",
      "      0       99.81     99.81\n",
      "      1       99.95     99.95\n",
      "      2       99.95     99.95\n",
      "      3       99.95     99.95\n",
      "      4       99.95     99.95\n",
      "      5       99.95     99.95\n",
      "      6       99.95     99.95\n",
      "      7       99.95     99.95\n",
      "      8       99.95     99.95\n",
      "      9       99.95     99.95\n",
      "     10       99.95     99.95\n",
      "     11       99.95     99.95\n",
      "     12       99.95     99.95\n",
      "     13       99.95     99.95\n",
      "     14       99.95     99.95\n",
      "     15       99.95     99.95\n",
      "     16       99.95     99.95\n",
      "     17       99.95     99.95\n",
      "     18       99.95     99.95\n",
      "     19       99.95     99.95\n",
      "     20       99.95     99.95\n",
      "     21       99.95     99.95\n",
      "     22       99.95     99.95\n",
      "     23       99.95     99.95\n",
      "     24       99.95     99.95\n",
      "     25       99.95     99.95\n",
      "     26       99.95     99.95\n",
      "     27       99.95     99.95\n",
      "     28       99.95     99.95\n",
      "     29       99.95     99.95\n",
      "     30       99.95     99.95\n",
      "     31       99.95     99.95\n",
      "     32       99.95     99.95\n",
      "     33       99.95     99.95\n",
      "     34       99.95     99.95\n",
      "     35       99.95     99.95\n",
      "     36       99.95     99.95\n",
      "     37       99.95     99.95\n",
      "     38       99.95     99.95\n",
      "     39       99.95     99.95\n",
      "     40       99.95     99.95\n",
      "     41       99.95     99.95\n",
      "     42       99.95     99.95\n",
      "     43       99.95     99.95\n",
      "     44       99.95     99.95\n",
      "     45       99.95     99.95\n",
      "     46       99.95     99.95\n",
      "     47       99.91     99.91\n",
      "     48       99.95     99.95\n",
      "     49      100.00    100.00\n",
      "     50      100.00    100.00\n",
      "     51       99.95     99.95\n",
      "     52       99.95     99.95\n",
      "     53       99.95     99.95\n",
      "     54       99.95     99.95\n",
      "     55       99.95     99.95\n",
      "     56       99.91     99.91\n",
      "     57       99.91     99.91\n",
      "     58       99.95     99.95\n",
      "     59       99.91     99.91\n",
      "     60       99.95     99.95\n",
      "     61       99.95     99.95\n",
      "     62       99.91     99.91\n",
      "     63       99.91     99.91\n",
      "     64       99.95     99.95\n",
      "     65       99.91     99.91\n",
      "     66       99.91     99.91\n",
      "     67       99.91     99.91\n",
      "     68       99.91     99.91\n",
      "     69       99.95     99.95\n",
      "     70       99.95     99.95\n",
      "     71       99.95     99.95\n",
      "     72       99.95     99.95\n",
      "     73       99.91     99.91\n",
      "     74       99.95     99.95\n",
      "     75       99.91     99.91\n",
      "     76       99.95     99.95\n",
      "     77       99.95     99.95\n",
      "     78       99.95     99.95\n",
      "     79       99.91     99.91\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9991 \n",
      "Accuracies (test): [0.9991]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 2\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc\n",
      "-------  ----------  --------  --------\n",
      "      0       92.66     99.72     85.60\n",
      "      1       96.39     98.91     93.88\n",
      "      2       97.25     98.72     95.79\n",
      "      3       97.53     98.44     96.62\n",
      "      4       97.78     98.35     97.21\n",
      "      5       97.78     98.11     97.45\n",
      "      6       97.95     98.20     97.70\n",
      "      7       97.95     97.87     98.04\n",
      "      8       98.12     97.87     98.38\n",
      "      9       98.20     97.78     98.63\n",
      "     10       98.23     97.78     98.68\n",
      "     11       98.28     97.78     98.78\n",
      "     12       98.26     97.64     98.87\n",
      "     13       98.30     97.64     98.97\n",
      "     14       98.37     97.73     99.02\n",
      "     15       98.45     97.83     99.07\n",
      "     16       98.38     97.68     99.07\n",
      "     17       98.40     97.68     99.12\n",
      "     18       98.42     97.83     99.02\n",
      "     19       98.35     97.73     98.97\n",
      "     20       98.33     97.59     99.07\n",
      "     21       98.50     97.83     99.17\n",
      "     22       98.33     97.59     99.07\n",
      "     23       98.42     97.73     99.12\n",
      "     24       98.33     97.59     99.07\n",
      "     25       98.43     97.59     99.27\n",
      "     26       98.45     97.73     99.17\n",
      "     27       98.50     97.83     99.17\n",
      "     28       98.67     98.06     99.27\n",
      "     29       98.57     97.78     99.36\n",
      "     30       98.45     97.54     99.36\n",
      "     31       98.50     97.68     99.31\n",
      "     32       98.41     97.64     99.17\n",
      "     33       98.50     97.68     99.31\n",
      "     34       98.45     97.73     99.17\n",
      "     35       98.57     97.83     99.31\n",
      "     36       98.50     97.87     99.12\n",
      "     37       98.47     97.83     99.12\n",
      "     38       98.59     98.11     99.07\n",
      "     39       98.67     97.97     99.36\n",
      "     40       98.54     97.87     99.22\n",
      "     41       98.69     98.06     99.31\n",
      "     42       98.59     97.92     99.27\n",
      "     43       98.50     97.87     99.12\n",
      "     44       98.57     97.87     99.27\n",
      "     45       98.47     97.78     99.17\n",
      "     46       98.54     97.78     99.31\n",
      "     47       98.45     97.68     99.22\n",
      "     48       98.38     97.54     99.22\n",
      "     49       98.54     97.78     99.31\n",
      "     50       98.50     97.73     99.27\n",
      "     51       98.57     97.97     99.17\n",
      "     52       98.54     97.92     99.17\n",
      "     53       98.59     97.97     99.22\n",
      "     54       98.50     97.78     99.22\n",
      "     55       98.52     97.87     99.17\n",
      "     56       98.52     97.83     99.22\n",
      "     57       98.52     97.78     99.27\n",
      "     58       98.47     97.64     99.31\n",
      "     59       98.55     97.64     99.46\n",
      "     60       98.40     97.49     99.31\n",
      "     61       98.52     97.73     99.31\n",
      "     62       98.50     97.73     99.27\n",
      "     63       98.52     97.83     99.22\n",
      "     64       98.50     97.73     99.27\n",
      "     65       98.67     97.92     99.41\n",
      "     66       98.59     97.83     99.36\n",
      "     67       98.54     97.78     99.31\n",
      "     68       98.59     97.78     99.41\n",
      "     69       98.47     97.49     99.46\n",
      "     70       98.59     97.73     99.46\n",
      "     71       98.43     97.54     99.31\n",
      "     72       98.41     97.40     99.41\n",
      "     73       98.48     97.45     99.51\n",
      "     74       98.55     97.64     99.46\n",
      "     75       98.52     97.83     99.22\n",
      "     76       98.54     97.97     99.12\n",
      "     77       98.59     97.92     99.27\n",
      "     78       98.57     97.87     99.27\n",
      "     79       98.57     97.78     99.36\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9857 \n",
      "Accuracies (test): [0.9778, 0.9936]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 3\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc\n",
      "-------  ----------  --------  --------  --------\n",
      "      0       89.27     98.63     98.48     70.70\n",
      "      1       95.89     98.77     97.65     91.25\n",
      "      2       96.63     98.53     97.06     94.29\n",
      "      3       96.86     98.53     95.74     96.32\n",
      "      4       96.98     98.25     95.79     96.91\n",
      "      5       97.10     98.11     95.49     97.71\n",
      "      6       97.06     97.87     94.96     98.35\n",
      "      7       96.92     97.64     94.66     98.45\n",
      "      8       96.68     97.26     94.12     98.67\n",
      "      9       96.69     97.07     94.27     98.72\n",
      "     10       96.77     97.21     94.22     98.88\n",
      "     11       96.58     96.78     93.93     99.04\n",
      "     12       96.65     97.12     93.73     99.09\n",
      "     13       96.76     96.83     94.37     99.09\n",
      "     14       96.49     96.69     93.63     99.15\n",
      "     15       96.51     96.64     93.73     99.15\n",
      "     16       96.54     96.83     93.63     99.15\n",
      "     17       96.47     96.55     93.78     99.09\n",
      "     18       96.31     96.45     93.34     99.15\n",
      "     19       96.44     96.50     93.63     99.20\n",
      "     20       96.54     96.50     94.03     99.09\n",
      "     21       96.47     96.45     93.93     99.04\n",
      "     22       96.53     96.36     94.07     99.15\n",
      "     23       96.51     96.50     93.93     99.09\n",
      "     24       96.19     95.93     93.34     99.31\n",
      "     25       96.28     96.08     93.44     99.31\n",
      "     26       96.37     96.12     93.68     99.31\n",
      "     27       96.40     96.26     93.68     99.25\n",
      "     28       96.42     96.17     93.78     99.31\n",
      "     29       96.20     95.89     93.29     99.41\n",
      "     30       96.31     95.93     93.58     99.41\n",
      "     31       96.24     96.03     93.34     99.36\n",
      "     32       96.54     96.55     93.93     99.15\n",
      "     33       96.45     96.03     94.07     99.25\n",
      "     34       96.64     96.60     94.27     99.04\n",
      "     35       96.50     96.17     93.98     99.36\n",
      "     36       96.25     95.89     93.44     99.41\n",
      "     37       95.97     95.46     93.05     99.41\n",
      "     38       96.38     95.89     93.93     99.31\n",
      "     39       96.09     95.51     93.34     99.41\n",
      "     40       96.19     95.56     93.54     99.47\n",
      "     41       96.27     95.70     93.63     99.47\n",
      "     42       96.13     95.65     93.34     99.41\n",
      "     43       96.37     95.98     93.73     99.41\n",
      "     44       96.34     96.17     93.49     99.36\n",
      "     45       96.60     96.22     94.32     99.25\n",
      "     46       96.41     95.89     94.03     99.31\n",
      "     47       96.15     95.60     93.44     99.41\n",
      "     48       96.16     95.84     93.24     99.41\n",
      "     49       96.25     95.84     93.54     99.36\n",
      "     50       96.18     95.84     93.34     99.36\n",
      "     51       96.41     95.98     93.78     99.47\n",
      "     52       96.21     95.32     93.83     99.47\n",
      "     53       96.54     96.08     94.12     99.41\n",
      "     54       96.26     95.93     93.44     99.41\n",
      "     55       96.48     96.17     94.03     99.25\n",
      "     56       96.33     95.89     93.73     99.36\n",
      "     57       96.36     96.12     93.54     99.41\n",
      "     58       96.12     95.41     93.54     99.41\n",
      "     59       96.36     95.93     93.73     99.41\n",
      "     60       96.62     96.17     94.37     99.31\n",
      "     61       96.68     96.83     94.66     98.56\n",
      "     62       96.75     96.74     94.32     99.20\n",
      "     63       96.39     95.98     93.68     99.52\n",
      "     64       96.38     96.03     93.63     99.47\n",
      "     65       96.35     96.03     93.44     99.57\n",
      "     66       96.25     96.08     93.14     99.52\n",
      "     67       96.18     95.89     93.14     99.52\n",
      "     68       96.18     95.79     93.19     99.57\n",
      "     69       96.09     95.79     92.90     99.57\n",
      "     70       96.25     95.93     93.29     99.52\n",
      "     71       96.39     96.03     93.63     99.52\n",
      "     72       96.60     96.17     94.22     99.41\n",
      "     73       96.36     96.08     93.58     99.41\n",
      "     74       96.16     95.89     93.19     99.41\n",
      "     75       96.26     95.98     93.34     99.47\n",
      "     76       96.32     95.89     93.49     99.57\n",
      "     77       96.46     95.98     93.83     99.57\n",
      "     78       96.20     95.89     93.14     99.57\n",
      "     79       96.09     95.84     92.90     99.52\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9609 \n",
      "Accuracies (test): [0.9584, 0.929, 0.9952]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 4\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "      0       91.50     97.78     95.10     98.45     74.67\n",
      "      1       95.08     97.83     94.66     98.13     89.68\n",
      "      2       95.66     98.11     94.37     97.39     92.75\n",
      "      3       95.85     97.97     94.37     96.69     94.36\n",
      "      4       96.11     98.06     94.27     96.42     95.67\n",
      "      5       96.10     98.01     94.07     96.26     96.07\n",
      "      6       96.15     98.01     94.22     96.00     96.37\n",
      "      7       96.13     97.92     93.68     96.21     96.73\n",
      "      8       95.91     97.83     93.14     95.89     96.78\n",
      "      9       95.98     97.68     93.39     95.62     97.23\n",
      "     10       95.88     97.64     93.34     95.52     97.03\n",
      "     11       95.90     97.68     93.24     95.41     97.28\n",
      "     12       95.88     97.45     93.39     95.14     97.53\n",
      "     13       95.89     97.35     93.34     95.41     97.48\n",
      "     14       95.89     97.26     93.39     95.14     97.78\n",
      "     15       95.81     97.07     93.24     95.09     97.83\n",
      "     16       95.82     97.26     93.00     94.98     98.04\n",
      "     17       95.82     97.12     93.19     94.88     98.09\n",
      "     18       95.69     96.83     93.05     94.72     98.14\n",
      "     19       95.85     97.21     93.05     95.09     98.04\n",
      "     20       95.82     97.02     93.10     94.98     98.19\n",
      "     21       95.69     96.88     92.95     94.77     98.14\n",
      "     22       95.65     96.78     92.65     94.93     98.24\n",
      "     23       95.74     96.88     92.85     94.98     98.24\n",
      "     24       95.65     96.64     92.95     94.72     98.29\n",
      "     25       95.57     96.50     93.14     94.29     98.34\n",
      "     26       95.66     96.74     93.00     94.66     98.24\n",
      "     27       95.61     96.74     92.70     94.61     98.39\n",
      "     28       95.70     96.74     93.00     94.72     98.34\n",
      "     29       95.58     96.50     92.85     94.50     98.49\n",
      "     30       95.65     96.36     93.19     94.50     98.54\n",
      "     31       95.58     96.31     92.80     94.72     98.49\n",
      "     32       95.56     96.22     92.90     94.56     98.54\n",
      "     33       95.56     96.45     92.70     94.61     98.49\n",
      "     34       95.60     96.45     92.95     94.50     98.49\n",
      "     35       95.72     96.69     92.75     95.14     98.29\n",
      "     36       95.75     96.83     92.75     95.09     98.34\n",
      "     37       95.62     96.41     92.90     94.77     98.39\n",
      "     38       95.56     96.26     92.51     94.93     98.54\n",
      "     39       95.59     96.41     92.70     94.61     98.64\n",
      "     40       95.61     96.26     92.85     94.72     98.59\n",
      "     41       95.53     96.26     92.70     94.50     98.64\n",
      "     42       95.50     96.36     92.41     94.72     98.49\n",
      "     43       95.71     96.50     92.80     95.04     98.49\n",
      "     44       95.64     96.41     92.80     94.77     98.59\n",
      "     45       95.65     96.22     92.85     94.98     98.54\n",
      "     46       95.60     96.22     92.80     94.72     98.64\n",
      "     47       95.62     96.45     92.36     95.14     98.54\n",
      "     48       95.64     96.31     92.80     94.82     98.64\n",
      "     49       95.53     96.31     92.61     94.61     98.59\n",
      "     50       95.52     96.17     92.65     94.77     98.49\n",
      "     51       95.50     96.17     92.46     94.77     98.59\n",
      "     52       95.67     96.55     92.51     95.14     98.49\n",
      "     53       95.52     96.03     92.75     94.82     98.49\n",
      "     54       95.57     96.31     92.65     94.72     98.59\n",
      "     55       95.49     96.26     92.36     94.82     98.54\n",
      "     56       95.51     96.12     92.41     94.88     98.64\n",
      "     57       95.53     96.36     92.36     94.77     98.64\n",
      "     58       95.58     96.26     92.65     94.72     98.69\n",
      "     59       95.56     96.26     92.46     94.77     98.74\n",
      "     60       95.50     96.26     92.65     94.50     98.59\n",
      "     61       95.60     96.17     92.70     94.77     98.74\n",
      "     62       95.64     96.31     92.85     94.82     98.59\n",
      "     63       95.53     96.26     92.56     94.72     98.59\n",
      "     64       95.66     96.36     92.80     94.88     98.59\n",
      "     65       95.60     96.26     92.70     94.77     98.69\n",
      "     66       95.44     96.03     92.41     94.66     98.64\n",
      "     67       95.49     95.89     92.56     94.61     98.89\n",
      "     68       95.60     96.17     92.75     94.66     98.84\n",
      "     69       95.47     95.98     92.31     94.77     98.84\n",
      "     70       95.49     96.12     92.41     94.61     98.84\n",
      "     71       95.50     95.98     92.41     94.77     98.84\n",
      "     72       95.52     96.17     92.46     94.66     98.79\n",
      "     73       95.62     96.22     92.70     94.98     98.59\n",
      "     74       95.58     96.08     92.65     94.88     98.69\n",
      "     75       95.56     96.22     92.56     94.77     98.69\n",
      "     76       95.53     96.17     92.41     94.82     98.74\n",
      "     77       95.54     96.03     92.41     94.77     98.94\n",
      "     78       95.53     96.12     92.41     94.72     98.89\n",
      "     79       95.54     95.89     92.61     94.72     98.94\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9554 \n",
      "Accuracies (test): [0.9589, 0.9261, 0.9472, 0.9894]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 5\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "      0       86.35     97.07     92.61     93.44     98.79     49.82\n",
      "      1       91.19     97.02     92.26     92.64     98.49     75.54\n",
      "      2       92.27     96.64     91.97     92.05     97.78     82.90\n",
      "      3       92.69     96.55     91.97     91.73     97.53     85.68\n",
      "      4       93.02     96.60     91.67     91.46     97.48     87.90\n",
      "      5       93.09     96.50     90.74     91.57     97.08     89.56\n",
      "      6       93.04     96.31     91.19     90.02     96.73     90.97\n",
      "      7       92.97     96.17     90.65     90.18     96.17     91.68\n",
      "      8       93.16     96.17     91.14     89.91     96.12     92.44\n",
      "      9       93.09     96.08     90.55     89.65     96.02     93.14\n",
      "     10       93.19     96.17     90.94     89.81     95.82     93.19\n",
      "     11       93.15     96.12     90.70     89.49     95.67     93.75\n",
      "     12       93.08     95.98     90.79     89.43     95.32     93.90\n",
      "     13       92.94     95.89     89.96     89.38     95.32     94.15\n",
      "     14       92.98     96.03     90.11     89.27     95.07     94.40\n",
      "     15       93.09     95.84     90.74     89.22     95.02     94.65\n",
      "     16       92.98     95.93     90.65     88.63     94.56     95.11\n",
      "     17       92.93     95.89     90.01     89.01     94.56     95.16\n",
      "     18       93.00     95.93     90.30     89.06     94.61     95.11\n",
      "     19       92.95     95.70     90.16     89.06     94.56     95.26\n",
      "     20       92.96     95.98     89.96     89.06     94.56     95.26\n",
      "     21       93.00     96.03     89.96     89.01     94.46     95.56\n",
      "     22       92.90     95.89     89.81     89.11     94.16     95.51\n",
      "     23       92.97     95.51     89.91     89.06     94.61     95.76\n",
      "     24       92.88     95.79     90.16     88.63     94.21     95.61\n",
      "     25       92.89     95.74     89.62     89.06     94.31     95.71\n",
      "     26       92.87     95.74     89.57     88.74     94.16     96.12\n",
      "     27       92.84     95.32     89.96     88.85     93.91     96.17\n",
      "     28       92.89     95.51     90.06     88.47     94.11     96.32\n",
      "     29       92.91     95.65     89.96     88.47     93.91     96.57\n",
      "     30       92.96     95.51     89.76     88.85     93.96     96.72\n",
      "     31       92.85     95.37     90.01     88.26     93.81     96.82\n",
      "     32       92.97     95.46     89.86     88.79     94.06     96.67\n",
      "     33       92.95     95.32     90.11     88.79     93.91     96.62\n",
      "     34       92.86     95.41     90.01     88.53     93.61     96.72\n",
      "     35       92.92     95.32     90.11     88.90     93.55     96.72\n",
      "     36       92.85     95.13     89.57     88.63     93.91     97.02\n",
      "     37       92.88     95.32     89.67     88.42     94.01     96.97\n",
      "     38       92.95     95.60     89.86     88.90     93.66     96.72\n",
      "     39       92.86     95.74     89.96     88.15     93.55     96.92\n",
      "     40       92.88     95.22     90.06     88.53     93.66     96.92\n",
      "     41       92.89     95.56     89.86     88.63     93.50     96.92\n",
      "     42       92.93     95.22     90.06     88.79     93.66     96.92\n",
      "     43       92.77     95.13     89.72     88.37     93.50     97.13\n",
      "     44       92.93     95.18     89.86     88.74     93.76     97.13\n",
      "     45       92.77     95.13     89.76     88.15     93.66     97.13\n",
      "     46       92.86     95.13     89.91     88.74     93.50     97.02\n",
      "     47       92.65     95.13     89.42     88.47     93.15     97.08\n",
      "     48       92.80     95.13     89.81     88.42     93.50     97.13\n",
      "     49       92.78     95.18     89.86     88.74     93.00     97.13\n",
      "     50       92.79     94.99     89.76     88.37     93.55     97.28\n",
      "     51       92.75     95.27     89.86     88.47     93.05     97.08\n",
      "     52       92.81     95.04     90.01     88.47     93.35     97.18\n",
      "     53       92.90     95.27     89.91     88.53     93.45     97.33\n",
      "     54       92.77     95.22     89.62     88.53     93.20     97.28\n",
      "     55       92.79     95.08     89.91     88.53     93.00     97.43\n",
      "     56       92.70     95.08     89.86     88.42     92.95     97.18\n",
      "     57       92.74     95.04     89.72     88.53     93.05     97.38\n",
      "     58       92.78     95.32     89.62     88.53     93.20     97.23\n",
      "     59       92.79     95.13     89.81     88.74     93.15     97.13\n",
      "     60       92.95     95.27     90.01     89.17     93.10     97.18\n",
      "     61       92.86     95.04     90.06     88.85     93.15     97.18\n",
      "     62       92.68     95.04     89.62     88.58     92.90     97.28\n",
      "     63       92.84     95.13     89.81     88.90     93.10     97.28\n",
      "     64       92.75     95.22     89.76     88.63     92.95     97.18\n",
      "     65       92.70     95.27     89.72     88.31     92.90     97.28\n",
      "     66       92.68     95.18     89.72     88.31     92.90     97.28\n",
      "     67       92.75     95.04     89.81     88.47     93.00     97.43\n",
      "     68       92.84     94.94     89.91     89.06     93.00     97.28\n",
      "     69       92.70     95.08     89.81     88.53     92.80     97.28\n",
      "     70       92.90     95.22     89.86     89.17     92.95     97.28\n",
      "     71       92.70     95.13     89.91     88.21     92.85     97.38\n",
      "     72       92.75     95.08     89.67     88.53     93.00     97.48\n",
      "     73       92.66     94.89     89.76     88.37     92.95     97.33\n",
      "     74       92.78     95.08     89.91     88.53     92.80     97.58\n",
      "     75       92.65     95.08     89.47     88.47     92.80     97.43\n",
      "     76       92.67     94.89     89.67     88.42     92.85     97.53\n",
      "     77       92.71     94.94     89.57     88.69     92.85     97.48\n",
      "     78       92.85     95.08     89.91     88.85     93.05     97.38\n",
      "     79       92.90     95.32     89.72     88.90     93.05     97.53\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9290 \n",
      "Accuracies (test): [0.9532, 0.8972, 0.889, 0.9305, 0.9753]\n",
      "---\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n",
      "\n",
      "0.92904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fsvi_match.pkl\", task_sequence)\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-FSVI (larger networks) <a name=\"res2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 1081.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on oat2.cs.ox.ac.uk\n",
      "Jax is running on gpu\n",
      "\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"command\":\"cl\",\n",
      "    \"data_training\":\"continual_learning_smnist_sh\",\n",
      "    \"data_ood\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"model_type\":\"fsvi_mlp\",\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":[\n",
      "        400\n",
      "    ],\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0.0\",\n",
      "    \"prior_cov\":\"0.001\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":80,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0.0,\n",
      "    \"inducing_points\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"uniform_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"equal\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":0.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":5,\n",
      "    \"tau\":1.0,\n",
      "    \"noise_std\":1.0,\n",
      "    \"ind_lim\":\"ind_-1_1\",\n",
      "    \"logging_frequency\":10,\n",
      "    \"figsize\":[\n",
      "        \"10\",\n",
      "        \"4\"\n",
      "    ],\n",
      "    \"seed\":5,\n",
      "    \"save\":false,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"not_specified\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"logroot\":\"ablation\",\n",
      "    \"subdir\":\"reproduce_main_results_2\",\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"n_inducing_inputs\":40,\n",
      "    \"n_inducing_inputs_first_task\":\"not_specified\",\n",
      "    \"n_inducing_inputs_second_task\":\"not_specified\",\n",
      "    \"inducing_inputs_bound\":[\n",
      "        0.0,\n",
      "        1.0\n",
      "    ],\n",
      "    \"fix_shuffle\":false,\n",
      "    \"use_generative_model\":false,\n",
      "    \"inducing_inputs_add_mode\":0,\n",
      "    \"inducing_input_adjustment\":false,\n",
      "    \"not_use_coreset\":false,\n",
      "    \"inducing_input_augmentation\":false,\n",
      "    \"plotting\":false,\n",
      "    \"logging\":1,\n",
      "    \"use_val_split\":false,\n",
      "    \"not_use_val_split\":true,\n",
      "    \"coreset\":\"random\",\n",
      "    \"coreset_entropy_mode\":\"soft_highest\",\n",
      "    \"coreset_entropy_offset\":\"0.0\",\n",
      "    \"coreset_kl_heuristic\":\"lowest\",\n",
      "    \"coreset_kl_offset\":\"0.0\",\n",
      "    \"coreset_elbo_heuristic\":\"lowest\",\n",
      "    \"coreset_elbo_offset\":\"0.0\",\n",
      "    \"coreset_elbo_n_samples\":5,\n",
      "    \"coreset_n_tasks\":\"not_specified\",\n",
      "    \"coreset_entropy_n_mixed\":1,\n",
      "    \"n_coreset_inputs_per_task\":\"not_specified\",\n",
      "    \"full_ntk\":false,\n",
      "    \"constant_inducing_points\":false,\n",
      "    \"n_permuted_tasks\":10,\n",
      "    \"n_omniglot_tasks\":20,\n",
      "    \"epochs_first_task\":80,\n",
      "    \"identity_cov\":false,\n",
      "    \"n_epochs_save_params\":\"not_specified\",\n",
      "    \"n_augment\":\"not_specified\",\n",
      "    \"augment_mode\":\"constant\",\n",
      "    \"n_valid\":\"same\",\n",
      "    \"learning_rate_first_task\":\"not_specified\",\n",
      "    \"save_alt\":true,\n",
      "    \"save_first_task\":false,\n",
      "    \"first_task_load_exp_path\":\"not_specified\",\n",
      "    \"only_task_id\":\"not_specified\",\n",
      "    \"loss_type\":1,\n",
      "    \"only_trainable_head\":false,\n",
      "    \"n_omniglot_coreset_chars\":2,\n",
      "    \"omniglot_randomize_test_split\":false,\n",
      "    \"omniglot_randomize_task_sequence\":false,\n",
      "    \"n_inducing_input_adjust_amount\":\"not_specified\",\n",
      "    \"save_all_params\":false,\n",
      "    \"task\":\"continual_learning_smnist_sh\",\n",
      "    \"architecture_arg\":\"fc_400\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (posterior): False\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "\n",
      "Learning task 1\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------\n",
      "  epoch    mean acc    t1 acc\n",
      "-------  ----------  --------\n",
      "      0       96.69     96.69\n",
      "      1       99.72     99.72\n",
      "      2       99.86     99.86\n",
      "      3       99.86     99.86\n",
      "      4       99.91     99.91\n",
      "      5       99.91     99.91\n",
      "      6       99.95     99.95\n",
      "      7       99.95     99.95\n",
      "      8       99.95     99.95\n",
      "      9       99.95     99.95\n",
      "     10       99.95     99.95\n",
      "     11       99.95     99.95\n",
      "     12       99.95     99.95\n",
      "     13       99.95     99.95\n",
      "     14       99.95     99.95\n",
      "     15       99.95     99.95\n",
      "     16       99.95     99.95\n",
      "     17       99.95     99.95\n",
      "     18       99.95     99.95\n",
      "     19       99.95     99.95\n",
      "     20       99.95     99.95\n",
      "     21       99.95     99.95\n",
      "     22       99.95     99.95\n",
      "     23       99.95     99.95\n",
      "     24       99.95     99.95\n",
      "     25       99.95     99.95\n",
      "     26       99.95     99.95\n",
      "     27       99.91     99.91\n",
      "     28       99.91     99.91\n",
      "     29       99.91     99.91\n",
      "     30       99.91     99.91\n",
      "     31       99.91     99.91\n",
      "     32       99.91     99.91\n",
      "     33       99.91     99.91\n",
      "     34       99.91     99.91\n",
      "     35       99.91     99.91\n",
      "     36       99.91     99.91\n",
      "     37       99.91     99.91\n",
      "     38       99.91     99.91\n",
      "     39       99.91     99.91\n",
      "     40       99.95     99.95\n",
      "     41       99.95     99.95\n",
      "     42       99.95     99.95\n",
      "     43       99.95     99.95\n",
      "     44       99.95     99.95\n",
      "     45       99.95     99.95\n",
      "     46       99.95     99.95\n",
      "     47       99.95     99.95\n",
      "     48       99.95     99.95\n",
      "     49       99.95     99.95\n",
      "     50       99.95     99.95\n",
      "     51       99.95     99.95\n",
      "     52       99.95     99.95\n",
      "     53       99.95     99.95\n",
      "     54       99.95     99.95\n",
      "     55       99.95     99.95\n",
      "     56       99.95     99.95\n",
      "     57       99.95     99.95\n",
      "     58       99.95     99.95\n",
      "     59       99.95     99.95\n",
      "     60       99.95     99.95\n",
      "     61       99.95     99.95\n",
      "     62       99.95     99.95\n",
      "     63       99.95     99.95\n",
      "     64       99.95     99.95\n",
      "     65       99.95     99.95\n",
      "     66       99.95     99.95\n",
      "     67       99.95     99.95\n",
      "     68       99.95     99.95\n",
      "     69       99.95     99.95\n",
      "     70       99.95     99.95\n",
      "     71       99.95     99.95\n",
      "     72       99.95     99.95\n",
      "     73       99.95     99.95\n",
      "     74       99.95     99.95\n",
      "     75       99.95     99.95\n",
      "     76       99.95     99.95\n",
      "     77       99.95     99.95\n",
      "     78       99.95     99.95\n",
      "     79       99.95     99.95\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9995 \n",
      "Accuracies (test): [0.9995]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 2\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc\n",
      "-------  ----------  --------  --------\n",
      "      0       85.04     99.81     70.27\n",
      "      1       93.16     99.39     86.92\n",
      "      2       94.39     99.15     89.62\n",
      "      3       95.52     98.91     92.12\n",
      "      4       95.88     98.77     93.00\n",
      "      5       96.18     98.68     93.68\n",
      "      6       96.38     98.53     94.22\n",
      "      7       96.50     98.44     94.56\n",
      "      8       96.72     98.25     95.20\n",
      "      9       97.00     98.20     95.79\n",
      "     10       97.08     97.87     96.28\n",
      "     11       97.25     97.97     96.52\n",
      "     12       97.25     97.78     96.72\n",
      "     13       97.32     97.73     96.91\n",
      "     14       97.38     97.64     97.11\n",
      "     15       97.47     97.64     97.31\n",
      "     16       97.65     97.59     97.70\n",
      "     17       97.60     97.45     97.75\n",
      "     18       97.67     97.45     97.89\n",
      "     19       97.65     97.26     98.04\n",
      "     20       97.77     97.35     98.19\n",
      "     21       97.72     97.26     98.19\n",
      "     22       97.72     97.16     98.29\n",
      "     23       97.89     97.26     98.53\n",
      "     24       97.89     97.21     98.58\n",
      "     25       97.77     97.12     98.43\n",
      "     26       97.94     97.26     98.63\n",
      "     27       97.85     97.07     98.63\n",
      "     28       97.90     97.12     98.68\n",
      "     29       97.97     97.21     98.73\n",
      "     30       97.90     97.12     98.68\n",
      "     31       97.92     97.12     98.73\n",
      "     32       97.82     97.02     98.63\n",
      "     33       97.88     97.02     98.73\n",
      "     34       97.85     97.02     98.68\n",
      "     35       97.88     97.02     98.73\n",
      "     36       97.85     97.02     98.68\n",
      "     37       97.92     97.12     98.73\n",
      "     38       97.97     97.07     98.87\n",
      "     39       97.88     97.07     98.68\n",
      "     40       97.86     96.93     98.78\n",
      "     41       97.92     97.02     98.82\n",
      "     42       97.85     96.97     98.73\n",
      "     43       97.80     96.88     98.73\n",
      "     44       97.89     96.97     98.82\n",
      "     45       97.94     97.07     98.82\n",
      "     46       97.97     97.07     98.87\n",
      "     47       97.88     96.97     98.78\n",
      "     48       97.86     96.93     98.78\n",
      "     49       97.90     97.02     98.78\n",
      "     50       97.97     97.16     98.78\n",
      "     51       97.94     97.07     98.82\n",
      "     52       97.94     97.07     98.82\n",
      "     53       97.92     97.07     98.78\n",
      "     54       97.86     96.93     98.78\n",
      "     55       97.94     97.07     98.82\n",
      "     56       97.90     97.02     98.78\n",
      "     57       97.89     96.97     98.82\n",
      "     58       97.86     96.93     98.78\n",
      "     59       97.83     96.88     98.78\n",
      "     60       97.78     96.78     98.78\n",
      "     61       97.83     96.78     98.87\n",
      "     62       97.85     96.88     98.82\n",
      "     63       97.88     96.93     98.82\n",
      "     64       97.88     96.83     98.92\n",
      "     65       97.75     96.69     98.82\n",
      "     66       97.85     96.83     98.87\n",
      "     67       97.83     96.83     98.82\n",
      "     68       97.86     96.93     98.78\n",
      "     69       97.88     96.93     98.82\n",
      "     70       97.85     96.88     98.82\n",
      "     71       98.02     97.21     98.82\n",
      "     72       97.92     97.02     98.82\n",
      "     73       97.90     97.02     98.78\n",
      "     74       97.88     96.88     98.87\n",
      "     75       97.83     96.83     98.82\n",
      "     76       97.86     96.74     98.97\n",
      "     77       97.81     96.64     98.97\n",
      "     78       97.81     96.64     98.97\n",
      "     79       97.85     96.83     98.87\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9785 \n",
      "Accuracies (test): [0.9683, 0.9887]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 3\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc\n",
      "-------  ----------  --------  --------  --------\n",
      "      0       86.73     97.26     98.48     64.46\n",
      "      1       93.07     97.26     97.65     84.31\n",
      "      2       94.97     97.16     97.21     90.55\n",
      "      3       95.57     97.21     96.91     92.58\n",
      "      4       95.98     96.88     96.87     94.18\n",
      "      5       96.15     96.69     96.77     94.98\n",
      "      6       96.23     96.78     96.62     95.30\n",
      "      7       96.43     96.69     96.43     96.16\n",
      "      8       96.39     96.55     96.03     96.58\n",
      "      9       96.53     96.55     96.03     97.01\n",
      "     10       96.54     96.45     95.94     97.23\n",
      "     11       96.49     96.36     95.45     97.65\n",
      "     12       96.59     96.41     95.54     97.81\n",
      "     13       96.61     96.31     95.54     97.97\n",
      "     14       96.45     96.22     95.10     98.03\n",
      "     15       96.45     96.12     95.05     98.19\n",
      "     16       96.45     96.17     95.00     98.19\n",
      "     17       96.52     96.22     95.15     98.19\n",
      "     18       96.43     96.08     94.91     98.29\n",
      "     19       96.41     96.08     94.91     98.24\n",
      "     20       96.38     95.93     94.81     98.40\n",
      "     21       96.35     95.84     94.71     98.51\n",
      "     22       96.13     95.70     94.12     98.56\n",
      "     23       96.24     95.79     94.42     98.51\n",
      "     24       96.34     95.89     94.61     98.51\n",
      "     25       96.21     95.84     94.17     98.61\n",
      "     26       96.24     95.74     94.37     98.61\n",
      "     27       96.38     95.93     94.76     98.45\n",
      "     28       96.21     95.74     94.32     98.56\n",
      "     29       96.21     95.74     94.12     98.77\n",
      "     30       96.15     95.60     93.98     98.88\n",
      "     31       96.18     95.65     94.07     98.83\n",
      "     32       96.21     95.84     94.03     98.77\n",
      "     33       96.15     95.60     94.03     98.83\n",
      "     34       96.17     95.60     94.07     98.83\n",
      "     35       96.34     95.84     94.47     98.72\n",
      "     36       96.22     95.74     94.03     98.88\n",
      "     37       96.22     95.84     93.98     98.83\n",
      "     38       96.18     95.79     93.88     98.88\n",
      "     39       96.04     95.70     93.54     98.88\n",
      "     40       96.07     95.65     93.68     98.88\n",
      "     41       96.01     95.60     93.49     98.93\n",
      "     42       95.88     95.51     93.19     98.93\n",
      "     43       95.97     95.65     93.39     98.88\n",
      "     44       95.91     95.60     93.24     98.88\n",
      "     45       96.26     95.89     94.12     98.77\n",
      "     46       96.16     95.74     93.98     98.77\n",
      "     47       96.04     95.51     93.68     98.93\n",
      "     48       96.06     95.56     93.73     98.88\n",
      "     49       95.91     95.46     93.34     98.93\n",
      "     50       95.96     95.51     93.44     98.93\n",
      "     51       95.98     95.51     93.49     98.93\n",
      "     52       95.88     95.46     93.24     98.93\n",
      "     53       95.94     95.56     93.34     98.93\n",
      "     54       95.94     95.46     93.44     98.93\n",
      "     55       95.91     95.51     93.29     98.93\n",
      "     56       96.02     95.65     93.58     98.83\n",
      "     57       95.96     95.56     93.49     98.83\n",
      "     58       95.97     95.41     93.63     98.88\n",
      "     59       95.91     95.51     93.34     98.88\n",
      "     60       95.84     95.46     93.19     98.88\n",
      "     61       95.83     95.37     93.19     98.93\n",
      "     62       95.73     95.22     93.05     98.93\n",
      "     63       95.75     95.22     93.10     98.93\n",
      "     64       95.82     95.32     93.14     98.99\n",
      "     65       95.79     95.27     93.10     98.99\n",
      "     66       95.80     95.22     93.29     98.88\n",
      "     67       95.96     95.51     93.54     98.83\n",
      "     68       96.24     95.74     94.22     98.77\n",
      "     69       96.12     95.65     93.93     98.77\n",
      "     70       95.91     95.41     93.54     98.77\n",
      "     71       95.89     95.46     93.34     98.88\n",
      "     72       95.94     95.46     93.58     98.77\n",
      "     73       95.86     95.46     93.24     98.88\n",
      "     74       95.81     95.41     93.14     98.88\n",
      "     75       95.85     95.37     93.19     98.99\n",
      "     76       95.85     95.37     93.24     98.93\n",
      "     77       95.85     95.37     93.19     98.99\n",
      "     78       95.82     95.37     93.10     98.99\n",
      "     79       95.88     95.46     93.29     98.88\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9588 \n",
      "Accuracies (test): [0.9546, 0.9329, 0.9888]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 4\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "      0       83.67     96.41     94.03     98.67     45.57\n",
      "      1       92.10     96.60     93.83     98.56     79.41\n",
      "      2       93.95     96.60     93.49     98.35     87.36\n",
      "      3       94.52     96.41     93.44     98.35     89.88\n",
      "      4       94.77     96.55     92.95     98.24     91.34\n",
      "      5       94.92     96.41     92.85     97.92     92.50\n",
      "      6       95.06     96.45     92.95     97.60     93.25\n",
      "      7       95.20     96.41     92.70     97.65     94.06\n",
      "      8       95.17     96.41     92.56     97.44     94.26\n",
      "      9       95.23     96.45     92.56     97.17     94.76\n",
      "     10       95.28     96.31     92.65     97.07     95.07\n",
      "     11       95.30     96.22     92.65     96.91     95.42\n",
      "     12       95.26     96.22     92.31     96.85     95.67\n",
      "     13       95.29     96.17     92.41     96.74     95.82\n",
      "     14       95.22     95.98     92.31     96.37     96.22\n",
      "     15       95.27     96.12     92.16     96.48     96.32\n",
      "     16       95.28     96.03     92.21     96.37     96.53\n",
      "     17       95.14     95.84     91.87     96.16     96.68\n",
      "     18       95.24     95.84     92.07     96.37     96.68\n",
      "     19       95.21     95.93     92.07     96.05     96.78\n",
      "     20       95.12     95.84     91.97     95.84     96.83\n",
      "     21       95.16     95.79     91.92     96.05     96.88\n",
      "     22       95.25     96.08     92.16     95.78     96.98\n",
      "     23       95.16     95.79     91.92     95.84     97.08\n",
      "     24       95.17     95.84     91.92     95.78     97.13\n",
      "     25       95.15     95.79     91.87     95.78     97.18\n",
      "     26       95.08     95.70     91.77     95.62     97.23\n",
      "     27       95.09     95.74     91.72     95.52     97.38\n",
      "     28       95.16     95.65     91.92     95.57     97.48\n",
      "     29       95.14     95.70     91.82     95.62     97.43\n",
      "     30       95.03     95.79     91.72     95.20     97.43\n",
      "     31       95.09     95.98     91.63     95.25     97.48\n",
      "     32       95.03     95.79     91.67     95.14     97.53\n",
      "     33       95.07     95.79     91.67     95.20     97.63\n",
      "     34       95.03     95.70     91.63     95.14     97.63\n",
      "     35       95.02     95.84     91.48     95.04     97.73\n",
      "     36       95.05     95.70     91.67     95.09     97.73\n",
      "     37       95.08     95.84     91.72     95.14     97.63\n",
      "     38       95.05     95.84     91.58     95.04     97.73\n",
      "     39       95.02     95.79     91.58     94.98     97.73\n",
      "     40       94.97     95.79     91.33     95.04     97.73\n",
      "     41       95.10     95.89     91.63     95.14     97.73\n",
      "     42       95.01     95.60     91.67     94.98     97.78\n",
      "     43       94.98     95.70     91.48     94.93     97.83\n",
      "     44       94.90     95.46     91.33     94.93     97.89\n",
      "     45       94.94     95.70     91.19     94.98     97.89\n",
      "     46       94.90     95.60     91.19     94.98     97.83\n",
      "     47       94.89     95.51     91.19     94.98     97.89\n",
      "     48       94.94     95.70     91.19     94.98     97.89\n",
      "     49       94.88     95.60     91.09     94.93     97.89\n",
      "     50       94.91     95.51     91.14     94.98     97.99\n",
      "     51       94.86     95.46     91.04     94.93     97.99\n",
      "     52       94.89     95.51     91.19     94.93     97.94\n",
      "     53       94.83     95.51     90.94     94.93     97.94\n",
      "     54       94.89     95.60     91.09     94.88     97.99\n",
      "     55       94.86     95.51     91.09     94.88     97.94\n",
      "     56       94.84     95.46     90.99     94.88     98.04\n",
      "     57       94.78     95.37     90.99     94.88     97.89\n",
      "     58       94.84     95.37     91.14     94.77     98.09\n",
      "     59       94.83     95.51     90.99     94.72     98.09\n",
      "     60       94.86     95.41     91.04     94.88     98.09\n",
      "     61       94.89     95.60     91.04     94.93     97.99\n",
      "     62       94.83     95.46     90.99     94.82     98.04\n",
      "     63       94.88     95.51     91.09     94.77     98.14\n",
      "     64       94.84     95.41     90.99     94.82     98.14\n",
      "     65       94.81     95.41     91.04     94.66     98.14\n",
      "     66       94.78     95.32     90.89     94.77     98.14\n",
      "     67       94.79     95.32     90.99     94.77     98.09\n",
      "     68       94.79     95.27     90.99     94.82     98.09\n",
      "     69       94.77     95.32     90.94     94.72     98.09\n",
      "     70       94.79     95.41     90.89     94.66     98.19\n",
      "     71       94.74     95.32     90.99     94.45     98.19\n",
      "     72       94.71     95.32     90.94     94.40     98.19\n",
      "     73       94.79     95.27     90.99     94.72     98.19\n",
      "     74       94.78     95.37     90.89     94.61     98.24\n",
      "     75       94.76     95.41     90.89     94.61     98.14\n",
      "     76       94.79     95.27     90.99     94.77     98.14\n",
      "     77       94.74     95.18     90.94     94.61     98.24\n",
      "     78       94.77     95.22     90.94     94.72     98.19\n",
      "     79       94.73     95.18     90.79     94.61     98.34\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9473 \n",
      "Accuracies (test): [0.9518, 0.9079, 0.9461, 0.9834]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 5\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "      0       75.88     95.51     90.99     94.56     98.34      0.00\n",
      "      1       77.32     95.46     90.89     94.77     98.24      7.26\n",
      "      2       82.36     95.74     90.89     94.72     98.29     32.17\n",
      "      3       85.82     95.74     91.14     94.29     98.29     49.62\n",
      "      4       88.20     95.79     90.79     93.97     98.34     62.13\n",
      "      5       89.26     95.70     90.89     93.60     98.24     67.88\n",
      "      6       90.19     95.56     90.70     93.49     98.14     73.07\n",
      "      7       90.93     95.70     90.50     93.22     98.24     77.00\n",
      "      8       91.34     95.65     90.40     93.01     98.09     79.53\n",
      "      9       91.71     95.65     90.35     92.80     97.99     81.74\n",
      "     10       91.92     95.65     90.16     92.37     97.99     83.41\n",
      "     11       92.27     95.65     90.16     92.26     97.99     85.27\n",
      "     12       92.36     95.56     90.01     92.10     97.89     86.23\n",
      "     13       92.45     95.65     90.01     91.84     97.83     86.94\n",
      "     14       92.61     95.65     89.91     91.94     97.78     87.75\n",
      "     15       92.68     95.60     89.67     91.57     97.68     88.86\n",
      "     16       92.86     95.65     89.76     91.68     97.63     89.56\n",
      "     17       92.91     95.60     89.72     91.25     97.63     90.37\n",
      "     18       92.87     95.56     89.52     91.14     97.53     90.62\n",
      "     19       92.98     95.51     89.57     91.30     97.48     91.02\n",
      "     20       93.04     95.46     89.42     91.09     97.53     91.68\n",
      "     21       93.08     95.41     89.47     91.20     97.53     91.78\n",
      "     22       93.02     95.32     89.28     91.14     97.33     92.03\n",
      "     23       93.08     95.46     89.28     90.98     97.28     92.39\n",
      "     24       93.08     95.41     89.32     90.88     97.28     92.49\n",
      "     25       93.06     95.37     89.23     90.82     97.33     92.54\n",
      "     26       93.05     95.22     89.28     90.82     97.23     92.69\n",
      "     27       93.08     95.22     89.37     90.66     97.23     92.94\n",
      "     28       93.09     95.22     89.42     90.72     97.18     92.89\n",
      "     29       93.09     95.18     89.28     90.66     97.18     93.14\n",
      "     30       93.10     95.08     89.28     90.66     97.08     93.39\n",
      "     31       93.02     95.13     88.98     90.45     97.18     93.34\n",
      "     32       93.13     95.08     89.13     90.55     97.08     93.80\n",
      "     33       93.02     95.08     88.88     90.45     97.03     93.65\n",
      "     34       93.04     95.13     88.98     90.02     96.93     94.15\n",
      "     35       93.03     95.13     89.03     90.18     96.93     93.90\n",
      "     36       93.01     95.08     88.93     89.86     96.98     94.20\n",
      "     37       93.07     95.04     88.88     90.02     96.88     94.55\n",
      "     38       93.04     95.08     88.98     89.70     96.93     94.50\n",
      "     39       93.01     95.08     88.79     90.02     96.83     94.35\n",
      "     40       92.99     95.08     88.88     89.59     96.83     94.55\n",
      "     41       92.94     95.13     88.54     89.59     96.68     94.76\n",
      "     42       93.02     95.18     88.74     89.65     96.73     94.81\n",
      "     43       92.88     95.08     88.44     89.38     96.63     94.86\n",
      "     44       92.94     95.13     88.39     89.65     96.58     94.96\n",
      "     45       92.87     95.04     88.39     89.38     96.58     94.96\n",
      "     46       92.86     95.04     88.44     89.27     96.58     94.96\n",
      "     47       92.95     95.13     88.49     89.54     96.63     94.96\n",
      "     48       92.91     95.04     88.54     89.38     96.58     95.01\n",
      "     49       92.87     95.08     88.30     89.27     96.63     95.06\n",
      "     50       92.81     95.08     87.90     89.38     96.53     95.16\n",
      "     51       92.83     95.13     88.20     89.27     96.48     95.06\n",
      "     52       92.86     95.13     88.25     89.17     96.48     95.26\n",
      "     53       92.84     95.08     88.00     89.38     96.42     95.31\n",
      "     54       92.77     94.94     87.95     89.22     96.48     95.26\n",
      "     55       92.72     94.99     88.05     88.79     96.42     95.36\n",
      "     56       92.75     95.04     87.90     89.01     96.42     95.36\n",
      "     57       92.66     94.89     87.66     88.95     96.37     95.41\n",
      "     58       92.71     94.99     87.81     88.95     96.37     95.41\n",
      "     59       92.74     94.99     87.86     88.95     96.37     95.51\n",
      "     60       92.74     95.04     87.86     88.95     96.22     95.61\n",
      "     61       92.70     94.99     87.95     88.79     96.22     95.56\n",
      "     62       92.71     94.94     87.86     89.01     96.17     95.56\n",
      "     63       92.76     95.04     87.81     89.01     96.22     95.71\n",
      "     64       92.75     94.99     87.90     88.90     96.22     95.76\n",
      "     65       92.66     94.94     87.61     88.95     96.12     95.66\n",
      "     66       92.67     94.94     87.76     88.74     96.22     95.71\n",
      "     67       92.68     94.99     87.81     88.63     96.12     95.86\n",
      "     68       92.67     94.99     87.86     88.53     96.07     95.92\n",
      "     69       92.70     95.04     87.51     89.01     96.07     95.86\n",
      "     70       92.69     94.85     87.95     88.79     96.02     95.86\n",
      "     71       92.71     95.04     87.56     88.69     96.12     96.12\n",
      "     72       92.71     95.04     87.61     88.69     96.12     96.07\n",
      "     73       92.68     94.99     87.61     88.58     96.12     96.12\n",
      "     74       92.67     94.94     87.66     88.58     96.02     96.17\n",
      "     75       92.59     94.80     87.46     88.58     95.92     96.17\n",
      "     76       92.71     94.85     87.86     88.63     96.02     96.17\n",
      "     77       92.67     94.75     87.71     88.53     95.97     96.37\n",
      "     78       92.68     94.94     87.71     88.53     96.02     96.22\n",
      "     79       92.63     94.75     87.46     88.63     95.97     96.32\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9263 \n",
      "Accuracies (test): [0.9475, 0.8746, 0.8863, 0.9597, 0.9632]\n",
      "---\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n",
      "\n",
      "0.9262599999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fsvi_optimized.pkl\", task_sequence)\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-FSVI (no coreset) <a name=\"res3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 1181.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on clpc158.cs.ox.ac.uk\n",
      "Jax is running on gpu\n",
      "\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"command\":\"cl\",\n",
      "    \"data_training\":\"continual_learning_smnist_sh\",\n",
      "    \"data_ood\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"model_type\":\"fsvi_mlp\",\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":[\n",
      "        256,\n",
      "        256\n",
      "    ],\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0.0\",\n",
      "    \"prior_cov\":\"100.0\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":80,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0.0,\n",
      "    \"inducing_points\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"train_pixel_rand_1.0\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"equal\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":0.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":5,\n",
      "    \"tau\":1.0,\n",
      "    \"noise_std\":1.0,\n",
      "    \"ind_lim\":\"ind_-1_1\",\n",
      "    \"logging_frequency\":10,\n",
      "    \"figsize\":[\n",
      "        \"10\",\n",
      "        \"4\"\n",
      "    ],\n",
      "    \"seed\":12,\n",
      "    \"save\":false,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"not_specified\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"logroot\":\"ablation\",\n",
      "    \"subdir\":\"reproduce_main_results_2\",\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"n_inducing_inputs\":40,\n",
      "    \"n_inducing_inputs_first_task\":\"not_specified\",\n",
      "    \"n_inducing_inputs_second_task\":\"not_specified\",\n",
      "    \"inducing_inputs_bound\":[\n",
      "        0.0,\n",
      "        1.0\n",
      "    ],\n",
      "    \"fix_shuffle\":false,\n",
      "    \"use_generative_model\":false,\n",
      "    \"inducing_inputs_add_mode\":0,\n",
      "    \"inducing_input_adjustment\":false,\n",
      "    \"not_use_coreset\":true,\n",
      "    \"inducing_input_augmentation\":false,\n",
      "    \"plotting\":false,\n",
      "    \"logging\":1,\n",
      "    \"use_val_split\":false,\n",
      "    \"not_use_val_split\":true,\n",
      "    \"coreset\":\"random\",\n",
      "    \"coreset_entropy_mode\":\"soft_highest\",\n",
      "    \"coreset_entropy_offset\":\"0.0\",\n",
      "    \"coreset_kl_heuristic\":\"lowest\",\n",
      "    \"coreset_kl_offset\":\"0.0\",\n",
      "    \"coreset_elbo_heuristic\":\"lowest\",\n",
      "    \"coreset_elbo_offset\":\"0.0\",\n",
      "    \"coreset_elbo_n_samples\":5,\n",
      "    \"coreset_n_tasks\":\"not_specified\",\n",
      "    \"coreset_entropy_n_mixed\":1,\n",
      "    \"n_coreset_inputs_per_task\":\"not_specified\",\n",
      "    \"full_ntk\":false,\n",
      "    \"constant_inducing_points\":true,\n",
      "    \"n_permuted_tasks\":10,\n",
      "    \"n_omniglot_tasks\":20,\n",
      "    \"epochs_first_task\":80,\n",
      "    \"identity_cov\":false,\n",
      "    \"n_epochs_save_params\":\"not_specified\",\n",
      "    \"n_augment\":\"not_specified\",\n",
      "    \"augment_mode\":\"constant\",\n",
      "    \"n_valid\":\"same\",\n",
      "    \"learning_rate_first_task\":\"not_specified\",\n",
      "    \"save_alt\":true,\n",
      "    \"save_first_task\":false,\n",
      "    \"first_task_load_exp_path\":\"not_specified\",\n",
      "    \"only_task_id\":\"not_specified\",\n",
      "    \"loss_type\":1,\n",
      "    \"only_trainable_head\":false,\n",
      "    \"n_omniglot_coreset_chars\":2,\n",
      "    \"omniglot_randomize_test_split\":false,\n",
      "    \"omniglot_randomize_task_sequence\":false,\n",
      "    \"n_inducing_input_adjust_amount\":\"not_specified\",\n",
      "    \"save_all_params\":false,\n",
      "    \"task\":\"continual_learning_smnist_sh\",\n",
      "    \"architecture_arg\":\"fc_256_256\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (posterior): False\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "\n",
      "Learning task 1\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------\n",
      "  epoch    mean acc    t1 acc\n",
      "-------  ----------  --------\n",
      "      0       99.81     99.81\n",
      "      1       98.53     98.53\n",
      "      2       99.91     99.91\n",
      "      3       95.22     95.22\n",
      "      4       98.39     98.39\n",
      "      5       98.16     98.16\n",
      "      6       99.91     99.91\n",
      "      7       99.95     99.95\n",
      "      8       99.67     99.67\n",
      "      9       99.39     99.39\n",
      "     10       99.95     99.95\n",
      "     11       99.95     99.95\n",
      "     12       99.81     99.81\n",
      "     13       99.95     99.95\n",
      "     14       99.95     99.95\n",
      "     15       99.86     99.86\n",
      "     16       99.95     99.95\n",
      "     17       99.91     99.91\n",
      "     18       99.95     99.95\n",
      "     19       99.39     99.39\n",
      "     20       99.91     99.91\n",
      "     21       99.95     99.95\n",
      "     22       99.91     99.91\n",
      "     23       99.86     99.86\n",
      "     24       99.91     99.91\n",
      "     25       99.81     99.81\n",
      "     26       99.95     99.95\n",
      "     27       99.95     99.95\n",
      "     28       99.95     99.95\n",
      "     29       99.91     99.91\n",
      "     30       99.91     99.91\n",
      "     31       99.91     99.91\n",
      "     32       99.43     99.43\n",
      "     33       99.95     99.95\n",
      "     34       99.91     99.91\n",
      "     35       99.91     99.91\n",
      "     36       99.95     99.95\n",
      "     37       99.91     99.91\n",
      "     38       99.91     99.91\n",
      "     39       99.95     99.95\n",
      "     40       99.95     99.95\n",
      "     41       99.91     99.91\n",
      "     42       99.91     99.91\n",
      "     43       99.91     99.91\n",
      "     44       99.91     99.91\n",
      "     45       99.95     99.95\n",
      "     46       99.95     99.95\n",
      "     47       99.95     99.95\n",
      "     48       99.91     99.91\n",
      "     49       99.95     99.95\n",
      "     50       96.45     96.45\n",
      "     51      100.00    100.00\n",
      "     52       99.76     99.76\n",
      "     53       99.91     99.91\n",
      "     54       99.95     99.95\n",
      "     55       99.95     99.95\n",
      "     56       99.95     99.95\n",
      "     57       99.95     99.95\n",
      "     58       99.95     99.95\n",
      "     59       99.91     99.91\n",
      "     60       99.91     99.91\n",
      "     61       99.95     99.95\n",
      "     62       99.91     99.91\n",
      "     63       99.95     99.95\n",
      "     64       99.95     99.95\n",
      "     65       99.91     99.91\n",
      "     66       99.95     99.95\n",
      "     67       99.95     99.95\n",
      "     68       99.95     99.95\n",
      "     69       99.91     99.91\n",
      "     70       99.91     99.91\n",
      "     71       99.67     99.67\n",
      "     72       99.95     99.95\n",
      "     73       99.86     99.86\n",
      "     74       99.95     99.95\n",
      "     75      100.00    100.00\n",
      "     76       99.95     99.95\n",
      "     77       99.95     99.95\n",
      "     78       99.95     99.95\n",
      "     79      100.00    100.00\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 1.0000 \n",
      "Accuracies (test): [1.0]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 2\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc\n",
      "-------  ----------  --------  --------\n",
      "      0       77.41     86.90     67.92\n",
      "      1       48.21      0.05     96.38\n",
      "      2       63.55     34.75     92.36\n",
      "      3       73.10     51.73     94.47\n",
      "      4       36.96      4.73     69.20\n",
      "      5       59.19     91.58     26.79\n",
      "      6       57.25     17.59     96.91\n",
      "      7       63.27     57.78     68.76\n",
      "      8       52.63      7.38     97.89\n",
      "      9       49.17      0.05     98.29\n",
      "     10       54.07     10.78     97.36\n",
      "     11       58.43     22.74     94.12\n",
      "     12       54.65     10.97     98.33\n",
      "     13       49.06      0.24     97.89\n",
      "     14       56.65     19.95     93.34\n",
      "     15       50.78     18.16     83.40\n",
      "     16       49.70      0.52     98.87\n",
      "     17       50.24      1.56     98.92\n",
      "     18       49.92      0.76     99.07\n",
      "     19       51.87      4.87     98.87\n",
      "     20       63.30     50.73     75.86\n",
      "     21       52.88      7.61     98.14\n",
      "     22       60.86     27.94     93.78\n",
      "     23       40.20     13.85     66.55\n",
      "     24       57.79     18.77     96.82\n",
      "     25       59.22     22.65     95.79\n",
      "     26       51.17      4.54     97.80\n",
      "     27       54.50     11.25     97.75\n",
      "     28       59.89     21.80     97.99\n",
      "     29       62.42     28.18     96.67\n",
      "     30       51.33      3.74     98.92\n",
      "     31       49.53      5.96     93.10\n",
      "     32       56.41     49.55     63.27\n",
      "     33       54.02      9.03     99.02\n",
      "     34       60.51     24.35     96.67\n",
      "     35       49.92      0.57     99.27\n",
      "     36       50.09      0.76     99.41\n",
      "     37       56.21     26.86     85.55\n",
      "     38       50.13      0.76     99.51\n",
      "     39       51.09      3.07     99.12\n",
      "     40       52.92      6.57     99.27\n",
      "     41       59.58     22.93     96.23\n",
      "     42       49.44      0.00     98.87\n",
      "     43       49.97      0.28     99.66\n",
      "     44       49.85      0.05     99.66\n",
      "     45       49.90      0.19     99.61\n",
      "     46       53.70      8.37     99.02\n",
      "     47       51.20      3.22     99.17\n",
      "     48       50.42      2.65     98.19\n",
      "     49       49.83      0.00     99.66\n",
      "     50       49.80      0.00     99.61\n",
      "     51       68.23     45.53     90.94\n",
      "     52       49.80      0.00     99.61\n",
      "     53       49.85      0.05     99.66\n",
      "     54       49.83      0.00     99.66\n",
      "     55       49.63      0.00     99.27\n",
      "     56       49.80      0.00     99.61\n",
      "     57       49.78      0.00     99.56\n",
      "     58       49.59      0.00     99.17\n",
      "     59       49.56      1.23     97.89\n",
      "     60       49.83      0.00     99.66\n",
      "     61       49.83      0.00     99.66\n",
      "     62       49.85      0.00     99.71\n",
      "     63       49.78      0.00     99.56\n",
      "     64       49.83      0.00     99.66\n",
      "     65       49.88      0.00     99.76\n",
      "     66       49.83      0.00     99.66\n",
      "     67       49.16      0.00     98.33\n",
      "     68       48.31      0.00     96.62\n",
      "     69       49.66      0.00     99.31\n",
      "     70       49.83      0.00     99.66\n",
      "     71       48.66      0.00     97.31\n",
      "     72       49.61      0.00     99.22\n",
      "     73       49.76      0.00     99.51\n",
      "     74       49.70      0.00     99.41\n",
      "     75       49.56      0.00     99.12\n",
      "     76       49.85      0.00     99.71\n",
      "     77       49.78      0.00     99.56\n",
      "     78       49.83      0.00     99.66\n",
      "     79       53.09      6.81     99.36\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.5309 \n",
      "Accuracies (test): [0.0681, 0.9936]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 3\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc\n",
      "-------  ----------  --------  --------  --------\n",
      "      0       44.04     27.04     22.04     83.03\n",
      "      1       34.48     12.72      0.64     90.07\n",
      "      2       30.09      0.00      0.10     90.18\n",
      "      3       33.59      1.37      0.10     99.31\n",
      "      4       32.43      0.00      0.29     97.01\n",
      "      5       36.95      6.38      4.75     99.73\n",
      "      6       33.18      0.76      0.05     98.72\n",
      "      7       32.96      0.00      0.05     98.83\n",
      "      8       33.28      0.00      0.00     99.84\n",
      "      9       33.12      0.00      0.00     99.36\n",
      "     10       32.84      0.00      0.00     98.51\n",
      "     11       33.28      0.00      0.00     99.84\n",
      "     12       41.34     26.38      0.10     97.55\n",
      "     13       35.37      6.95      0.00     99.15\n",
      "     14       33.30      0.00      0.00     99.89\n",
      "     15       33.19      0.00      0.00     99.57\n",
      "     16       32.96      0.00      0.00     98.88\n",
      "     17       33.30      0.00      0.00     99.89\n",
      "     18       33.26      0.00      0.00     99.79\n",
      "     19       33.28      0.00      0.00     99.84\n",
      "     20       35.83      7.61      0.00     99.89\n",
      "     21       33.58      0.85      0.00     99.89\n",
      "     22       49.13     47.71      0.00     99.68\n",
      "     23       33.24      0.00      0.00     99.73\n",
      "     24       33.08      0.00      0.00     99.25\n",
      "     25       33.24      0.00      0.00     99.73\n",
      "     26       33.30      0.00      0.00     99.89\n",
      "     27       33.30      0.00      0.00     99.89\n",
      "     28       33.28      0.28      0.00     99.57\n",
      "     29       33.30      0.00      0.00     99.89\n",
      "     30       33.26      0.00      0.00     99.79\n",
      "     31       34.19      2.74      0.00     99.84\n",
      "     32       33.16      0.00      0.00     99.47\n",
      "     33       33.30      0.00      0.00     99.89\n",
      "     34       33.28      0.00      0.00     99.84\n",
      "     35       32.89      0.00      0.00     98.67\n",
      "     36       33.30      0.00      0.00     99.89\n",
      "     37       33.30      0.00      0.00     99.89\n",
      "     38       32.98      0.00      0.00     98.93\n",
      "     39       33.32      0.00      0.00     99.95\n",
      "     40       37.77     13.52      0.00     99.79\n",
      "     41       33.26      0.00      0.00     99.79\n",
      "     42       33.30      0.00      0.00     99.89\n",
      "     43       33.32      0.00      0.00     99.95\n",
      "     44       33.32      0.00      0.00     99.95\n",
      "     45       33.28      0.00      0.00     99.84\n",
      "     46       33.26      0.00      0.00     99.79\n",
      "     47       33.32      0.00      0.00     99.95\n",
      "     48       33.30      0.00      0.00     99.89\n",
      "     49       33.30      0.00      0.00     99.89\n",
      "     50       33.36      0.19      0.00     99.89\n",
      "     51       33.32      0.00      0.00     99.95\n",
      "     52       33.30      0.05      0.00     99.84\n",
      "     53       33.32      0.00      0.00     99.95\n",
      "     54       35.00      5.11      0.00     99.89\n",
      "     55       33.30      0.00      0.00     99.89\n",
      "     56       33.32      0.00      0.00     99.95\n",
      "     57       33.32      0.00      0.00     99.95\n",
      "     58       33.35      0.09      0.00     99.95\n",
      "     59       33.32      0.00      0.00     99.95\n",
      "     60       36.89     10.73      0.00     99.95\n",
      "     61       35.73      7.23      0.00     99.95\n",
      "     62       33.30      0.00      0.00     99.89\n",
      "     63       33.24      0.00      0.00     99.73\n",
      "     64       33.28      0.00      0.00     99.84\n",
      "     65       33.28      0.05      0.00     99.79\n",
      "     66       33.28      0.00      0.00     99.84\n",
      "     67       33.30      0.00      0.00     99.89\n",
      "     68       33.30      0.00      0.00     99.89\n",
      "     69       33.32      0.00      0.00     99.95\n",
      "     70       34.40      3.36      0.00     99.84\n",
      "     71       33.30      0.00      0.00     99.89\n",
      "     72       33.32      0.00      0.00     99.95\n",
      "     73       33.38      0.19      0.00     99.95\n",
      "     74       33.30      0.00      0.00     99.89\n",
      "     75       33.32      0.00      0.00     99.95\n",
      "     76       33.30      0.00      0.00     99.89\n",
      "     77       33.28      0.00      0.00     99.84\n",
      "     78       33.77      1.42      0.00     99.89\n",
      "     79       33.30      0.00      0.00     99.89\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.3330 \n",
      "Accuracies (test): [0.0, 0.0, 0.9989]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 4\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "      0       27.53     11.44      0.00      0.00     98.69\n",
      "      1       27.03     19.62      0.00      0.00     88.52\n",
      "      2       25.64      3.45      0.00      0.00     99.09\n",
      "      3       24.91      0.00      0.00      0.00     99.65\n",
      "      4       24.94      0.00      0.00      0.00     99.75\n",
      "      5       24.91      0.00      0.00      0.00     99.65\n",
      "      6       24.80      0.00      0.00      0.00     99.19\n",
      "      7       25.33      1.65      0.00      0.00     99.65\n",
      "      8       24.93      0.00      0.00      0.00     99.70\n",
      "      9       24.94      0.00      0.00      0.00     99.75\n",
      "     10       24.93      0.00      0.00      0.00     99.70\n",
      "     11       24.96      0.00      0.00      0.00     99.85\n",
      "     12       24.94      0.00      0.00      0.00     99.75\n",
      "     13       24.94      0.00      0.00      0.00     99.75\n",
      "     14       24.96      0.00      0.00      0.00     99.85\n",
      "     15       24.96      0.00      0.00      0.00     99.85\n",
      "     16       25.52      2.27      0.00      0.00     99.80\n",
      "     17       24.93      0.00      0.00      0.00     99.70\n",
      "     18       26.37      5.77      0.00      0.00     99.70\n",
      "     19       24.93      0.00      0.00      0.00     99.70\n",
      "     20       25.00      0.05      0.00      0.00     99.95\n",
      "     21       24.96      0.00      0.00      0.00     99.85\n",
      "     22       24.95      0.00      0.00      0.00     99.80\n",
      "     23       24.96      0.00      0.00      0.00     99.85\n",
      "     24       24.93      0.00      0.00      0.00     99.70\n",
      "     25       24.99      0.00      0.00      0.00     99.95\n",
      "     26       27.39      9.69      0.00      0.00     99.85\n",
      "     27       24.96      0.00      0.00      0.00     99.85\n",
      "     28       24.96      0.00      0.00      0.00     99.85\n",
      "     29       24.99      0.00      0.00      0.00     99.95\n",
      "     30       24.95      0.00      0.00      0.00     99.80\n",
      "     31       25.00      0.05      0.00      0.00     99.95\n",
      "     32       24.95      0.14      0.00      0.00     99.65\n",
      "     33       24.91      0.00      0.00      0.00     99.65\n",
      "     34       24.99      0.00      0.00      0.00     99.95\n",
      "     35       24.93      0.00      0.00      0.00     99.70\n",
      "     36       24.95      0.00      0.00      0.00     99.80\n",
      "     37       24.96      0.00      0.00      0.00     99.85\n",
      "     38       25.47      1.99      0.00      0.00     99.90\n",
      "     39       24.99      0.00      0.00      0.00     99.95\n",
      "     40       24.91      0.00      0.00      0.00     99.65\n",
      "     41       24.95      0.00      0.00      0.00     99.80\n",
      "     42       25.94      3.92      0.00      0.00     99.85\n",
      "     43       29.93     20.00      0.00      0.00     99.70\n",
      "     44       24.91      0.00      0.00      0.00     99.65\n",
      "     45       24.98      0.00      0.00      0.00     99.90\n",
      "     46       24.91      0.00      0.00      0.00     99.65\n",
      "     47       24.99      0.00      0.00      0.00     99.95\n",
      "     48       25.01      0.24      0.00      0.00     99.80\n",
      "     49       26.99      8.09      0.00      0.00     99.85\n",
      "     50       24.96      0.00      0.00      0.00     99.85\n",
      "     51       25.04      0.52      0.00      0.00     99.65\n",
      "     52       24.95      0.00      0.00      0.00     99.80\n",
      "     53       24.96      0.14      0.00      0.00     99.70\n",
      "     54       25.41      1.84      0.00      0.00     99.80\n",
      "     55       24.94      0.00      0.00      0.00     99.75\n",
      "     56       24.98      0.00      0.00      0.00     99.90\n",
      "     57       25.38      1.56      0.00      0.00     99.95\n",
      "     58       24.98      0.00      0.00      0.00     99.90\n",
      "     59       24.99      0.00      0.00      0.00     99.95\n",
      "     60       24.98      0.00      0.00      0.00     99.90\n",
      "     61       24.96      0.00      0.00      0.00     99.85\n",
      "     62       25.02      0.14      0.00      0.00     99.95\n",
      "     63       24.98      0.00      0.00      0.00     99.90\n",
      "     64       25.02      0.19      0.00      0.00     99.90\n",
      "     65       24.95      0.00      0.00      0.00     99.80\n",
      "     66       24.93      0.00      0.00      0.00     99.70\n",
      "     67       25.55      2.32      0.00      0.00     99.90\n",
      "     68       26.70      6.90      0.00      0.00     99.90\n",
      "     69       25.54      2.36      0.00      0.00     99.80\n",
      "     70       24.99      0.00      0.00      0.00     99.95\n",
      "     71       25.40      1.70      0.00      0.00     99.90\n",
      "     72       29.03     16.26      0.00      0.00     99.85\n",
      "     73       24.98      0.00      0.00      0.00     99.90\n",
      "     74       25.01      0.09      0.00      0.00     99.95\n",
      "     75       28.13     12.67      0.00      0.00     99.85\n",
      "     76       26.37      5.58      0.00      0.00     99.90\n",
      "     77       25.04      0.24      0.00      0.00     99.90\n",
      "     78       27.76     11.25      0.00      0.00     99.80\n",
      "     79       24.95      0.00      0.00      0.00     99.80\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.2495 \n",
      "Accuracies (test): [0.0, 0.0, 0.0, 0.998]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 5\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "      0       27.64     46.57      0.20      0.00      0.20     91.23\n",
      "      1       20.67      5.67      0.00      0.00      0.00     97.68\n",
      "      2       19.94      2.08      0.44      0.00      0.00     97.18\n",
      "      3       19.70      0.00      0.00      0.00      0.00     98.49\n",
      "      4       19.39      0.05      0.00      0.00      0.00     96.92\n",
      "      5       20.41      4.40      0.00      0.00      0.00     97.63\n",
      "      6       19.54      0.00      0.00      0.00      0.00     97.68\n",
      "      7       22.80     15.37      0.00      0.00      0.00     98.64\n",
      "      8       20.08      1.75      0.00      0.00      0.00     98.64\n",
      "      9       20.14      1.84      0.00      0.00      0.00     98.84\n",
      "     10       22.08     11.68      0.00      0.00      0.00     98.74\n",
      "     11       24.03     21.18      0.00      0.00      0.00     98.99\n",
      "     12       19.81      0.00      0.00      0.00      0.00     99.04\n",
      "     13       19.71      0.00      0.00      0.00      0.00     98.54\n",
      "     14       19.96      0.76      0.00      0.00      0.00     99.04\n",
      "     15       19.85      0.00      0.00      0.00      0.00     99.24\n",
      "     16       19.85      0.00      0.00      0.00      0.00     99.24\n",
      "     17       19.82      0.05      0.00      0.00      0.00     99.04\n",
      "     18       20.68      4.30      0.00      0.00      0.00     99.09\n",
      "     19       19.84      0.00      0.00      0.00      0.00     99.19\n",
      "     20       19.89      0.14      0.00      0.00      0.00     99.29\n",
      "     21       20.49      3.07      0.00      0.00      0.00     99.39\n",
      "     22       19.84      0.00      0.00      0.00      0.00     99.19\n",
      "     23       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     24       19.85      0.00      0.00      0.00      0.00     99.24\n",
      "     25       19.80      0.00      0.00      0.00      0.00     98.99\n",
      "     26       19.82      0.00      0.00      0.00      0.00     99.09\n",
      "     27       19.84      0.00      0.00      0.00      0.00     99.19\n",
      "     28       19.84      0.00      0.00      0.00      0.00     99.19\n",
      "     29       19.78      0.00      0.00      0.00      0.00     98.89\n",
      "     30       19.74      0.00      0.00      0.00      0.00     98.69\n",
      "     31       20.01      0.76      0.00      0.00      0.00     99.29\n",
      "     32       19.78      0.00      0.00      0.00      0.00     98.89\n",
      "     33       19.83      0.00      0.00      0.00      0.00     99.14\n",
      "     34       21.38      7.47      0.00      0.00      0.00     99.45\n",
      "     35       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     36       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     37       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     38       20.78      4.63      0.00      0.00      0.00     99.29\n",
      "     39       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     40       19.90      0.14      0.00      0.00      0.00     99.34\n",
      "     41       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     42       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     43       19.86      0.00      0.00      0.00      0.00     99.29\n",
      "     44       19.89      0.05      0.00      0.00      0.00     99.39\n",
      "     45       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     46       19.80      0.00      0.00      0.00      0.00     98.99\n",
      "     47       19.90      0.00      0.00      0.00      0.00     99.50\n",
      "     48       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     49       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     50       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     51       19.89      0.00      0.00      0.00      0.00     99.45\n",
      "     52       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     53       20.14      1.51      0.00      0.00      0.00     99.19\n",
      "     54       19.89      0.05      0.00      0.00      0.00     99.39\n",
      "     55       19.89      0.00      0.00      0.00      0.00     99.45\n",
      "     56       20.59      3.55      0.10      0.00      0.00     99.29\n",
      "     57       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     58       21.71      9.08      0.00      0.00      0.00     99.45\n",
      "     59       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     60       22.05     10.87      0.00      0.00      0.00     99.39\n",
      "     61       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     62       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     63       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     64       19.86      0.00      0.00      0.00      0.00     99.29\n",
      "     65       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     66       19.89      0.05      0.00      0.00      0.00     99.39\n",
      "     67       20.77      4.44      0.00      0.00      0.00     99.39\n",
      "     68       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "     69       19.89      0.00      0.00      0.00      0.00     99.45\n",
      "     70       19.89      0.00      0.00      0.00      0.00     99.45\n",
      "     71       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     72       19.85      0.00      0.00      0.00      0.00     99.24\n",
      "     73       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     74       19.85      0.00      0.00      0.00      0.00     99.24\n",
      "     75       19.90      0.05      0.00      0.00      0.00     99.45\n",
      "     76       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     77       19.88      0.00      0.00      0.00      0.00     99.39\n",
      "     78       23.21     16.60      0.00      0.00      0.00     99.45\n",
      "     79       19.87      0.00      0.00      0.00      0.00     99.34\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.1987 \n",
      "Accuracies (test): [0.0, 0.0, 0.0, 0.0, 0.9934]\n",
      "---\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n",
      "\n",
      "0.19868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fsvi_no_coreset.pkl\", task_sequence)\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-FSVI (minimal coreset) <a name=\"res4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 1195.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on clpc158.cs.ox.ac.uk\n",
      "Jax is running on gpu\n",
      "\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"command\":\"cl\",\n",
      "    \"data_training\":\"continual_learning_smnist_sh\",\n",
      "    \"data_ood\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"model_type\":\"fsvi_mlp\",\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":[\n",
      "        300,\n",
      "        300\n",
      "    ],\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0.0\",\n",
      "    \"prior_cov\":\"10.0\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":80,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.0005,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0.0,\n",
      "    \"inducing_points\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"train_pixel_rand_0.0\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"equal\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":0.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":5,\n",
      "    \"tau\":1.0,\n",
      "    \"noise_std\":1.0,\n",
      "    \"ind_lim\":\"ind_-1_1\",\n",
      "    \"logging_frequency\":10,\n",
      "    \"figsize\":[\n",
      "        \"10\",\n",
      "        \"4\"\n",
      "    ],\n",
      "    \"seed\":4,\n",
      "    \"save\":false,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":false,\n",
      "    \"kl_sup\":\"not_specified\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"logroot\":\"ablation\",\n",
      "    \"subdir\":\"reproduce_main_results_2\",\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"n_inducing_inputs\":10,\n",
      "    \"n_inducing_inputs_first_task\":\"not_specified\",\n",
      "    \"n_inducing_inputs_second_task\":\"not_specified\",\n",
      "    \"inducing_inputs_bound\":[\n",
      "        0.0,\n",
      "        1.0\n",
      "    ],\n",
      "    \"fix_shuffle\":false,\n",
      "    \"use_generative_model\":false,\n",
      "    \"inducing_inputs_add_mode\":0,\n",
      "    \"inducing_input_adjustment\":false,\n",
      "    \"not_use_coreset\":false,\n",
      "    \"inducing_input_augmentation\":false,\n",
      "    \"plotting\":false,\n",
      "    \"logging\":1,\n",
      "    \"use_val_split\":false,\n",
      "    \"not_use_val_split\":true,\n",
      "    \"coreset\":\"random_per_class\",\n",
      "    \"coreset_entropy_mode\":\"soft_highest\",\n",
      "    \"coreset_entropy_offset\":\"0.0\",\n",
      "    \"coreset_kl_heuristic\":\"lowest\",\n",
      "    \"coreset_kl_offset\":\"0.0\",\n",
      "    \"coreset_elbo_heuristic\":\"lowest\",\n",
      "    \"coreset_elbo_offset\":\"0.0\",\n",
      "    \"coreset_elbo_n_samples\":5,\n",
      "    \"coreset_n_tasks\":\"not_specified\",\n",
      "    \"coreset_entropy_n_mixed\":1,\n",
      "    \"n_coreset_inputs_per_task\":\"10\",\n",
      "    \"full_ntk\":false,\n",
      "    \"constant_inducing_points\":false,\n",
      "    \"n_permuted_tasks\":10,\n",
      "    \"n_omniglot_tasks\":20,\n",
      "    \"epochs_first_task\":80,\n",
      "    \"identity_cov\":false,\n",
      "    \"n_epochs_save_params\":\"not_specified\",\n",
      "    \"n_augment\":\"not_specified\",\n",
      "    \"augment_mode\":\"constant\",\n",
      "    \"n_valid\":\"same\",\n",
      "    \"learning_rate_first_task\":\"not_specified\",\n",
      "    \"save_alt\":true,\n",
      "    \"save_first_task\":false,\n",
      "    \"first_task_load_exp_path\":\"not_specified\",\n",
      "    \"only_task_id\":\"not_specified\",\n",
      "    \"loss_type\":1,\n",
      "    \"only_trainable_head\":false,\n",
      "    \"n_omniglot_coreset_chars\":2,\n",
      "    \"omniglot_randomize_test_split\":false,\n",
      "    \"omniglot_randomize_task_sequence\":false,\n",
      "    \"n_inducing_input_adjust_amount\":\"not_specified\",\n",
      "    \"save_all_params\":false,\n",
      "    \"task\":\"continual_learning_smnist_sh\",\n",
      "    \"architecture_arg\":\"fc_300_300\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (posterior): False\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "\n",
      "Learning task 1\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------\n",
      "  epoch    mean acc    t1 acc\n",
      "-------  ----------  --------\n",
      "      0       99.95     99.95\n",
      "      1       99.95     99.95\n",
      "      2       99.91     99.91\n",
      "      3       99.95     99.95\n",
      "      4       99.95     99.95\n",
      "      5       99.91     99.91\n",
      "      6       99.95     99.95\n",
      "      7       99.95     99.95\n",
      "      8       99.91     99.91\n",
      "      9       99.95     99.95\n",
      "     10       99.95     99.95\n",
      "     11       99.95     99.95\n",
      "     12       99.91     99.91\n",
      "     13       99.95     99.95\n",
      "     14       99.95     99.95\n",
      "     15       99.95     99.95\n",
      "     16       99.95     99.95\n",
      "     17       99.95     99.95\n",
      "     18       99.95     99.95\n",
      "     19       99.95     99.95\n",
      "     20       99.95     99.95\n",
      "     21       99.95     99.95\n",
      "     22       99.95     99.95\n",
      "     23       99.95     99.95\n",
      "     24       99.95     99.95\n",
      "     25       99.95     99.95\n",
      "     26       99.95     99.95\n",
      "     27       99.95     99.95\n",
      "     28       99.95     99.95\n",
      "     29       99.95     99.95\n",
      "     30       99.95     99.95\n",
      "     31       99.95     99.95\n",
      "     32       99.95     99.95\n",
      "     33       99.95     99.95\n",
      "     34       99.95     99.95\n",
      "     35      100.00    100.00\n",
      "     36       99.95     99.95\n",
      "     37       99.95     99.95\n",
      "     38       99.95     99.95\n",
      "     39       99.95     99.95\n",
      "     40       99.95     99.95\n",
      "     41       99.95     99.95\n",
      "     42      100.00    100.00\n",
      "     43       99.95     99.95\n",
      "     44      100.00    100.00\n",
      "     45       99.95     99.95\n",
      "     46       99.95     99.95\n",
      "     47       99.95     99.95\n",
      "     48      100.00    100.00\n",
      "     49       99.95     99.95\n",
      "     50       99.95     99.95\n",
      "     51       99.95     99.95\n",
      "     52      100.00    100.00\n",
      "     53      100.00    100.00\n",
      "     54      100.00    100.00\n",
      "     55      100.00    100.00\n",
      "     56      100.00    100.00\n",
      "     57       99.95     99.95\n",
      "     58      100.00    100.00\n",
      "     59      100.00    100.00\n",
      "     60      100.00    100.00\n",
      "     61      100.00    100.00\n",
      "     62      100.00    100.00\n",
      "     63       99.95     99.95\n",
      "     64      100.00    100.00\n",
      "     65      100.00    100.00\n",
      "     66       99.95     99.95\n",
      "     67      100.00    100.00\n",
      "     68       99.95     99.95\n",
      "     69       99.95     99.95\n",
      "     70       99.95     99.95\n",
      "     71      100.00    100.00\n",
      "     72       99.95     99.95\n",
      "     73       99.95     99.95\n",
      "     74      100.00    100.00\n",
      "     75       99.95     99.95\n",
      "     76       99.95     99.95\n",
      "     77       99.95     99.95\n",
      "     78       99.95     99.95\n",
      "     79       99.95     99.95\n",
      "Adding inducing inputs to the coreset randomly, 10 samples in total, with each class getting the same number of samples\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9995 \n",
      "Accuracies (test): [0.9995]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 2\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc\n",
      "-------  ----------  --------  --------\n",
      "      0       92.92     88.89     96.96\n",
      "      1       92.61     87.90     97.31\n",
      "      2       91.94     86.00     97.89\n",
      "      3       91.81     85.25     98.38\n",
      "      4       91.80     85.01     98.58\n",
      "      5       91.78     84.68     98.87\n",
      "      6       91.42     83.92     98.92\n",
      "      7       91.47     83.78     99.17\n",
      "      8       90.77     82.22     99.31\n",
      "      9       90.70     82.22     99.17\n",
      "     10       90.46     81.51     99.41\n",
      "     11       90.65     81.99     99.31\n",
      "     12       91.17     83.07     99.27\n",
      "     13       90.20     81.09     99.31\n",
      "     14       89.63     80.14     99.12\n",
      "     15       89.75     80.14     99.36\n",
      "     16       90.50     81.89     99.12\n",
      "     17       89.39     79.57     99.22\n",
      "     18       90.50     81.70     99.31\n",
      "     19       89.57     79.67     99.46\n",
      "     20       89.97     80.66     99.27\n",
      "     21       89.85     80.19     99.51\n",
      "     22       89.70     79.95     99.46\n",
      "     23       89.27     78.87     99.66\n",
      "     24       89.64     79.72     99.56\n",
      "     25       89.14     78.72     99.56\n",
      "     26       88.59     77.97     99.22\n",
      "     27       89.41     79.20     99.61\n",
      "     28       88.50     77.45     99.56\n",
      "     29       89.78     79.95     99.61\n",
      "     30       88.98     78.39     99.56\n",
      "     31       89.00     78.72     99.27\n",
      "     32       89.38     79.15     99.61\n",
      "     33       89.22     78.87     99.56\n",
      "     34       88.91     78.20     99.61\n",
      "     35       88.96     78.30     99.61\n",
      "     36       88.43     77.35     99.51\n",
      "     37       89.07     78.63     99.51\n",
      "     38       89.69     79.72     99.66\n",
      "     39       90.20     80.85     99.56\n",
      "     40       88.53     77.40     99.66\n",
      "     41       88.55     77.49     99.61\n",
      "     42       89.03     78.49     99.56\n",
      "     43       89.90     80.19     99.61\n",
      "     44       87.87     76.17     99.56\n",
      "     45       89.94     80.28     99.61\n",
      "     46       89.57     79.53     99.61\n",
      "     47       89.08     78.49     99.66\n",
      "     48       89.38     79.15     99.61\n",
      "     49       88.98     78.30     99.66\n",
      "     50       86.47     73.33     99.61\n",
      "     51       87.94     76.26     99.61\n",
      "     52       89.92     80.19     99.66\n",
      "     53       87.94     76.26     99.61\n",
      "     54       87.34     75.13     99.56\n",
      "     55       88.67     77.78     99.56\n",
      "     56       87.86     76.12     99.61\n",
      "     57       88.34     77.12     99.56\n",
      "     58       88.55     77.49     99.61\n",
      "     59       89.05     78.49     99.61\n",
      "     60       87.77     75.93     99.61\n",
      "     61       88.50     77.35     99.66\n",
      "     62       88.34     77.12     99.56\n",
      "     63       89.05     78.53     99.56\n",
      "     64       88.39     77.16     99.61\n",
      "     65       89.78     79.95     99.61\n",
      "     66       86.92     74.23     99.61\n",
      "     67       87.70     75.79     99.61\n",
      "     68       89.03     78.49     99.56\n",
      "     69       87.34     75.08     99.61\n",
      "     70       89.05     78.49     99.61\n",
      "     71       87.67     75.74     99.61\n",
      "     72       88.53     77.45     99.61\n",
      "     73       88.11     76.60     99.61\n",
      "     74       88.83     78.01     99.66\n",
      "     75       88.15     76.64     99.66\n",
      "     76       87.59     75.56     99.61\n",
      "     77       89.31     79.01     99.61\n",
      "     78       88.17     76.74     99.61\n",
      "     79       86.00     72.29     99.71\n",
      "Adding inducing inputs to the coreset randomly, 10 samples in total, with each class getting the same number of samples\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8600 \n",
      "Accuracies (test): [0.7229, 0.9971]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 3\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc\n",
      "-------  ----------  --------  --------  --------\n",
      "      0       82.95     78.30     72.14     98.40\n",
      "      1       80.88     75.41     67.97     99.25\n",
      "      2       78.92     73.19     64.01     99.57\n",
      "      3       78.29     73.10     62.19     99.57\n",
      "      4       78.56     72.10     64.01     99.57\n",
      "      5       77.41     71.54     61.02     99.68\n",
      "      6       76.48     70.26     59.35     99.84\n",
      "      7       76.38     71.06     58.28     99.79\n",
      "      8       75.75     69.69     57.84     99.73\n",
      "      9       76.68     71.54     58.72     99.79\n",
      "     10       76.80     70.83     59.84     99.73\n",
      "     11       76.32     68.65     60.48     99.84\n",
      "     12       76.92     71.96     59.06     99.73\n",
      "     13       74.62     67.85     56.12     99.89\n",
      "     14       76.00     70.07     58.13     99.79\n",
      "     15       76.10     71.06     57.44     99.79\n",
      "     16       75.78     69.22     58.28     99.84\n",
      "     17       76.40     70.40     59.11     99.68\n",
      "     18       75.98     71.35     56.81     99.79\n",
      "     19       76.56     70.12     59.89     99.68\n",
      "     20       75.90     70.26     57.64     99.79\n",
      "     21       75.89     70.35     57.59     99.73\n",
      "     22       75.91     70.26     57.64     99.84\n",
      "     23       76.34     72.67     56.61     99.73\n",
      "     24       77.08     72.15     59.40     99.68\n",
      "     25       75.16     70.31     55.44     99.73\n",
      "     26       76.20     72.81     56.07     99.73\n",
      "     27       75.61     69.65     57.39     99.79\n",
      "     28       75.15     68.46     57.20     99.79\n",
      "     29       76.42     70.26     59.26     99.73\n",
      "     30       75.46     69.50     57.05     99.84\n",
      "     31       75.80     71.25     56.42     99.73\n",
      "     32       74.84     68.27     56.42     99.84\n",
      "     33       76.29     70.17     58.91     99.79\n",
      "     34       76.43     71.16     58.28     99.84\n",
      "     35       75.30     69.93     56.17     99.79\n",
      "     36       76.15     70.78     57.98     99.68\n",
      "     37       76.18     70.45     58.42     99.68\n",
      "     38       76.50     72.62     57.10     99.79\n",
      "     39       76.16     70.07     58.57     99.84\n",
      "     40       76.13     69.69     58.96     99.73\n",
      "     41       76.12     70.59     57.98     99.79\n",
      "     42       76.57     69.65     60.33     99.73\n",
      "     43       75.70     69.88     57.49     99.73\n",
      "     44       75.94     70.12     57.98     99.73\n",
      "     45       76.25     72.06     56.81     99.89\n",
      "     46       75.92     69.50     58.57     99.68\n",
      "     47       74.54     68.65     55.24     99.73\n",
      "     48       75.81     69.41     58.28     99.73\n",
      "     49       75.69     68.98     58.37     99.73\n",
      "     50       75.72     70.07     57.30     99.79\n",
      "     51       76.56     71.82     58.13     99.73\n",
      "     52       75.96     70.17     57.98     99.73\n",
      "     53       75.35     71.16     55.00     99.89\n",
      "     54       75.85     70.02     57.64     99.89\n",
      "     55       75.83     70.54     57.15     99.79\n",
      "     56       75.67     70.02     57.15     99.84\n",
      "     57       75.68     69.27     57.98     99.79\n",
      "     58       76.48     72.01     57.69     99.73\n",
      "     59       76.61     70.26     59.84     99.73\n",
      "     60       76.34     70.50     58.67     99.84\n",
      "     61       75.81     68.79     58.81     99.84\n",
      "     62       75.32     68.89     57.35     99.73\n",
      "     63       75.66     69.88     57.30     99.79\n",
      "     64       75.85     68.61     59.11     99.84\n",
      "     65       75.79     69.31     58.18     99.89\n",
      "     66       76.19     70.69     58.03     99.84\n",
      "     67       76.85     71.82     59.01     99.73\n",
      "     68       76.38     70.97     58.28     99.89\n",
      "     69       75.56     68.70     58.08     99.89\n",
      "     70       75.42     67.90     58.52     99.84\n",
      "     71       76.54     70.83     59.06     99.73\n",
      "     72       75.77     69.22     58.42     99.68\n",
      "     73       76.24     69.50     59.50     99.73\n",
      "     74       75.54     69.93     56.90     99.79\n",
      "     75       76.91     71.54     59.40     99.79\n",
      "     76       76.63     69.98     60.19     99.73\n",
      "     77       75.98     70.40     57.69     99.84\n",
      "     78       76.37     70.07     59.30     99.73\n",
      "     79       75.43     69.36     57.10     99.84\n",
      "Adding inducing inputs to the coreset randomly, 10 samples in total, with each class getting the same number of samples\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7543 \n",
      "Accuracies (test): [0.6936, 0.571, 0.9984]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 4\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "      0       69.08     68.98     55.29     53.63     98.44\n",
      "      1       65.03     63.97     48.53     48.45     99.19\n",
      "      2       63.32     62.55     46.72     44.66     99.35\n",
      "      3       62.91     62.27     47.45     42.37     99.55\n",
      "      4       62.41     61.94     44.96     43.17     99.55\n",
      "      5       62.01     60.90     45.10     42.58     99.45\n",
      "      6       60.66     57.92     44.17     40.88     99.65\n",
      "      7       60.06     57.59     44.37     38.63     99.65\n",
      "      8       60.57     59.53     43.54     39.59     99.60\n",
      "      9       61.04     59.01     45.25     40.29     99.60\n",
      "     10       58.88     56.12     42.51     37.25     99.65\n",
      "     11       60.42     57.97     45.05     39.06     99.60\n",
      "     12       60.43     60.19     43.44     38.42     99.65\n",
      "     13       60.09     56.88     43.93     39.91     99.65\n",
      "     14       59.94     58.77     44.37     36.93     99.70\n",
      "     15       59.27     57.16     42.07     38.21     99.65\n",
      "     16       59.31     56.55     42.51     38.53     99.65\n",
      "     17       60.70     59.43     44.76     38.95     99.65\n",
      "     18       59.18     55.79     42.21     39.06     99.65\n",
      "     19       60.69     60.05     44.17     38.95     99.60\n",
      "     20       59.69     57.21     44.32     37.57     99.65\n",
      "     21       58.36     55.98     40.89     36.93     99.65\n",
      "     22       59.60     58.39     42.41     37.94     99.65\n",
      "     23       58.27     54.75     41.19     37.51     99.65\n",
      "     24       59.16     56.26     42.80     37.94     99.65\n",
      "     25       58.67     55.79     42.07     37.19     99.65\n",
      "     26       59.45     57.73     42.85     37.62     99.60\n",
      "     27       59.76     57.12     43.73     38.53     99.65\n",
      "     28       59.17     57.59     41.97     37.41     99.70\n",
      "     29       60.09     58.11     43.54     39.17     99.55\n",
      "     30       60.37     59.67     44.12     38.05     99.65\n",
      "     31       59.39     56.45     42.95     38.53     99.65\n",
      "     32       59.93     56.78     44.81     38.58     99.55\n",
      "     33       59.79     56.64     43.98     38.95     99.60\n",
      "     34       59.08     55.84     42.80     37.99     99.70\n",
      "     35       58.86     55.51     42.65     37.62     99.65\n",
      "     36       59.11     55.04     44.66     37.03     99.70\n",
      "     37       58.92     56.88     42.12     36.98     99.70\n",
      "     38       59.13     56.36     44.07     36.39     99.70\n",
      "     39       59.93     57.45     45.05     37.51     99.70\n",
      "     40       58.89     56.64     41.92     37.30     99.70\n",
      "     41       58.62     55.41     42.70     36.66     99.70\n",
      "     42       58.68     56.03     42.56     36.45     99.70\n",
      "     43       59.25     56.55     43.24     37.51     99.70\n",
      "     44       58.42     55.60     42.07     36.29     99.70\n",
      "     45       59.02     56.69     43.19     36.50     99.70\n",
      "     46       59.32     56.55     43.24     37.83     99.65\n",
      "     47       59.70     57.73     43.78     37.62     99.65\n",
      "     48       60.01     58.72     43.14     38.53     99.65\n",
      "     49       59.38     57.40     43.00     37.51     99.60\n",
      "     50       59.02     56.08     43.54     36.82     99.65\n",
      "     51       58.03     54.37     41.87     36.29     99.60\n",
      "     52       60.27     58.49     43.78     39.22     99.60\n",
      "     53       59.15     57.07     42.80     37.03     99.70\n",
      "     54       58.73     55.46     41.97     37.78     99.70\n",
      "     55       58.49     54.23     42.70     37.35     99.70\n",
      "     56       59.25     56.93     42.75     37.67     99.65\n",
      "     57       60.17     58.16     44.07     38.79     99.65\n",
      "     58       59.73     57.73     42.95     38.53     99.70\n",
      "     59       60.04     57.26     43.98     39.27     99.65\n",
      "     60       59.06     55.56     42.90     38.10     99.70\n",
      "     61       59.29     56.17     44.42     36.87     99.70\n",
      "     62       59.19     56.78     43.19     37.09     99.70\n",
      "     63       59.68     57.92     43.49     37.62     99.70\n",
      "     64       59.17     56.50     42.85     37.62     99.70\n",
      "     65       60.13     58.25     44.37     38.21     99.70\n",
      "     66       59.54     57.30     43.24     37.94     99.70\n",
      "     67       59.62     57.40     43.44     37.94     99.70\n",
      "     68       59.09     56.36     42.80     37.57     99.65\n",
      "     69       59.48     57.12     44.47     36.61     99.70\n",
      "     70       59.12     57.45     41.67     37.67     99.70\n",
      "     71       59.03     56.36     43.14     36.98     99.65\n",
      "     72       59.85     57.07     43.63     39.06     99.65\n",
      "     73       59.03     55.60     43.83     36.98     99.70\n",
      "     74       59.62     56.69     43.83     38.26     99.70\n",
      "     75       59.15     56.08     43.63     37.19     99.70\n",
      "     76       60.65     59.43     45.45     38.05     99.65\n",
      "     77       59.72     57.40     43.73     38.05     99.70\n",
      "     78       60.27     58.39     44.56     38.42     99.70\n",
      "     79       59.16     55.65     43.93     37.35     99.70\n",
      "Adding inducing inputs to the coreset randomly, 10 samples in total, with each class getting the same number of samples\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.5916 \n",
      "Accuracies (test): [0.5565, 0.4393, 0.3735, 0.997]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 5\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "      0       61.00     61.04     49.17     29.46     68.88     96.47\n",
      "      1       60.05     62.55     46.38     27.85     65.61     97.88\n",
      "      2       58.74     60.28     42.56     28.18     64.15     98.54\n",
      "      3       56.51     57.87     39.91     23.91     62.08     98.79\n",
      "      4       56.10     55.13     40.74     24.76     60.98     98.89\n",
      "      5       55.04     55.79     37.71     23.37     59.16     99.19\n",
      "      6       55.23     56.78     38.44     22.95     58.86     99.14\n",
      "      7       53.45     53.10     36.04     21.34     57.40     99.39\n",
      "      8       53.52     51.91     36.83     21.99     57.50     99.39\n",
      "      9       53.69     54.04     36.09     21.66     57.30     99.34\n",
      "     10       53.95     53.43     37.41     21.93     57.65     99.34\n",
      "     11       54.09     55.46     36.78     21.50     57.35     99.34\n",
      "     12       53.62     54.85     35.60     21.77     56.55     99.34\n",
      "     13       53.70     55.32     35.70     21.77     56.39     99.34\n",
      "     14       53.99     58.25     35.21     21.45     55.69     99.34\n",
      "     15       52.89     53.00     34.13     21.99     55.94     99.39\n",
      "     16       53.00     54.61     34.82     20.86     55.14     99.55\n",
      "     17       53.79     55.65     35.85     22.15     55.84     99.45\n",
      "     18       52.67     53.10     33.99     21.88     54.98     99.39\n",
      "     19       53.20     55.65     35.26     20.28     55.19     99.60\n",
      "     20       52.37     52.77     34.67     20.12     54.68     99.60\n",
      "     21       52.64     54.28     33.74     20.49     55.14     99.55\n",
      "     22       53.03     55.98     34.38     20.92     54.43     99.45\n",
      "     23       53.55     55.89     35.80     21.40     55.09     99.55\n",
      "     24       52.22     52.58     33.74     20.60     54.68     99.50\n",
      "     25       52.57     54.42     33.89     20.60     54.43     99.50\n",
      "     26       52.50     54.00     34.04     20.49     54.43     99.55\n",
      "     27       52.31     54.23     33.40     20.01     54.43     99.50\n",
      "     28       52.97     55.70     33.84     20.97     54.78     99.55\n",
      "     29       52.88     55.56     33.40     21.08     54.83     99.55\n",
      "     30       52.15     52.29     34.04     20.33     54.58     99.50\n",
      "     31       52.11     53.85     33.74     19.32     54.18     99.45\n",
      "     32       52.51     54.75     33.74     20.01     54.48     99.55\n",
      "     33       52.73     54.85     34.48     20.06     54.73     99.55\n",
      "     34       52.31     55.18     33.89     18.68     54.28     99.50\n",
      "     35       51.49     51.35     33.25     19.21     54.13     99.50\n",
      "     36       52.68     54.89     34.08     21.02     53.93     99.50\n",
      "     37       52.07     53.48     33.40     19.69     54.18     99.60\n",
      "     38       51.88     50.83     34.38     20.49     54.08     99.60\n",
      "     39       52.17     54.14     33.84     19.37     53.93     99.55\n",
      "     40       52.17     54.89     32.96     19.64     53.83     99.55\n",
      "     41       52.32     54.42     34.18     19.74     53.78     99.50\n",
      "     42       52.29     54.70     33.45     19.69     54.08     99.55\n",
      "     43       51.59     54.33     31.19     18.89     53.98     99.55\n",
      "     44       52.34     55.37     33.79     18.89     54.08     99.55\n",
      "     45       52.17     54.33     33.55     19.21     54.23     99.55\n",
      "     46       52.49     55.32     33.99     19.74     53.83     99.55\n",
      "     47       51.72     53.85     32.91     18.68     53.63     99.55\n",
      "     48       52.18     54.70     33.40     19.64     53.68     99.50\n",
      "     49       52.13     53.71     34.67     19.00     53.73     99.55\n",
      "     50       51.79     53.29     33.35     19.32     53.47     99.50\n",
      "     51       52.38     54.85     33.10     20.65     53.68     99.60\n",
      "     52       51.49     52.34     32.91     19.10     53.58     99.50\n",
      "     53       52.12     54.70     32.76     19.80     53.73     99.60\n",
      "     54       53.54     60.24     34.43     19.96     53.58     99.50\n",
      "     55       52.38     56.55     33.45     18.57     53.78     99.55\n",
      "     56       51.95     52.58     34.13     19.64     53.88     99.50\n",
      "     57       52.60     54.85     34.43     20.54     53.68     99.50\n",
      "     58       51.29     52.72     31.49     18.94     53.78     99.50\n",
      "     59       52.32     54.70     33.94     19.48     53.98     99.50\n",
      "     60       53.04     55.74     35.11     20.81     53.93     99.60\n",
      "     61       52.71     55.89     33.94     20.44     53.78     99.50\n",
      "     62       52.44     56.50     32.13     20.33     53.73     99.50\n",
      "     63       51.15     51.54     32.37     19.37     52.87     99.60\n",
      "     64       50.95     49.74     33.15     18.68     53.68     99.50\n",
      "     65       51.97     54.33     33.74     18.62     53.68     99.50\n",
      "     66       51.55     50.21     34.48     19.85     53.73     99.50\n",
      "     67       52.77     55.51     34.43     20.54     53.88     99.50\n",
      "     68       52.06     54.66     32.32     20.22     53.58     99.50\n",
      "     69       52.55     57.26     33.06     19.58     53.32     99.55\n",
      "     70       51.37     52.91     33.55     17.40     53.47     99.50\n",
      "     71       51.26     52.39     32.13     18.57     53.73     99.50\n",
      "     72       51.69     52.67     33.10     19.42     53.78     99.50\n",
      "     73       51.88     54.52     33.45     18.57     53.37     99.50\n",
      "     74       52.53     55.98     33.94     19.74     53.47     99.50\n",
      "     75       51.83     55.60     32.17     18.78     53.12     99.50\n",
      "     76       51.31     52.20     33.06     18.84     52.87     99.60\n",
      "     77       51.15     50.21     33.79     19.10     53.07     99.60\n",
      "     78       51.45     52.62     32.47     19.37     53.17     99.60\n",
      "     79       51.56     51.82     33.25     19.48     53.63     99.60\n",
      "Adding inducing inputs to the coreset randomly, 10 samples in total, with each class getting the same number of samples\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.5156 \n",
      "Accuracies (test): [0.5182, 0.3325, 0.1948, 0.5363, 0.996]\n",
      "---\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n",
      "\n",
      "0.51556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fsvi_minimal_coreset.pkl\", task_sequence)\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FROMP (with lambda-descend coreset) <a name=\"res5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 5451.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on oat2.cs.ox.ac.uk\n",
      "Running with: python /auto/users/timner/qixuan/function-space-variational-inference/baselines/fromp/run_fromp.py --dataset smnist_sh --n_tasks 5 --batch_size 128 --hidden_size 256 --n_layers 2 --lr 0.0001 --n_epochs 15 --seed 6 --n_seeds 1 --n_points 40 --select_method lambda_descend --tau 10.0 --n_permuted_tasks 10 --smnist_eps 1e-06 --logroot ablation --subdir reproduce_main_results_3 --save_alt --n_coreset_inputs_per_task 200 --n_steps not_specified\n",
      "\n",
      "smnist_sh, seed 6\n",
      "start working on task 0\n",
      "Test accuracies, task 1: mean = 0.9995, all = [0.9995]\n",
      "nb classes 10\n",
      "memorable points appended!\n",
      "updated fisher!\n",
      "start working on task 1\n",
      "Test accuracies, task 2: mean = 0.5496, all = [0.1154 0.9838]\n",
      "nb classes 10\n",
      "memorable points appended!\n",
      "updated fisher!\n",
      "start working on task 2\n",
      "Test accuracies, task 3: mean = 0.5618, all = [0.1674 0.526  0.992 ]\n",
      "nb classes 10\n",
      "memorable points appended!\n",
      "updated fisher!\n",
      "start working on task 3\n",
      "Test accuracies, task 4: mean = 0.4302, all = [0.1745 0.404  0.1515 0.9909]\n",
      "nb classes 10\n",
      "memorable points appended!\n",
      "updated fisher!\n",
      "start working on task 4\n",
      "Test accuracies, task 5: mean = 0.3344, all = [0.1121 0.2615 0.016  0.3046 0.9778]\n",
      "nb classes 10\n",
      "memorable points appended!\n",
      "updated fisher!\n",
      "\n",
      "Test accuracy: mean = 0.3344, std = 0.0000\n",
      "\n",
      "0.33440348490185456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fromp_with_coreset.pkl\", task_sequence, \"fromp\")\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp, runner=\"fromp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VCL (random-choice coreset) <a name=\"res6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 5882.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on oat18.cs.ox.ac.uk\n",
      "Running with: python /auto/users/timner/qixuan/function-space-variational-inference/fsvi_cl/baselines/vcl/run_vcl.py --dataset smnist_sh --n_epochs 100 --batch_size 256 --hidden_size 256 --n_layers 2 --seed 5 --select_method random_choice --n_permuted_tasks 10 --logroot ablation --subdir reproduce_main_results_3 --n_coreset_inputs_per_task 200\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('Epoch:', '0001', 'cost=', '0.113861091')\n",
      "('Epoch:', '0006', 'cost=', '0.000303962')\n",
      "('Epoch:', '0011', 'cost=', '0.000059380')\n",
      "('Epoch:', '0016', 'cost=', '0.000022568')\n",
      "('Epoch:', '0021', 'cost=', '0.000011832')\n",
      "('Epoch:', '0026', 'cost=', '0.000006914')\n",
      "('Epoch:', '0031', 'cost=', '0.000004239')\n",
      "('Epoch:', '0036', 'cost=', '0.000002657')\n",
      "('Epoch:', '0041', 'cost=', '0.000001821')\n",
      "('Epoch:', '0046', 'cost=', '0.000001267')\n",
      "('Epoch:', '0051', 'cost=', '0.000000909')\n",
      "('Epoch:', '0056', 'cost=', '0.000000672')\n",
      "('Epoch:', '0061', 'cost=', '0.000000508')\n",
      "('Epoch:', '0066', 'cost=', '0.000000397')\n",
      "('Epoch:', '0071', 'cost=', '0.000000317')\n",
      "('Epoch:', '0076', 'cost=', '0.000000262')\n",
      "('Epoch:', '0081', 'cost=', '0.000000200')\n",
      "('Epoch:', '0086', 'cost=', '0.000000163')\n",
      "('Epoch:', '0091', 'cost=', '0.000000134')\n",
      "('Epoch:', '0096', 'cost=', '0.000000114')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '53.848405566')\n",
      "('Epoch:', '0006', 'cost=', '51.163323383')\n",
      "('Epoch:', '0011', 'cost=', '48.524043725')\n",
      "('Epoch:', '0016', 'cost=', '45.891273966')\n",
      "('Epoch:', '0021', 'cost=', '43.264921072')\n",
      "('Epoch:', '0026', 'cost=', '40.645399444')\n",
      "('Epoch:', '0031', 'cost=', '38.035255821')\n",
      "('Epoch:', '0036', 'cost=', '35.436259523')\n",
      "('Epoch:', '0041', 'cost=', '32.852674445')\n",
      "('Epoch:', '0046', 'cost=', '30.285437253')\n",
      "('Epoch:', '0051', 'cost=', '27.742038260')\n",
      "('Epoch:', '0056', 'cost=', '25.230200553')\n",
      "('Epoch:', '0061', 'cost=', '22.755655717')\n",
      "('Epoch:', '0066', 'cost=', '20.331830433')\n",
      "('Epoch:', '0071', 'cost=', '17.972893540')\n",
      "('Epoch:', '0076', 'cost=', '15.693164514')\n",
      "('Epoch:', '0081', 'cost=', '13.513110570')\n",
      "('Epoch:', '0086', 'cost=', '11.458841051')\n",
      "('Epoch:', '0091', 'cost=', '9.554967510')\n",
      "('Epoch:', '0096', 'cost=', '7.820587557')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '32.190925598')\n",
      "('Epoch:', '0006', 'cost=', '32.158424377')\n",
      "('Epoch:', '0011', 'cost=', '32.121562958')\n",
      "('Epoch:', '0016', 'cost=', '32.121917725')\n",
      "('Epoch:', '0021', 'cost=', '32.050369263')\n",
      "('Epoch:', '0026', 'cost=', '32.031482697')\n",
      "('Epoch:', '0031', 'cost=', '31.982522964')\n",
      "('Epoch:', '0036', 'cost=', '31.954931259')\n",
      "('Epoch:', '0041', 'cost=', '31.919614792')\n",
      "('Epoch:', '0046', 'cost=', '31.878671646')\n",
      "('Epoch:', '0051', 'cost=', '31.852344513')\n",
      "('Epoch:', '0056', 'cost=', '31.810844421')\n",
      "('Epoch:', '0061', 'cost=', '31.776830673')\n",
      "('Epoch:', '0066', 'cost=', '31.742906570')\n",
      "('Epoch:', '0071', 'cost=', '31.711341858')\n",
      "('Epoch:', '0076', 'cost=', '31.678670883')\n",
      "('Epoch:', '0081', 'cost=', '31.645214081')\n",
      "('Epoch:', '0086', 'cost=', '31.626588821')\n",
      "('Epoch:', '0091', 'cost=', '31.592735291')\n",
      "('Epoch:', '0096', 'cost=', '31.544286728')\n",
      "Optimization Finished!\n",
      "Accuracy at task 0: [0.9995271867612293]\n",
      "('Epoch:', '0001', 'cost=', '120.095971777')\n",
      "('Epoch:', '0006', 'cost=', '5.531603793')\n",
      "('Epoch:', '0011', 'cost=', '2.992715947')\n",
      "('Epoch:', '0016', 'cost=', '2.359633583')\n",
      "('Epoch:', '0021', 'cost=', '2.028972968')\n",
      "('Epoch:', '0026', 'cost=', '1.868275653')\n",
      "('Epoch:', '0031', 'cost=', '1.758477059')\n",
      "('Epoch:', '0036', 'cost=', '1.644492383')\n",
      "('Epoch:', '0041', 'cost=', '1.589522641')\n",
      "('Epoch:', '0046', 'cost=', '1.538510591')\n",
      "('Epoch:', '0051', 'cost=', '1.476153381')\n",
      "('Epoch:', '0056', 'cost=', '1.425559265')\n",
      "('Epoch:', '0061', 'cost=', '1.371105940')\n",
      "('Epoch:', '0066', 'cost=', '1.327144823')\n",
      "('Epoch:', '0071', 'cost=', '1.277944659')\n",
      "('Epoch:', '0076', 'cost=', '1.233780199')\n",
      "('Epoch:', '0081', 'cost=', '1.198526479')\n",
      "('Epoch:', '0086', 'cost=', '1.172554523')\n",
      "('Epoch:', '0091', 'cost=', '1.141207094')\n",
      "('Epoch:', '0096', 'cost=', '1.117339641')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '20.055970192')\n",
      "('Epoch:', '0006', 'cost=', '19.816394806')\n",
      "('Epoch:', '0011', 'cost=', '19.198091507')\n",
      "('Epoch:', '0016', 'cost=', '18.955224991')\n",
      "('Epoch:', '0021', 'cost=', '18.743588448')\n",
      "('Epoch:', '0026', 'cost=', '18.563527107')\n",
      "('Epoch:', '0031', 'cost=', '18.575380325')\n",
      "('Epoch:', '0036', 'cost=', '18.314683914')\n",
      "('Epoch:', '0041', 'cost=', '18.152055740')\n",
      "('Epoch:', '0046', 'cost=', '18.076599121')\n",
      "('Epoch:', '0051', 'cost=', '17.899688721')\n",
      "('Epoch:', '0056', 'cost=', '17.581949234')\n",
      "('Epoch:', '0061', 'cost=', '17.543533325')\n",
      "('Epoch:', '0066', 'cost=', '17.352676392')\n",
      "('Epoch:', '0071', 'cost=', '16.993176460')\n",
      "('Epoch:', '0076', 'cost=', '16.799522400')\n",
      "('Epoch:', '0081', 'cost=', '16.795780182')\n",
      "('Epoch:', '0086', 'cost=', '16.747246742')\n",
      "('Epoch:', '0091', 'cost=', '16.556273460')\n",
      "('Epoch:', '0096', 'cost=', '16.455026627')\n",
      "Optimization Finished!\n",
      "Accuracy at task 1: [0.9470449172576832, 0.519588638589618]\n",
      "('Epoch:', '0001', 'cost=', '8.142321012')\n",
      "('Epoch:', '0006', 'cost=', '5.105029009')\n",
      "('Epoch:', '0011', 'cost=', '3.698192651')\n",
      "('Epoch:', '0016', 'cost=', '1.572297191')\n",
      "('Epoch:', '0021', 'cost=', '1.429487998')\n",
      "('Epoch:', '0026', 'cost=', '1.398562702')\n",
      "('Epoch:', '0031', 'cost=', '1.351847462')\n",
      "('Epoch:', '0036', 'cost=', '1.341691052')\n",
      "('Epoch:', '0041', 'cost=', '1.275998289')\n",
      "('Epoch:', '0046', 'cost=', '1.269529435')\n",
      "('Epoch:', '0051', 'cost=', '1.236713420')\n",
      "('Epoch:', '0056', 'cost=', '1.184030855')\n",
      "('Epoch:', '0061', 'cost=', '1.161362139')\n",
      "('Epoch:', '0066', 'cost=', '1.125585461')\n",
      "('Epoch:', '0071', 'cost=', '1.094764845')\n",
      "('Epoch:', '0076', 'cost=', '1.075599968')\n",
      "('Epoch:', '0081', 'cost=', '1.045497451')\n",
      "('Epoch:', '0086', 'cost=', '0.569161279')\n",
      "('Epoch:', '0091', 'cost=', '0.387937769')\n",
      "('Epoch:', '0096', 'cost=', '0.326512960')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '17.492446899')\n",
      "('Epoch:', '0006', 'cost=', '16.800459544')\n",
      "('Epoch:', '0011', 'cost=', '16.089454969')\n",
      "('Epoch:', '0016', 'cost=', '15.630043030')\n",
      "('Epoch:', '0021', 'cost=', '15.233593941')\n",
      "('Epoch:', '0026', 'cost=', '14.717794418')\n",
      "('Epoch:', '0031', 'cost=', '14.427866618')\n",
      "('Epoch:', '0036', 'cost=', '14.052911758')\n",
      "('Epoch:', '0041', 'cost=', '13.952827136')\n",
      "('Epoch:', '0046', 'cost=', '13.667154312')\n",
      "('Epoch:', '0051', 'cost=', '13.684413274')\n",
      "('Epoch:', '0056', 'cost=', '13.242173195')\n",
      "('Epoch:', '0061', 'cost=', '13.154453913')\n",
      "('Epoch:', '0066', 'cost=', '12.900175730')\n",
      "('Epoch:', '0071', 'cost=', '12.821066221')\n",
      "('Epoch:', '0076', 'cost=', '12.617156029')\n",
      "('Epoch:', '0081', 'cost=', '12.690327326')\n",
      "('Epoch:', '0086', 'cost=', '12.455737432')\n",
      "('Epoch:', '0091', 'cost=', '12.288268407')\n",
      "('Epoch:', '0096', 'cost=', '12.290962219')\n",
      "Optimization Finished!\n",
      "Accuracy at task 2: [0.0, 0.0, 0.9845250800426895]\n",
      "('Epoch:', '0001', 'cost=', '18.939500159')\n",
      "('Epoch:', '0006', 'cost=', '4.461954599')\n",
      "('Epoch:', '0011', 'cost=', '2.780383014')\n",
      "('Epoch:', '0016', 'cost=', '2.085172651')\n",
      "('Epoch:', '0021', 'cost=', '1.668511543')\n",
      "('Epoch:', '0026', 'cost=', '1.418135285')\n",
      "('Epoch:', '0031', 'cost=', '1.302461216')\n",
      "('Epoch:', '0036', 'cost=', '1.233513700')\n",
      "('Epoch:', '0041', 'cost=', '1.201283422')\n",
      "('Epoch:', '0046', 'cost=', '1.156113490')\n",
      "('Epoch:', '0051', 'cost=', '1.126275057')\n",
      "('Epoch:', '0056', 'cost=', '1.085695690')\n",
      "('Epoch:', '0061', 'cost=', '1.051648868')\n",
      "('Epoch:', '0066', 'cost=', '1.022039840')\n",
      "('Epoch:', '0071', 'cost=', '0.509438406')\n",
      "('Epoch:', '0076', 'cost=', '0.302111078')\n",
      "('Epoch:', '0081', 'cost=', '0.259592335')\n",
      "('Epoch:', '0086', 'cost=', '0.225968853')\n",
      "('Epoch:', '0091', 'cost=', '0.207175941')\n",
      "('Epoch:', '0096', 'cost=', '0.176170710')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '43.411353111')\n",
      "('Epoch:', '0006', 'cost=', '26.917461395')\n",
      "('Epoch:', '0011', 'cost=', '22.543670654')\n",
      "('Epoch:', '0016', 'cost=', '18.745458126')\n",
      "('Epoch:', '0021', 'cost=', '17.005057812')\n",
      "('Epoch:', '0026', 'cost=', '14.574745178')\n",
      "('Epoch:', '0031', 'cost=', '14.613692760')\n",
      "('Epoch:', '0036', 'cost=', '13.962332964')\n",
      "('Epoch:', '0041', 'cost=', '13.134233952')\n",
      "('Epoch:', '0046', 'cost=', '12.804678440')\n",
      "('Epoch:', '0051', 'cost=', '12.006545544')\n",
      "('Epoch:', '0056', 'cost=', '12.415373087')\n",
      "('Epoch:', '0061', 'cost=', '11.816358089')\n",
      "('Epoch:', '0066', 'cost=', '11.311417580')\n",
      "('Epoch:', '0071', 'cost=', '11.046998739')\n",
      "('Epoch:', '0076', 'cost=', '10.930528164')\n",
      "('Epoch:', '0081', 'cost=', '10.739916801')\n",
      "('Epoch:', '0086', 'cost=', '10.433614969')\n",
      "('Epoch:', '0091', 'cost=', '10.399966478')\n",
      "('Epoch:', '0096', 'cost=', '10.393211842')\n",
      "Optimization Finished!\n",
      "Accuracy at task 3: [0.0, 0.0, 0.46851654215581645, 0.9279959718026183]\n",
      "('Epoch:', '0001', 'cost=', '71.503203765')\n",
      "('Epoch:', '0006', 'cost=', '5.569875655')\n",
      "('Epoch:', '0011', 'cost=', '3.037510183')\n",
      "('Epoch:', '0016', 'cost=', '2.231682570')\n",
      "('Epoch:', '0021', 'cost=', '1.670421717')\n",
      "('Epoch:', '0026', 'cost=', '1.140215359')\n",
      "('Epoch:', '0031', 'cost=', '0.809071551')\n",
      "('Epoch:', '0036', 'cost=', '0.683086108')\n",
      "('Epoch:', '0041', 'cost=', '0.616648399')\n",
      "('Epoch:', '0046', 'cost=', '0.556205426')\n",
      "('Epoch:', '0051', 'cost=', '0.518597065')\n",
      "('Epoch:', '0056', 'cost=', '0.484160748')\n",
      "('Epoch:', '0061', 'cost=', '0.453433759')\n",
      "('Epoch:', '0066', 'cost=', '0.419931128')\n",
      "('Epoch:', '0071', 'cost=', '0.396104031')\n",
      "('Epoch:', '0076', 'cost=', '0.364639847')\n",
      "('Epoch:', '0081', 'cost=', '0.338141320')\n",
      "('Epoch:', '0086', 'cost=', '0.305386250')\n",
      "('Epoch:', '0091', 'cost=', '0.282568282')\n",
      "('Epoch:', '0096', 'cost=', '0.259684431')\n",
      "Optimization Finished!\n",
      "('Epoch:', '0001', 'cost=', '17.885663033')\n",
      "('Epoch:', '0006', 'cost=', '15.614822149')\n",
      "('Epoch:', '0011', 'cost=', '13.596643209')\n",
      "('Epoch:', '0016', 'cost=', '12.180986166')\n",
      "('Epoch:', '0021', 'cost=', '11.917979479')\n",
      "('Epoch:', '0026', 'cost=', '11.463542938')\n",
      "('Epoch:', '0031', 'cost=', '10.872410297')\n",
      "('Epoch:', '0036', 'cost=', '10.574742317')\n",
      "('Epoch:', '0041', 'cost=', '10.513534784')\n",
      "('Epoch:', '0046', 'cost=', '10.107223511')\n",
      "('Epoch:', '0051', 'cost=', '9.927984715')\n",
      "('Epoch:', '0056', 'cost=', '9.662940264')\n",
      "('Epoch:', '0061', 'cost=', '9.450565338')\n",
      "('Epoch:', '0066', 'cost=', '9.349601507')\n",
      "('Epoch:', '0071', 'cost=', '9.099406958')\n",
      "('Epoch:', '0076', 'cost=', '8.928174973')\n",
      "('Epoch:', '0081', 'cost=', '8.849201679')\n",
      "('Epoch:', '0086', 'cost=', '8.763258219')\n",
      "('Epoch:', '0091', 'cost=', '8.604754448')\n",
      "('Epoch:', '0096', 'cost=', '8.498201847')\n",
      "Optimization Finished!\n",
      "Accuracy at task 4: [0.4884160756501182, 0.0, 0.0, 0.41540785498489424, 0.8845184064548663]\n",
      "[[0.99952719        nan        nan        nan        nan]\n",
      " [0.94704492 0.51958864        nan        nan        nan]\n",
      " [0.         0.         0.98452508        nan        nan]\n",
      " [0.         0.         0.46851654 0.92799597        nan]\n",
      " [0.48841608 0.         0.         0.41540785 0.88451841]]\n",
      "\n",
      "0.35766846741797576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"vcl_random_coreset.pkl\", task_sequence, \"vcl\")\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp, runner=\"vcl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsvi",
   "language": "python",
   "name": "fsvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}