{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Environment Setup](#setup)\n",
    "* [Results](#results)\n",
    "    * [S-FSVI](#res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run as Colab notebook\n",
    "\n",
    "**Important: Before connecting to a kernel, select a GPU runtime. To do so, open the `Runtime` tab above, click `Change runtime type`, and select `GPU`. Run the setup cell below only after you've done this.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pull S-FSVI repository\n",
    "!git clone https://github.com/timrudner/S-FSVI.git\n",
    "# patch required packages\n",
    "!pip install -r ./S-FSVI/colab_requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**After successfully running the cell above, you need to restart the runtime. To do so, open the “Runtime” tab above and and click “Restart runtime”. Once the runtime was restarted, run the cell below. There is no need to re-run the installation in the cell above.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add the repo to path\n",
    "import os\n",
    "import sys\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"S-FSVI\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run as Jupyter notebook (-->skip ahead to “Results” if you are running this as a Colab notebook<--)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install conda environment `fsvi`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!conda env update -f ../environment.yml"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Troubleshooting:\n",
    "\n",
    " - In case there is an error when installing sklearn: run `pip install Cython==0.29.23` manually and then run the above command again.\n",
    " - In case you have access to a GPU, see instructions [here](https://github.com/google/jax#pip-installation-gpu-cuda) for installing the GPU version of `jaxlib`. This will make the experiment run significantly faster."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the command below to install the conda environment as a kernel of the jupyter notebook. Then switch to this kernel using the Jupyter Notebook menu bar by selecting `Kernel`, `Change kernel`, and then selecting `fsvi`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m ipykernel install --user --name=fsvi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Troubleshooting: For further details, see [here](https://medium.com/@nrk25693/how-to-add-your-conda-environment-to-your-jupyter-notebook-in-just-4-steps-abeab8b8d084)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# assuming os.getcwd() returns the directory containing this jupyter notebook\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results <a name=\"results\"></a>\n",
    "\n",
    "To read a model checkpoint instead of training the model from scratch, pass load_chkpt=True to the function read_config_and_run .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sfsvi.exps.utils.load_utils as lutils\n",
    "from notebooks.nb_utils.common import read_config_and_run, show_final_average_accuracy\n",
    "\n",
    "task_sequence = \"cifar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-FSVI <a name=\"res1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scratch/timner/applications/anaconda3/envs/fsvi/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/scratch/timner/applications/anaconda3/envs/fsvi/lib/python3.8/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 1223.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: TensorFlow is set to only use CPU.\n",
      "Jax is running on gpu\n",
      "WARNING: TensorFlow is set to only use CPU.\n",
      "Loading from cache:\n",
      "Running on clpc158.cs.ox.ac.uk\n",
      "Jax is running on gpu\n",
      "\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"command\":\"cl\",\n",
      "    \"data_training\":\"continual_learning_cifar\",\n",
      "    \"data_ood\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"model_type\":\"fsvi_cnn\",\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"six_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0.0\",\n",
      "    \"prior_cov\":\"0.01\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":50,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":512,\n",
      "    \"learning_rate\":0.0003,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0.0,\n",
      "    \"inducing_points\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":512,\n",
      "    \"inducing_input_type\":\"train_pixel_rand_0.5\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"normalized\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":0.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":5,\n",
      "    \"tau\":1.0,\n",
      "    \"noise_std\":1.0,\n",
      "    \"ind_lim\":\"ind_-1_1\",\n",
      "    \"logging_frequency\":10,\n",
      "    \"figsize\":[\n",
      "        \"10\",\n",
      "        \"4\"\n",
      "    ],\n",
      "    \"seed\":9,\n",
      "    \"save\":false,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":true,\n",
      "    \"kl_sup\":\"not_specified\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"logroot\":\"ablation\",\n",
      "    \"subdir\":\"reproduce_main_results_2\",\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"n_inducing_inputs\":50,\n",
      "    \"n_inducing_inputs_first_task\":\"10\",\n",
      "    \"n_inducing_inputs_second_task\":\"200\",\n",
      "    \"inducing_inputs_bound\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"fix_shuffle\":false,\n",
      "    \"use_generative_model\":false,\n",
      "    \"inducing_inputs_add_mode\":0,\n",
      "    \"inducing_input_adjustment\":false,\n",
      "    \"not_use_coreset\":false,\n",
      "    \"inducing_input_augmentation\":false,\n",
      "    \"plotting\":false,\n",
      "    \"logging\":1,\n",
      "    \"use_val_split\":false,\n",
      "    \"not_use_val_split\":true,\n",
      "    \"coreset\":\"random\",\n",
      "    \"coreset_entropy_mode\":\"soft_lowest\",\n",
      "    \"coreset_entropy_offset\":\"0.0\",\n",
      "    \"coreset_kl_heuristic\":\"lowest\",\n",
      "    \"coreset_kl_offset\":\"0.0\",\n",
      "    \"coreset_elbo_heuristic\":\"lowest\",\n",
      "    \"coreset_elbo_offset\":\"0.0\",\n",
      "    \"coreset_elbo_n_samples\":5,\n",
      "    \"coreset_n_tasks\":\"not_specified\",\n",
      "    \"coreset_entropy_n_mixed\":1,\n",
      "    \"n_coreset_inputs_per_task\":\"200\",\n",
      "    \"full_ntk\":false,\n",
      "    \"constant_inducing_points\":false,\n",
      "    \"n_permuted_tasks\":10,\n",
      "    \"n_omniglot_tasks\":20,\n",
      "    \"epochs_first_task\":\"200\",\n",
      "    \"identity_cov\":false,\n",
      "    \"n_epochs_save_params\":\"not_specified\",\n",
      "    \"n_augment\":\"not_specified\",\n",
      "    \"augment_mode\":\"constant\",\n",
      "    \"n_valid\":\"same\",\n",
      "    \"learning_rate_first_task\":\"5e-4\",\n",
      "    \"save_alt\":true,\n",
      "    \"save_first_task\":false,\n",
      "    \"first_task_load_exp_path\":\"not_specified\",\n",
      "    \"only_task_id\":\"not_specified\",\n",
      "    \"loss_type\":1,\n",
      "    \"only_trainable_head\":false,\n",
      "    \"n_omniglot_coreset_chars\":2,\n",
      "    \"omniglot_randomize_test_split\":false,\n",
      "    \"omniglot_randomize_task_sequence\":false,\n",
      "    \"n_inducing_input_adjust_amount\":\"not_specified\",\n",
      "    \"save_all_params\":false,\n",
      "    \"task\":\"continual_learning_cifar\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (posterior): False\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "\n",
      "Learning task 1\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------\n",
      "  epoch    mean acc    t1 acc\n",
      "-------  ----------  --------\n",
      "      0       38.27     38.27\n",
      "      1       42.91     42.91\n",
      "      2       46.70     46.70\n",
      "      3       48.64     48.64\n",
      "      4       51.16     51.16\n",
      "      5       52.00     52.00\n",
      "      6       54.49     54.49\n",
      "      7       55.12     55.12\n",
      "      8       55.20     55.20\n",
      "      9       56.99     56.99\n",
      "     10       59.44     59.44\n",
      "     11       59.85     59.85\n",
      "     12       61.25     61.25\n",
      "     13       60.58     60.58\n",
      "     14       62.32     62.32\n",
      "     15       59.57     59.57\n",
      "     16       63.54     63.54\n",
      "     17       63.90     63.90\n",
      "     18       64.26     64.26\n",
      "     19       63.93     63.93\n",
      "     20       64.58     64.58\n",
      "     21       65.66     65.66\n",
      "     22       65.89     65.89\n",
      "     23       66.61     66.61\n",
      "     24       65.53     65.53\n",
      "     25       65.01     65.01\n",
      "     26       63.81     63.81\n",
      "     27       67.43     67.43\n",
      "     28       65.75     65.75\n",
      "     29       67.67     67.67\n",
      "     30       68.18     68.18\n",
      "     31       68.16     68.16\n",
      "     32       69.41     69.41\n",
      "     33       68.18     68.18\n",
      "     34       68.29     68.29\n",
      "     35       68.70     68.70\n",
      "     36       68.65     68.65\n",
      "     37       69.96     69.96\n",
      "     38       69.72     69.72\n",
      "     39       69.81     69.81\n",
      "     40       69.18     69.18\n",
      "     41       70.64     70.64\n",
      "     42       69.97     69.97\n",
      "     43       69.15     69.15\n",
      "     44       70.57     70.57\n",
      "     45       70.31     70.31\n",
      "     46       71.42     71.42\n",
      "     47       70.91     70.91\n",
      "     48       71.07     71.07\n",
      "     49       71.68     71.68\n",
      "     50       72.55     72.55\n",
      "     51       70.77     70.77\n",
      "     52       70.65     70.65\n",
      "     53       72.16     72.16\n",
      "     54       71.68     71.68\n",
      "     55       72.69     72.69\n",
      "     56       72.98     72.98\n",
      "     57       73.05     73.05\n",
      "     58       73.09     73.09\n",
      "     59       73.15     73.15\n",
      "     60       73.06     73.06\n",
      "     61       74.09     74.09\n",
      "     62       72.61     72.61\n",
      "     63       74.14     74.14\n",
      "     64       73.58     73.58\n",
      "     65       72.67     72.67\n",
      "     66       73.13     73.13\n",
      "     67       73.92     73.92\n",
      "     68       74.12     74.12\n",
      "     69       73.90     73.90\n",
      "     70       73.69     73.69\n",
      "     71       73.15     73.15\n",
      "     72       73.52     73.52\n",
      "     73       73.92     73.92\n",
      "     74       73.95     73.95\n",
      "     75       73.22     73.22\n",
      "     76       74.39     74.39\n",
      "     77       74.25     74.25\n",
      "     78       75.11     75.11\n",
      "     79       75.46     75.46\n",
      "     80       74.52     74.52\n",
      "     81       75.48     75.48\n",
      "     82       75.59     75.59\n",
      "     83       73.56     73.56\n",
      "     84       75.56     75.56\n",
      "     85       75.14     75.14\n",
      "     86       75.82     75.82\n",
      "     87       75.64     75.64\n",
      "     88       75.56     75.56\n",
      "     89       74.90     74.90\n",
      "     90       75.44     75.44\n",
      "     91       75.79     75.79\n",
      "     92       76.09     76.09\n",
      "     93       75.84     75.84\n",
      "     94       75.58     75.58\n",
      "     95       75.85     75.85\n",
      "     96       76.01     76.01\n",
      "     97       75.09     75.09\n",
      "     98       75.67     75.67\n",
      "     99       76.01     76.01\n",
      "    100       75.69     75.69\n",
      "    101       75.94     75.94\n",
      "    102       76.52     76.52\n",
      "    103       76.01     76.01\n",
      "    104       75.37     75.37\n",
      "    105       76.39     76.39\n",
      "    106       75.62     75.62\n",
      "    107       76.20     76.20\n",
      "    108       76.29     76.29\n",
      "    109       76.09     76.09\n",
      "    110       76.57     76.57\n",
      "    111       76.21     76.21\n",
      "    112       77.05     77.05\n",
      "    113       75.54     75.54\n",
      "    114       76.85     76.85\n",
      "    115       75.46     75.46\n",
      "    116       77.37     77.37\n",
      "    117       77.31     77.31\n",
      "    118       75.76     75.76\n",
      "    119       77.18     77.18\n",
      "    120       76.77     76.77\n",
      "    121       76.47     76.47\n",
      "    122       76.57     76.57\n",
      "    123       76.32     76.32\n",
      "    124       77.26     77.26\n",
      "    125       74.94     74.94\n",
      "    126       76.33     76.33\n",
      "    127       77.22     77.22\n",
      "    128       77.11     77.11\n",
      "    129       77.68     77.68\n",
      "    130       77.14     77.14\n",
      "    131       77.40     77.40\n",
      "    132       76.84     76.84\n",
      "    133       76.46     76.46\n",
      "    134       76.95     76.95\n",
      "    135       78.03     78.03\n",
      "    136       77.48     77.48\n",
      "    137       77.44     77.44\n",
      "    138       77.12     77.12\n",
      "    139       76.30     76.30\n",
      "    140       77.46     77.46\n",
      "    141       77.41     77.41\n",
      "    142       77.41     77.41\n",
      "    143       77.15     77.15\n",
      "    144       77.72     77.72\n",
      "    145       76.87     76.87\n",
      "    146       76.97     76.97\n",
      "    147       77.64     77.64\n",
      "    148       78.29     78.29\n",
      "    149       77.73     77.73\n",
      "    150       77.92     77.92\n",
      "    151       78.21     78.21\n",
      "    152       77.08     77.08\n",
      "    153       77.75     77.75\n",
      "    154       77.49     77.49\n",
      "    155       76.72     76.72\n",
      "    156       77.93     77.93\n",
      "    157       77.46     77.46\n",
      "    158       77.54     77.54\n",
      "    159       78.16     78.16\n",
      "    160       78.15     78.15\n",
      "    161       77.97     77.97\n",
      "    162       77.85     77.85\n",
      "    163       77.17     77.17\n",
      "    164       77.88     77.88\n",
      "    165       77.99     77.99\n",
      "    166       77.83     77.83\n",
      "    167       77.91     77.91\n",
      "    168       78.60     78.60\n",
      "    169       78.28     78.28\n",
      "    170       77.90     77.90\n",
      "    171       78.00     78.00\n",
      "    172       77.87     77.87\n",
      "    173       77.82     77.82\n",
      "    174       78.00     78.00\n",
      "    175       78.47     78.47\n",
      "    176       78.46     78.46\n",
      "    177       78.62     78.62\n",
      "    178       77.34     77.34\n",
      "    179       77.92     77.92\n",
      "    180       78.13     78.13\n",
      "    181       78.37     78.37\n",
      "    182       78.16     78.16\n",
      "    183       78.33     78.33\n",
      "    184       77.89     77.89\n",
      "    185       78.14     78.14\n",
      "    186       78.03     78.03\n",
      "    187       77.94     77.94\n",
      "    188       77.89     77.89\n",
      "    189       77.77     77.77\n",
      "    190       78.67     78.67\n",
      "    191       78.23     78.23\n",
      "    192       78.45     78.45\n",
      "    193       78.63     78.63\n",
      "    194       77.24     77.24\n",
      "    195       78.30     78.30\n",
      "    196       78.52     78.52\n",
      "    197       77.93     77.93\n",
      "    198       78.00     78.00\n",
      "    199       78.60     78.60\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7860 \n",
      "Accuracies (test): [0.786]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 2\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc\n",
      "-------  ----------  --------  --------\n",
      "      0       56.46     78.42     34.50\n",
      "      1       60.96     78.41     43.50\n",
      "      2       62.62     78.65     46.60\n",
      "      3       63.25     78.61     47.90\n",
      "      4       65.17     78.54     51.80\n",
      "      5       66.48     78.46     54.50\n",
      "      6       67.44     78.38     56.50\n",
      "      7       68.23     78.36     58.10\n",
      "      8       69.13     78.37     59.90\n",
      "      9       70.17     78.25     62.10\n",
      "     10       70.75     78.29     63.20\n",
      "     11       71.30     77.90     64.70\n",
      "     12       71.68     78.16     65.20\n",
      "     13       72.25     78.09     66.40\n",
      "     14       72.36     77.93     66.80\n",
      "     15       72.75     77.99     67.50\n",
      "     16       73.02     77.93     68.10\n",
      "     17       73.77     78.05     69.50\n",
      "     18       73.75     77.90     69.60\n",
      "     19       73.91     77.92     69.90\n",
      "     20       74.08     77.95     70.20\n",
      "     21       74.41     78.03     70.80\n",
      "     22       74.95     77.91     72.00\n",
      "     23       74.77     77.84     71.70\n",
      "     24       75.11     77.73     72.50\n",
      "     25       75.21     77.82     72.60\n",
      "     26       75.40     77.80     73.00\n",
      "     27       75.90     77.80     74.00\n",
      "     28       75.76     77.72     73.80\n",
      "     29       75.88     77.76     74.00\n",
      "     30       76.14     77.98     74.30\n",
      "     31       76.14     77.89     74.40\n",
      "     32       75.98     77.67     74.30\n",
      "     33       76.17     77.55     74.80\n",
      "     34       76.27     77.73     74.80\n",
      "     35       76.56     77.72     75.40\n",
      "     36       76.61     77.51     75.70\n",
      "     37       76.69     77.48     75.90\n",
      "     38       76.66     77.21     76.10\n",
      "     39       76.74     77.48     76.00\n",
      "     40       76.82     77.44     76.20\n",
      "     41       76.85     77.70     76.00\n",
      "     42       76.97     77.34     76.60\n",
      "     43       76.86     77.62     76.10\n",
      "     44       77.08     77.26     76.90\n",
      "     45       77.03     77.65     76.40\n",
      "     46       77.03     77.46     76.60\n",
      "     47       77.16     77.51     76.80\n",
      "     48       77.00     77.61     76.40\n",
      "     49       77.20     77.51     76.90\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7721 \n",
      "Accuracies (test): [0.7751, 0.769]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 3\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc\n",
      "-------  ----------  --------  --------  --------\n",
      "      0       67.44     77.01     77.00     48.30\n",
      "      1       69.99     76.38     76.90     56.70\n",
      "      2       69.35     76.16     73.80     58.10\n",
      "      3       70.52     77.05     75.50     59.00\n",
      "      4       71.00     77.20     76.80     59.00\n",
      "      5       70.83     77.19     75.40     59.90\n",
      "      6       71.46     76.99     75.30     62.10\n",
      "      7       72.00     76.90     76.30     62.80\n",
      "      8       72.66     76.88     75.90     65.20\n",
      "      9       72.39     76.78     74.40     66.00\n",
      "     10       73.24     76.92     76.60     66.20\n",
      "     11       73.13     77.10     75.80     66.50\n",
      "     12       72.78     76.23     75.30     66.80\n",
      "     13       73.60     77.00     75.70     68.10\n",
      "     14       73.65     76.86     75.70     68.40\n",
      "     15       74.00     76.41     76.50     69.10\n",
      "     16       73.41     76.82     74.50     68.90\n",
      "     17       74.40     76.60     76.40     70.20\n",
      "     18       73.86     76.18     75.70     69.70\n",
      "     19       74.59     76.58     75.50     71.70\n",
      "     20       74.06     76.07     75.40     70.70\n",
      "     21       74.66     76.88     75.70     71.40\n",
      "     22       74.85     76.95     75.30     72.30\n",
      "     23       74.43     76.89     74.90     71.50\n",
      "     24       74.62     76.57     76.10     71.20\n",
      "     25       74.70     76.49     75.60     72.00\n",
      "     26       74.81     76.64     76.30     71.50\n",
      "     27       74.36     76.38     74.20     72.50\n",
      "     28       75.32     76.76     76.20     73.00\n",
      "     29       75.53     76.99     76.30     73.30\n",
      "     30       74.58     76.63     74.90     72.20\n",
      "     31       75.40     76.69     75.70     73.80\n",
      "     32       75.28     76.85     75.50     73.50\n",
      "     33       74.96     76.58     75.30     73.00\n",
      "     34       74.96     76.37     75.60     72.90\n",
      "     35       74.88     76.34     76.00     72.30\n",
      "     36       75.40     76.80     76.10     73.30\n",
      "     37       75.64     76.61     76.00     74.30\n",
      "     38       75.50     76.20     76.30     74.00\n",
      "     39       75.25     76.56     75.90     73.30\n",
      "     40       74.96     76.69     75.30     72.90\n",
      "     41       75.47     76.52     76.30     73.60\n",
      "     42       75.50     76.61     76.10     73.80\n",
      "     43       75.30     76.00     76.10     73.80\n",
      "     44       75.53     76.40     75.80     74.40\n",
      "     45       75.50     76.61     75.40     74.50\n",
      "     46       75.78     76.63     76.20     74.50\n",
      "     47       75.43     76.48     75.70     74.10\n",
      "     48       75.62     76.17     76.20     74.50\n",
      "     49       75.89     76.88     75.90     74.90\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7589 \n",
      "Accuracies (test): [0.7688, 0.759, 0.749]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 4\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "      0       68.36     76.93     75.20     74.20     47.10\n",
      "      1       71.43     76.83     76.00     74.60     58.30\n",
      "      2       71.84     75.78     75.50     75.20     60.90\n",
      "      3       72.48     76.33     76.00     74.40     63.20\n",
      "      4       72.90     76.00     76.50     74.00     65.10\n",
      "      5       72.89     76.07     76.60     73.70     65.20\n",
      "      6       73.34     75.97     76.60     74.20     66.60\n",
      "      7       73.38     75.81     76.30     73.90     67.50\n",
      "      8       73.92     75.79     76.90     75.00     68.00\n",
      "      9       74.08     75.72     76.90     74.80     68.90\n",
      "     10       73.98     75.53     75.90     74.60     69.90\n",
      "     11       74.00     75.58     75.70     74.20     70.50\n",
      "     12       74.34     76.07     76.30     74.60     70.40\n",
      "     13       74.40     76.09     77.20     73.20     71.10\n",
      "     14       74.29     75.67     76.50     73.70     71.30\n",
      "     15       74.31     75.76     76.30     73.60     71.60\n",
      "     16       74.67     75.69     76.40     73.90     72.70\n",
      "     17       74.57     75.77     75.50     73.90     73.10\n",
      "     18       75.13     75.74     76.70     74.30     73.80\n",
      "     19       75.22     75.90     76.60     74.30     74.10\n",
      "     20       75.04     75.66     75.90     74.00     74.60\n",
      "     21       75.11     75.66     76.60     73.90     74.30\n",
      "     22       75.06     76.04     75.80     74.10     74.30\n",
      "     23       75.54     75.97     76.70     74.10     75.40\n",
      "     24       75.31     75.72     76.10     74.20     75.20\n",
      "     25       75.13     75.62     75.50     74.40     75.00\n",
      "     26       75.58     75.71     76.10     74.90     75.60\n",
      "     27       75.70     75.82     76.90     74.50     75.60\n",
      "     28       75.65     75.39     76.50     74.70     76.00\n",
      "     29       75.43     75.42     75.60     74.50     76.20\n",
      "     30       75.81     75.93     75.80     74.50     77.00\n",
      "     31       75.55     75.38     76.20     74.60     76.00\n",
      "     32       75.86     75.63     76.80     74.10     76.90\n",
      "     33       75.77     75.59     76.70     73.50     77.30\n",
      "     34       75.75     75.29     76.30     74.40     77.00\n",
      "     35       76.13     75.32     77.00     75.00     77.20\n",
      "     36       75.85     75.80     76.20     74.50     76.90\n",
      "     37       75.76     75.64     75.60     74.40     77.40\n",
      "     38       76.06     75.76     76.30     74.50     77.70\n",
      "     39       75.84     75.75     76.10     74.00     77.50\n",
      "     40       75.84     75.77     76.40     73.40     77.80\n",
      "     41       75.89     75.64     75.70     74.00     78.20\n",
      "     42       75.92     75.69     75.80     74.10     78.10\n",
      "     43       76.06     75.62     76.60     73.60     78.40\n",
      "     44       75.87     75.39     75.80     73.70     78.60\n",
      "     45       76.03     75.74     75.60     74.40     78.40\n",
      "     46       76.19     75.65     76.20     74.40     78.50\n",
      "     47       76.14     75.76     75.90     74.00     78.90\n",
      "     48       76.49     75.67     77.10     74.30     78.90\n",
      "     49       76.43     75.71     76.80     74.20     79.00\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7643 \n",
      "Accuracies (test): [0.7571, 0.768, 0.742, 0.79]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 5\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "      0       70.46     75.98     76.80     74.10     79.30     46.10\n",
      "      1       71.39     75.57     75.40     73.90     78.00     54.10\n",
      "      2       71.97     75.85     75.90     73.80     78.50     55.80\n",
      "      3       72.07     75.77     75.60     74.40     78.10     56.50\n",
      "      4       72.29     75.86     75.90     73.80     77.90     58.00\n",
      "      5       72.61     75.94     76.00     73.10     78.70     59.30\n",
      "      6       73.07     75.84     75.70     73.90     79.00     60.90\n",
      "      7       73.27     75.45     75.60     75.10     78.60     61.60\n",
      "      8       73.35     75.83     76.20     74.30     78.40     62.00\n",
      "      9       73.42     75.70     75.20     75.00     78.30     62.90\n",
      "     10       73.42     75.98     75.40     73.90     78.70     63.10\n",
      "     11       73.77     75.96     76.40     73.80     78.70     64.00\n",
      "     12       73.74     75.98     75.60     74.60     78.70     63.80\n",
      "     13       73.93     76.03     75.60     74.30     78.40     65.30\n",
      "     14       73.75     75.73     75.00     74.40     78.20     65.40\n",
      "     15       73.78     75.59     75.00     74.80     78.40     65.10\n",
      "     16       74.30     75.82     76.10     74.50     78.40     66.70\n",
      "     17       74.24     76.08     76.10     74.50     78.50     66.00\n",
      "     18       73.93     75.84     75.40     73.60     78.30     66.50\n",
      "     19       74.12     75.82     75.30     74.30     78.40     66.80\n",
      "     20       74.26     75.89     75.60     73.90     78.90     67.00\n",
      "     21       74.02     75.91     74.80     73.70     78.50     67.20\n",
      "     22       74.18     75.92     75.20     73.70     78.30     67.80\n",
      "     23       74.42     75.62     75.20     74.10     78.40     68.80\n",
      "     24       74.47     75.97     75.30     74.30     78.50     68.30\n",
      "     25       74.20     75.59     75.20     73.70     78.30     68.20\n",
      "     26       74.47     75.63     75.80     73.70     78.30     68.90\n",
      "     27       74.45     75.75     75.10     73.90     78.20     69.30\n",
      "     28       74.81     76.03     75.40     74.20     79.20     69.20\n",
      "     29       74.68     75.88     75.60     74.10     78.70     69.10\n",
      "     30       74.67     76.07     75.30     73.70     78.20     70.10\n",
      "     31       74.96     75.91     76.00     74.20     78.30     70.40\n",
      "     32       74.88     75.81     75.70     73.30     78.80     70.80\n",
      "     33       74.84     76.00     75.60     73.80     78.40     70.40\n",
      "     34       75.07     76.05     75.70     74.40     77.90     71.30\n",
      "     35       75.04     75.68     75.40     74.80     78.20     71.10\n",
      "     36       74.38     75.32     75.00     73.60     77.50     70.50\n",
      "     37       74.97     75.77     75.50     73.50     78.40     71.70\n",
      "     38       75.07     75.75     75.30     74.60     78.00     71.70\n",
      "     39       74.76     75.90     74.30     73.80     77.80     72.00\n",
      "     40       74.58     75.70     74.20     73.70     77.50     71.80\n",
      "     41       75.23     75.74     75.40     73.90     78.40     72.70\n",
      "     42       75.01     75.96     75.00     73.50     78.10     72.50\n",
      "     43       75.08     75.92     75.00     73.70     78.20     72.60\n",
      "     44       74.94     75.52     74.20     74.20     77.90     72.90\n",
      "     45       75.09     75.87     74.90     73.80     77.90     73.00\n",
      "     46       74.89     75.83     75.00     72.50     77.80     73.30\n",
      "     47       75.38     76.11     75.70     72.70     78.30     74.10\n",
      "     48       75.10     75.79     74.40     73.50     78.10     73.70\n",
      "     49       75.25     75.95     74.40     73.50     78.20     74.20\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7525 \n",
      "Accuracies (test): [0.7595, 0.744, 0.735, 0.782, 0.742]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 6\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------\n",
      "      0       72.19     75.93     74.60     72.80     78.20     74.00     57.60\n",
      "      1       72.95     75.53     74.40     74.40     77.50     73.00     62.90\n",
      "      2       72.92     75.22     74.80     72.90     77.70     73.50     63.40\n",
      "      3       73.38     75.49     75.30     73.20     77.40     73.40     65.50\n",
      "      4       73.34     75.34     74.50     73.80     77.50     72.60     66.30\n",
      "      5       73.22     75.15     74.70     73.50     77.40     72.70     65.90\n",
      "      6       73.79     75.52     75.00     72.90     78.00     73.50     67.80\n",
      "      7       73.59     75.36     75.00     73.20     77.30     73.50     67.20\n",
      "      8       74.00     75.61     74.40     73.40     77.40     73.70     69.50\n",
      "      9       74.12     75.43     75.40     73.30     77.50     73.00     70.10\n",
      "     10       74.16     75.24     74.70     73.30     77.60     73.20     70.90\n",
      "     11       74.42     75.42     75.00     72.90     78.20     73.40     71.60\n",
      "     12       74.31     75.28     74.90     73.20     78.00     73.30     71.20\n",
      "     13       74.17     75.39     74.80     72.60     77.80     72.90     71.50\n",
      "     14       74.26     75.37     75.00     72.50     77.70     73.00     72.00\n",
      "     15       74.54     75.51     75.20     73.30     77.40     73.20     72.60\n",
      "     16       74.64     75.17     74.90     73.70     77.60     73.30     73.20\n",
      "     17       74.84     75.45     75.50     73.90     77.70     73.10     73.40\n",
      "     18       74.47     75.31     75.10     72.70     77.80     72.70     73.20\n",
      "     19       74.67     75.43     75.20     73.20     77.60     72.60     74.00\n",
      "     20       74.83     75.45     75.50     73.50     77.70     72.70     74.10\n",
      "     21       74.85     75.50     75.40     73.40     77.70     72.30     74.80\n",
      "     22       75.13     75.49     76.00     73.70     78.20     72.10     75.30\n",
      "     23       75.22     75.45     76.20     72.90     78.60     72.70     75.50\n",
      "     24       74.61     75.23     74.50     73.70     77.30     71.20     75.70\n",
      "     25       74.79     75.36     75.90     72.90     77.70     72.40     74.50\n",
      "     26       75.02     75.25     75.40     74.00     77.40     72.10     76.00\n",
      "     27       74.99     75.35     75.50     73.60     77.50     72.30     75.70\n",
      "     28       74.96     75.39     74.60     74.10     77.10     72.50     76.10\n",
      "     29       75.23     75.56     76.00     74.00     78.10     71.90     75.80\n",
      "     30       75.08     75.57     75.10     73.20     77.80     72.10     76.70\n",
      "     31       74.92     75.22     75.10     73.50     77.30     71.90     76.50\n",
      "     32       74.86     75.45     74.70     72.90     77.40     72.10     76.60\n",
      "     33       74.91     75.14     75.20     72.90     78.00     71.80     76.40\n",
      "     34       75.14     75.26     75.70     72.90     77.10     72.50     77.40\n",
      "     35       75.24     74.83     76.00     73.20     77.50     72.40     77.50\n",
      "     36       75.17     75.05     75.80     73.40     77.50     71.40     77.90\n",
      "     37       75.09     75.34     74.60     73.50     77.70     71.90     77.50\n",
      "     38       75.32     75.21     75.90     73.20     77.40     72.10     78.10\n",
      "     39       75.25     75.41     75.70     73.40     77.70     71.70     77.60\n",
      "     40       75.39     75.62     75.60     73.00     77.80     72.50     77.80\n",
      "     41       75.46     75.45     75.50     72.80     77.80     72.00     79.20\n",
      "     42       75.44     75.31     75.70     72.50     77.90     72.30     78.90\n",
      "     43       75.52     75.39     75.80     72.70     78.20     72.10     78.90\n",
      "     44       75.67     75.51     76.00     72.80     78.50     72.30     78.90\n",
      "     45       75.54     75.43     76.40     73.10     77.70     72.20     78.40\n",
      "     46       75.23     75.27     75.60     72.50     77.70     72.00     78.30\n",
      "     47       75.54     75.34     76.40     72.20     77.90     72.70     78.70\n",
      "     48       75.78     75.37     76.70     72.50     78.00     72.50     79.60\n",
      "     49       75.55     75.39     76.20     73.00     77.50     72.20     79.00\n",
      "Adding inducing inputs to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7555 \n",
      "Accuracies (test): [0.7539, 0.762, 0.73, 0.775, 0.722, 0.79]\n",
      "---\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n",
      "\n",
      "0.7554833333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fsvi_cifar.pkl\", task_sequence)\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsvi",
   "language": "python",
   "name": "fsvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}