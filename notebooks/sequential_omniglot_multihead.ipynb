{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Environment Setup](#setup)\n",
    "* [Results](#results)\n",
    "    * [S-FSVI](#res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run as Colab notebook\n",
    "\n",
    "**Important: Before connecting to a kernel, select a GPU runtime. To do so, open the `Runtime` tab above, click `Change runtime type`, and select `GPU`. Run the setup cell below only after you've done this.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pull S-FSVI repository\n",
    "!git clone https://github.com/timrudner/S-FSVI.git\n",
    "# patch required packages\n",
    "!pip install -r ./S-FSVI/colab_requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**After successfully running the cell above, you need to restart the runtime. To do so, open the “Runtime” tab above and and click “Restart runtime”. Once the runtime was restarted, run the cell below. There is no need to re-run the installation in the cell above.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add the repo to path\n",
    "import os\n",
    "import sys\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"S-FSVI\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run as Jupyter notebook (-->skip ahead to “Results” if you are running this as a Colab notebook<--)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install conda environment `fsvi`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!conda env update -f ../environment.yml"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Troubleshooting:\n",
    "\n",
    " - In case there is an error when installing sklearn: run `pip install Cython==0.29.23` manually and then run the above command again.\n",
    " - In case you have access to a GPU, see instructions [here](https://github.com/google/jax#pip-installation-gpu-cuda) for installing the GPU version of `jaxlib`. This will make the experiment run significantly faster."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the command below to install the conda environment as a kernel of the jupyter notebook. Then switch to this kernel using the Jupyter Notebook menu bar by selecting `Kernel`, `Change kernel`, and then selecting `fsvi`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m ipykernel install --user --name=fsvi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Troubleshooting: For further details, see [here](https://medium.com/@nrk25693/how-to-add-your-conda-environment-to-your-jupyter-notebook-in-just-4-steps-abeab8b8d084)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# assuming os.getcwd() returns the directory containing this jupyter notebook\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results <a name=\"results\"></a>\n",
    "\n",
    "To read a model checkpoint instead of training the model from scratch, pass load_chkpt=True to the function read_config_and_run .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.nb_utils.common import read_config_and_run, show_final_average_accuracy\n",
    "import sfsvi.exps.utils.load_utils as lutils\n",
    "\n",
    "task_sequence = \"omniglot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-FSVI <a name=\"res1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scratch/timner/applications/anaconda3/envs/fsvi/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/scratch/timner/applications/anaconda3/envs/fsvi/lib/python3.8/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "loading experiments: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 1213.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: TensorFlow is set to only use CPU.\n",
      "Jax is running on gpu\n",
      "WARNING: TensorFlow is set to only use CPU.\n",
      "Loading from cache:\n",
      "Running on oat1.cs.ox.ac.uk\n",
      "Use `tf.data.Dataset.get_single_element()`.\n",
      "Use `tf.data.Dataset.get_single_element()`.\n",
      "Jax is running on gpu\n",
      "\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"command\":\"cl\",\n",
      "    \"data_training\":\"continual_learning_omniglot\",\n",
      "    \"data_ood\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"model_type\":\"fsvi_cnn\",\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"architecture\":\"omniglot_cnn\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0.0\",\n",
      "    \"prior_cov\":\"1\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"epochs\":10,\n",
      "    \"start_var_opt\":0,\n",
      "    \"batch_size\":128,\n",
      "    \"learning_rate\":0.001,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0.0,\n",
      "    \"inducing_points\":0,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":128,\n",
      "    \"inducing_input_type\":\"uniform_rand\",\n",
      "    \"inducing_input_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"inducing_input_ood_data_size\":50000,\n",
      "    \"kl_scale\":\"normalized\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":0.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":5,\n",
      "    \"tau\":1.0,\n",
      "    \"noise_std\":1.0,\n",
      "    \"ind_lim\":\"ind_-1_1\",\n",
      "    \"logging_frequency\":1,\n",
      "    \"figsize\":[\n",
      "        \"10\",\n",
      "        \"4\"\n",
      "    ],\n",
      "    \"seed\":6,\n",
      "    \"save\":false,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":true,\n",
      "    \"kl_sup\":\"not_specified\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"logroot\":\"ablation\",\n",
      "    \"subdir\":\"reproduce_main_results_2\",\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"n_inducing_inputs\":1,\n",
      "    \"n_inducing_inputs_first_task\":\"10\",\n",
      "    \"n_inducing_inputs_second_task\":\"60\",\n",
      "    \"inducing_inputs_bound\":[\n",
      "        0.0,\n",
      "        1.0\n",
      "    ],\n",
      "    \"fix_shuffle\":false,\n",
      "    \"use_generative_model\":false,\n",
      "    \"inducing_inputs_add_mode\":0,\n",
      "    \"inducing_input_adjustment\":true,\n",
      "    \"not_use_coreset\":false,\n",
      "    \"inducing_input_augmentation\":false,\n",
      "    \"plotting\":false,\n",
      "    \"logging\":1,\n",
      "    \"use_val_split\":false,\n",
      "    \"not_use_val_split\":true,\n",
      "    \"coreset\":\"entropy\",\n",
      "    \"coreset_entropy_mode\":\"soft_lowest\",\n",
      "    \"coreset_entropy_offset\":\"0.0\",\n",
      "    \"coreset_kl_heuristic\":\"lowest\",\n",
      "    \"coreset_kl_offset\":\"0.0\",\n",
      "    \"coreset_elbo_heuristic\":\"lowest\",\n",
      "    \"coreset_elbo_offset\":\"0.0\",\n",
      "    \"coreset_elbo_n_samples\":5,\n",
      "    \"coreset_n_tasks\":\"25\",\n",
      "    \"coreset_entropy_n_mixed\":1,\n",
      "    \"n_coreset_inputs_per_task\":\"not_specified\",\n",
      "    \"full_ntk\":false,\n",
      "    \"constant_inducing_points\":false,\n",
      "    \"n_permuted_tasks\":10,\n",
      "    \"n_omniglot_tasks\":50,\n",
      "    \"epochs_first_task\":\"200\",\n",
      "    \"identity_cov\":false,\n",
      "    \"n_epochs_save_params\":\"not_specified\",\n",
      "    \"n_augment\":\"not_specified\",\n",
      "    \"augment_mode\":\"constant\",\n",
      "    \"n_valid\":\"same\",\n",
      "    \"learning_rate_first_task\":\"not_specified\",\n",
      "    \"save_alt\":true,\n",
      "    \"save_first_task\":false,\n",
      "    \"first_task_load_exp_path\":\"not_specified\",\n",
      "    \"only_task_id\":\"not_specified\",\n",
      "    \"loss_type\":1,\n",
      "    \"only_trainable_head\":true,\n",
      "    \"n_omniglot_coreset_chars\":2,\n",
      "    \"omniglot_randomize_test_split\":true,\n",
      "    \"omniglot_randomize_task_sequence\":true,\n",
      "    \"n_inducing_input_adjust_amount\":\"60\",\n",
      "    \"save_all_params\":false,\n",
      "    \"task\":\"continual_learning_omniglot\",\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_lin_minval\":0.0,\n",
      "    \"init_logvar_lin_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0\n",
      "} \n",
      "\n",
      "\n",
      "MAP initialization: False\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (posterior): False\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "\n",
      "Learning task 1\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------\n",
      "  epoch    mean acc    t1 acc\n",
      "-------  ----------  --------\n",
      "      0       43.75     43.75\n",
      "      1       75.00     75.00\n",
      "      2       86.25     86.25\n",
      "      3       89.38     89.38\n",
      "      4       89.38     89.38\n",
      "      5       91.25     91.25\n",
      "      6       89.38     89.38\n",
      "      7       89.38     89.38\n",
      "      8       90.62     90.62\n",
      "      9       89.38     89.38\n",
      "     10       88.75     88.75\n",
      "     11       91.88     91.88\n",
      "     12       91.25     91.25\n",
      "     13       88.75     88.75\n",
      "     14       90.62     90.62\n",
      "     15       90.00     90.00\n",
      "     16       91.88     91.88\n",
      "     17       91.88     91.88\n",
      "     18       91.25     91.25\n",
      "     19       90.62     90.62\n",
      "     20       91.88     91.88\n",
      "     21       92.50     92.50\n",
      "     22       91.88     91.88\n",
      "     23       91.88     91.88\n",
      "     24       93.12     93.12\n",
      "     25       90.00     90.00\n",
      "     26       90.00     90.00\n",
      "     27       91.88     91.88\n",
      "     28       91.25     91.25\n",
      "     29       93.12     93.12\n",
      "     30       90.62     90.62\n",
      "     31       90.00     90.00\n",
      "     32       90.00     90.00\n",
      "     33       90.62     90.62\n",
      "     34       91.25     91.25\n",
      "     35       90.62     90.62\n",
      "     36       89.38     89.38\n",
      "     37       90.62     90.62\n",
      "     38       90.00     90.00\n",
      "     39       90.00     90.00\n",
      "     40       91.25     91.25\n",
      "     41       90.00     90.00\n",
      "     42       91.25     91.25\n",
      "     43       90.62     90.62\n",
      "     44       90.00     90.00\n",
      "     45       88.75     88.75\n",
      "     46       91.25     91.25\n",
      "     47       90.00     90.00\n",
      "     48       92.50     92.50\n",
      "     49       91.25     91.25\n",
      "     50       91.88     91.88\n",
      "     51       90.62     90.62\n",
      "     52       90.00     90.00\n",
      "     53       91.25     91.25\n",
      "     54       90.62     90.62\n",
      "     55       90.62     90.62\n",
      "     56       90.00     90.00\n",
      "     57       91.88     91.88\n",
      "     58       90.00     90.00\n",
      "     59       91.25     91.25\n",
      "     60       91.88     91.88\n",
      "     61       92.50     92.50\n",
      "     62       90.00     90.00\n",
      "     63       90.62     90.62\n",
      "     64       90.62     90.62\n",
      "     65       90.00     90.00\n",
      "     66       90.62     90.62\n",
      "     67       89.38     89.38\n",
      "     68       91.25     91.25\n",
      "     69       91.88     91.88\n",
      "     70       91.88     91.88\n",
      "     71       91.25     91.25\n",
      "     72       89.38     89.38\n",
      "     73       89.38     89.38\n",
      "     74       90.62     90.62\n",
      "     75       91.25     91.25\n",
      "     76       91.25     91.25\n",
      "     77       91.25     91.25\n",
      "     78       90.00     90.00\n",
      "     79       90.00     90.00\n",
      "     80       90.00     90.00\n",
      "     81       89.38     89.38\n",
      "     82       89.38     89.38\n",
      "     83       90.00     90.00\n",
      "     84       91.25     91.25\n",
      "     85       90.00     90.00\n",
      "     86       90.00     90.00\n",
      "     87       91.25     91.25\n",
      "     88       90.00     90.00\n",
      "     89       89.38     89.38\n",
      "     90       89.38     89.38\n",
      "     91       90.00     90.00\n",
      "     92       90.62     90.62\n",
      "     93       90.00     90.00\n",
      "     94       90.62     90.62\n",
      "     95       90.00     90.00\n",
      "     96       89.38     89.38\n",
      "     97       90.00     90.00\n",
      "     98       91.25     91.25\n",
      "     99       90.00     90.00\n",
      "    100       91.25     91.25\n",
      "    101       90.00     90.00\n",
      "    102       89.38     89.38\n",
      "    103       90.00     90.00\n",
      "    104       89.38     89.38\n",
      "    105       90.62     90.62\n",
      "    106       90.00     90.00\n",
      "    107       90.62     90.62\n",
      "    108       90.00     90.00\n",
      "    109       89.38     89.38\n",
      "    110       90.00     90.00\n",
      "    111       90.00     90.00\n",
      "    112       91.25     91.25\n",
      "    113       90.00     90.00\n",
      "    114       88.75     88.75\n",
      "    115       89.38     89.38\n",
      "    116       91.25     91.25\n",
      "    117       90.62     90.62\n",
      "    118       90.62     90.62\n",
      "    119       90.62     90.62\n",
      "    120       88.75     88.75\n",
      "    121       90.00     90.00\n",
      "    122       90.62     90.62\n",
      "    123       91.25     91.25\n",
      "    124       90.62     90.62\n",
      "    125       91.25     91.25\n",
      "    126       90.62     90.62\n",
      "    127       90.62     90.62\n",
      "    128       90.00     90.00\n",
      "    129       89.38     89.38\n",
      "    130       89.38     89.38\n",
      "    131       91.25     91.25\n",
      "    132       90.00     90.00\n",
      "    133       90.00     90.00\n",
      "    134       90.62     90.62\n",
      "    135       90.00     90.00\n",
      "    136       90.00     90.00\n",
      "    137       90.62     90.62\n",
      "    138       89.38     89.38\n",
      "    139       90.00     90.00\n",
      "    140       90.00     90.00\n",
      "    141       90.00     90.00\n",
      "    142       90.00     90.00\n",
      "    143       90.00     90.00\n",
      "    144       90.00     90.00\n",
      "    145       90.00     90.00\n",
      "    146       90.00     90.00\n",
      "    147       90.00     90.00\n",
      "    148       90.62     90.62\n",
      "    149       90.00     90.00\n",
      "    150       90.00     90.00\n",
      "    151       90.00     90.00\n",
      "    152       90.62     90.62\n",
      "    153       90.62     90.62\n",
      "    154       90.00     90.00\n",
      "    155       90.00     90.00\n",
      "    156       90.00     90.00\n",
      "    157       90.00     90.00\n",
      "    158       90.00     90.00\n",
      "    159       90.00     90.00\n",
      "    160       90.00     90.00\n",
      "    161       90.00     90.00\n",
      "    162       89.38     89.38\n",
      "    163       90.62     90.62\n",
      "    164       90.00     90.00\n",
      "    165       89.38     89.38\n",
      "    166       90.00     90.00\n",
      "    167       90.62     90.62\n",
      "    168       90.62     90.62\n",
      "    169       90.62     90.62\n",
      "    170       90.62     90.62\n",
      "    171       90.62     90.62\n",
      "    172       90.62     90.62\n",
      "    173       90.00     90.00\n",
      "    174       90.62     90.62\n",
      "    175       90.62     90.62\n",
      "    176       90.62     90.62\n",
      "    177       90.62     90.62\n",
      "    178       90.00     90.00\n",
      "    179       90.62     90.62\n",
      "    180       90.62     90.62\n",
      "    181       90.62     90.62\n",
      "    182       88.12     88.12\n",
      "    183       90.62     90.62\n",
      "    184       91.88     91.88\n",
      "    185       91.25     91.25\n",
      "    186       91.25     91.25\n",
      "    187       91.25     91.25\n",
      "    188       90.62     90.62\n",
      "    189       91.25     91.25\n",
      "    190       90.62     90.62\n",
      "    191       90.62     90.62\n",
      "    192       91.25     91.25\n",
      "    193       90.62     90.62\n",
      "    194       91.25     91.25\n",
      "    195       90.62     90.62\n",
      "    196       91.25     91.25\n",
      "    197       91.25     91.25\n",
      "    198       90.62     90.62\n",
      "    199       90.62     90.62\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.     0.     0.0001]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.9062 \n",
      "Accuracies (test): [0.9062]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 2\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc\n",
      "-------  ----------  --------  --------\n",
      "      0       72.95     89.38     56.52\n",
      "      1       78.19     90.62     65.76\n",
      "      2       78.73     90.62     66.85\n",
      "      3       80.68     91.25     70.11\n",
      "      4       82.31     91.25     73.37\n",
      "      5       81.64     89.38     73.91\n",
      "      6       82.14     88.75     75.54\n",
      "      7       81.64     89.38     73.91\n",
      "      8       82.50     90.00     75.00\n",
      "      9       83.32     90.00     76.63\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0056 0.0646 0.3067]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8332 \n",
      "Accuracies (test): [0.9, 0.7663]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 3\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc\n",
      "-------  ----------  --------  --------  --------\n",
      "      0       72.63     90.00     72.28     55.62\n",
      "      1       75.82     90.00     71.20     66.25\n",
      "      2       77.37     90.62     73.37     68.12\n",
      "      3       78.36     90.62     74.46     70.00\n",
      "      4       77.01     90.62     72.28     68.12\n",
      "      5       78.78     90.00     74.46     71.88\n",
      "      6       79.19     90.62     74.46     72.50\n",
      "      7       79.72     91.25     72.28     75.62\n",
      "      8       79.87     90.62     73.37     75.62\n",
      "      9       80.68     91.25     73.91     76.88\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0219 0.1126 0.3645]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8068 \n",
      "Accuracies (test): [0.9125, 0.7391, 0.7688]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 4\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "      0       73.41     88.75     74.46     75.62     54.81\n",
      "      1       79.14     91.88     73.37     74.38     76.92\n",
      "      2       80.12     94.38     70.65     75.62     79.81\n",
      "      3       79.49     93.12     70.65     74.38     79.81\n",
      "      4       81.11     93.12     70.11     75.62     85.58\n",
      "      5       79.26     93.12     68.48     75.62     79.81\n",
      "      6       80.70     93.75     68.48     75.00     85.58\n",
      "      7       80.47     93.12     70.11     75.00     83.65\n",
      "      8       81.61     93.12     69.57     76.25     87.50\n",
      "      9       79.72     92.50     69.02     75.62     81.73\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0923 0.3314 0.7661]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7972 \n",
      "Accuracies (test): [0.925, 0.6902, 0.7562, 0.8173]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 5\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "      0       79.87     91.25     71.20     72.50     82.69     81.71\n",
      "      1       79.99     92.50     70.65     71.88     80.77     84.15\n",
      "      2       79.86     91.88     71.74     70.62     77.88     87.20\n",
      "      3       80.23     91.88     70.65     71.25     80.77     86.59\n",
      "      4       80.11     91.88     70.11     70.00     80.77     87.80\n",
      "      5       79.68     91.88     69.02     70.62     77.88     89.02\n",
      "      6       79.99     90.00     71.20     71.25     77.88     89.63\n",
      "      7       79.85     91.25     70.65     69.38     80.77     87.20\n",
      "      8       79.63     91.88     69.02     70.62     78.85     87.80\n",
      "      9       79.49     91.25     69.57     70.00     78.85     87.80\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0019 0.0195 0.1178]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7949 \n",
      "Accuracies (test): [0.9125, 0.6957, 0.7, 0.7885, 0.878]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 6\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------\n",
      "      0       79.06     91.88     67.39     72.50     76.92     89.63     76.06\n",
      "      1       80.77     93.12     69.02     74.38     79.81     89.02     79.26\n",
      "      2       80.84     93.12     64.67     75.62     78.85     90.85     81.91\n",
      "      3       80.17     92.50     65.76     75.62     75.00     90.24     81.91\n",
      "      4       80.10     93.75     66.85     73.75     75.96     88.41     81.91\n",
      "      5       80.63     93.12     66.30     74.38     76.92     89.02     84.04\n",
      "      6       80.46     93.75     63.04     75.62     77.88     88.41     84.04\n",
      "      7       80.96     94.38     64.67     75.00     78.85     87.20     85.64\n",
      "      8       80.01     95.00     64.13     75.62     74.04     87.20     84.04\n",
      "      9       80.62     93.75     64.67     76.88     75.00     87.80     85.64\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0027 0.025  0.1334]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8062 \n",
      "Accuracies (test): [0.9375, 0.6467, 0.7688, 0.75, 0.878, 0.8564]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 7\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------\n",
      "      0       79.63     92.50     64.13     73.75     70.19     84.15     86.17     86.54\n",
      "      1       80.54     93.12     63.04     74.38     73.08     84.15     85.64     90.38\n",
      "      2       80.44     93.75     65.22     73.75     70.19     84.15     85.64     90.38\n",
      "      3       80.31     93.12     63.59     75.62     71.15     84.15     85.11     89.42\n",
      "      4       79.92     94.38     63.04     73.12     71.15     84.15     85.11     88.46\n",
      "      5       79.99     93.12     64.13     71.88     72.12     84.76     83.51     90.38\n",
      "      6       80.72     93.75     65.22     76.25     71.15     84.76     83.51     90.38\n",
      "      7       79.93     92.50     65.76     71.88     70.19     84.76     84.04     90.38\n",
      "      8       80.62     93.75     63.59     72.50     70.19     85.37     85.64     93.27\n",
      "      9       80.04     93.75     63.59     70.62     70.19     84.15     85.64     92.31\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0019 0.017  0.0883]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8004 \n",
      "Accuracies (test): [0.9375, 0.6359, 0.7062, 0.7019, 0.8415, 0.8564, 0.9231]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 8\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------\n",
      "      0       80.41     95.00     65.76     76.88     71.15     84.76     84.57     91.35     73.78\n",
      "      1       80.61     95.00     67.39     73.12     73.08     85.37     82.98     92.31     75.61\n",
      "      2       80.72     95.00     66.85     71.25     73.08     85.98     83.51     93.27     76.83\n",
      "      3       81.21     95.00     67.39     72.50     74.04     85.98     85.64     92.31     76.83\n",
      "      4       80.66     94.38     66.85     71.25     71.15     85.98     83.51     92.31     79.88\n",
      "      5       81.36     94.38     67.93     72.50     75.96     85.98     83.51     91.35     79.27\n",
      "      6       80.57     95.00     65.76     71.25     74.04     86.59     81.91     91.35     78.66\n",
      "      7       81.24     93.12     70.11     71.88     73.08     85.98     85.11     91.35     79.27\n",
      "      8       80.79     91.88     65.22     71.88     74.04     87.80     84.57     92.31     78.66\n",
      "      9       81.31     93.12     69.02     71.88     74.04     86.59     86.17     90.38     79.27\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0035 0.0353 0.1987]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8131 \n",
      "Accuracies (test): [0.9312, 0.6902, 0.7188, 0.7404, 0.8659, 0.8617, 0.9038, 0.7927]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 9\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------\n",
      "      0       78.69     92.50     71.74     69.38     73.08     87.20     80.85     87.50     76.83     69.15\n",
      "      1       78.30     91.88     66.85     69.38     72.12     85.98     80.32     86.54     79.27     72.34\n",
      "      2       78.50     91.25     67.93     68.12     70.19     85.98     83.51     87.50     78.66     73.40\n",
      "      3       78.12     90.62     66.85     68.75     70.19     86.59     81.38     87.50     76.22     75.00\n",
      "      4       78.62     89.38     69.02     68.75     69.23     86.59     82.45     87.50     78.05     76.60\n",
      "      5       78.17     90.62     64.67     67.50     70.19     87.20     84.57     86.54     76.22     76.06\n",
      "      6       78.28     90.62     66.85     68.75     69.23     84.76     83.51     85.58     78.05     77.13\n",
      "      7       77.89     89.38     63.59     68.75     70.19     84.76     82.45     85.58     78.66     77.66\n",
      "      8       77.70     89.38     65.76     67.50     68.27     84.76     82.45     85.58     77.44     78.19\n",
      "      9       78.59     90.00     68.48     70.62     69.23     82.93     84.04     87.50     76.83     77.66\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0068 0.051  0.2312]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7859 \n",
      "Accuracies (test): [0.9, 0.6848, 0.7062, 0.6923, 0.8293, 0.8404, 0.875, 0.7683, 0.7766]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 10\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------\n",
      "      0       80.07     91.25     64.67     71.25     67.31     82.93     84.57     87.50     78.66     80.32      92.27\n",
      "      1       79.94     93.12     63.59     70.62     68.27     82.32     82.98     88.46     77.44     80.32      92.27\n",
      "      2       80.82     93.75     64.67     70.62     70.19     84.76     85.11     88.46     78.05     80.32      92.27\n",
      "      3       79.83     93.12     65.76     70.00     67.31     81.10     82.45     86.54     77.44     81.38      93.18\n",
      "      4       80.18     93.12     65.76     68.12     68.27     81.71     84.04     86.54     78.66     81.91      93.64\n",
      "      5       80.05     92.50     66.30     69.38     67.31     82.32     84.04     85.58     78.05     81.38      93.64\n",
      "      6       79.37     91.25     63.04     70.00     66.35     82.32     81.38     86.54     77.44     80.85      94.55\n",
      "      7       79.25     92.50     64.13     66.25     67.31     82.32     82.98     85.58     77.44     80.32      93.64\n",
      "      8       79.91     90.62     64.67     70.62     66.35     84.15     83.51     84.62     78.66     80.85      95.00\n",
      "      9       79.75     91.88     64.67     69.38     68.27     82.32     81.91     85.58     77.44     81.91      94.09\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0006 0.0063 0.0467]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7974 \n",
      "Accuracies (test): [0.9188, 0.6467, 0.6938, 0.6827, 0.8232, 0.8191, 0.8558, 0.7744, 0.8191, 0.9409]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 11\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------\n",
      "      0       80.72     90.00     64.13     66.88     67.31     82.32     84.04     87.50     79.88     76.60      95.00      94.23\n",
      "      1       81.42     93.12     66.30     68.12     64.42     83.54     83.51     88.46     78.66     79.26      95.00      95.19\n",
      "      2       81.40     92.50     66.85     70.00     65.38     82.93     82.98     89.42     77.44     78.72      95.00      94.23\n",
      "      3       81.32     92.50     66.30     69.38     66.35     81.71     81.38     90.38     78.05     79.26      95.91      93.27\n",
      "      4       80.43     92.50     62.50     69.38     65.38     82.32     81.91     86.54     76.83     77.66      95.45      94.23\n",
      "      5       80.83     92.50     65.76     70.00     65.38     82.32     83.51     86.54     76.22     78.19      95.45      93.27\n",
      "      6       80.86     93.12     63.59     70.00     66.35     82.32     83.51     87.50     75.61     78.72      94.55      94.23\n",
      "      7       80.65     93.12     64.67     67.50     64.42     82.93     81.91     86.54     78.05     79.26      94.55      94.23\n",
      "      8       80.97     93.12     66.30     70.62     65.38     83.54     81.38     86.54     76.83     78.19      95.45      93.27\n",
      "      9       81.13     92.50     67.39     71.88     64.42     83.54     81.38     85.58     76.22     80.85      95.45      93.27\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0031 0.0264 0.1608]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8113 \n",
      "Accuracies (test): [0.925, 0.6739, 0.7188, 0.6442, 0.8354, 0.8138, 0.8558, 0.7622, 0.8085, 0.9545, 0.9327]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 12\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------\n",
      "      0       80.06     87.50     61.41     69.38     67.31     80.49     78.72     81.73     75.61     79.26      92.73      93.27      93.33\n",
      "      1       80.41     90.00     59.78     65.62     65.38     81.71     80.85     83.65     76.83     78.19      93.64      94.23      95.00\n",
      "      2       80.05     89.38     63.04     67.50     62.50     81.71     79.79     83.65     75.00     77.13      93.64      92.31      95.00\n",
      "      3       80.41     90.00     63.04     65.62     62.50     82.32     80.85     82.69     77.44     77.66      94.55      93.27      95.00\n",
      "      4       80.71     90.62     64.67     66.88     62.50     81.71     79.79     83.65     78.66     77.13      94.09      93.27      95.56\n",
      "      5       80.50     89.38     60.87     65.62     62.50     82.32     82.98     82.69     78.66     77.13      95.00      93.27      95.56\n",
      "      6       80.59     90.00     60.87     67.50     65.38     80.49     81.91     83.65     76.83     77.66      94.55      93.27      95.00\n",
      "      7       80.22     89.38     61.96     66.88     61.54     79.88     80.32     83.65     77.44     79.26      94.09      93.27      95.00\n",
      "      8       80.26     89.38     61.41     68.12     62.50     80.49     81.38     81.73     77.44     78.72      93.64      93.27      95.00\n",
      "      9       80.51     90.62     63.04     67.50     61.54     79.27     79.79     82.69     78.66     78.72      94.55      94.23      95.56\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0009 0.0086 0.06  ]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8051 \n",
      "Accuracies (test): [0.9062, 0.6304, 0.675, 0.6154, 0.7927, 0.7979, 0.8269, 0.7866, 0.7872, 0.9455, 0.9423, 0.9556]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 13\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------\n",
      "      0       81.10     87.50     64.67     66.25     62.50     80.49     80.32     81.73     76.22     77.13      93.18      94.23      93.89      96.15\n",
      "      1       81.43     90.00     64.13     65.62     58.65     79.27     82.45     84.62     76.83     77.13      94.09      94.23      94.44      97.12\n",
      "      2       81.54     88.75     65.22     68.75     59.62     78.05     81.91     82.69     78.05     77.66      94.55      94.23      94.44      96.15\n",
      "      3       81.70     88.75     67.93     65.62     59.62     79.27     80.32     84.62     76.22     79.79      93.64      94.23      95.00      97.12\n",
      "      4       81.12     90.00     64.67     64.38     58.65     79.88     79.26     83.65     78.05     76.60      94.09      94.23      95.00      96.15\n",
      "      5       81.24     89.38     66.85     64.38     59.62     79.27     77.13     82.69     78.66     77.13      94.55      93.27      96.11      97.12\n",
      "      6       80.92     89.38     64.67     66.88     58.65     78.66     77.13     84.62     78.05     76.06      95.00      92.31      94.44      96.15\n",
      "      7       81.22     88.75     63.59     66.88     58.65     81.71     76.60     84.62     78.05     77.13      94.09      94.23      94.44      97.12\n",
      "      8       81.23     88.12     63.04     68.12     57.69     80.49     79.79     82.69     77.44     77.66      95.00      94.23      95.56      96.15\n",
      "      9       81.33     88.75     65.22     67.50     56.73     81.10     78.72     83.65     78.05     76.60      95.00      94.23      95.56      96.15\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0004 0.0057 0.0455]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8133 \n",
      "Accuracies (test): [0.8875, 0.6522, 0.675, 0.5673, 0.811, 0.7872, 0.8365, 0.7805, 0.766, 0.95, 0.9423, 0.9556, 0.9615]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 14\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       81.02     88.75     66.30     68.12     55.77     79.27     77.13     82.69     78.05     76.60      93.64      96.15      94.44      95.19      82.14\n",
      "      1       80.71     87.50     63.59     66.88     61.54     80.49     77.13     80.77     77.44     77.13      95.45      92.31      92.22      97.12      80.36\n",
      "      2       81.31     87.50     64.13     66.25     57.69     81.10     80.32     84.62     77.44     77.13      94.55      93.27      93.33      97.12      83.93\n",
      "      3       81.35     88.75     61.96     67.50     61.54     79.88     79.26     82.69     77.44     77.66      95.00      93.27      93.89      96.15      83.93\n",
      "      4       81.35     87.50     63.04     66.25     59.62     81.10     78.72     81.73     76.22     78.72      95.00      93.27      95.00      95.19      87.50\n",
      "      5       82.18     88.75     64.67     66.88     62.50     81.71     80.85     82.69     79.27     78.72      94.55      94.23      95.56      96.15      83.93\n",
      "      6       81.93     90.00     63.59     66.88     61.54     82.32     79.79     81.73     76.83     77.66      95.45      95.19      95.00      97.12      83.93\n",
      "      7       81.33     89.38     63.04     64.38     58.65     82.32     78.72     80.77     76.83     78.72      94.55      95.19      95.00      97.12      83.93\n",
      "      8       81.55     90.00     63.59     67.50     57.69     81.10     79.79     80.77     76.83     79.26      94.55      95.19      94.44      97.12      83.93\n",
      "      9       81.67     88.75     61.96     68.75     59.62     79.88     79.26     81.73     76.22     80.32      95.00      95.19      93.89      97.12      85.71\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0006 0.0113 0.1923]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8167 \n",
      "Accuracies (test): [0.8875, 0.6196, 0.6875, 0.5962, 0.7988, 0.7926, 0.8173, 0.7622, 0.8032, 0.95, 0.9519, 0.9389, 0.9712, 0.8571]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 15\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       81.75     87.50     66.30     67.50     58.65     82.93     78.72     78.85     76.83     75.53      94.09      94.23      93.89      95.19      85.71      90.38\n",
      "      1       81.63     89.38     64.13     66.25     56.73     81.71     80.32     81.73     77.44     76.06      94.09      93.27      93.89      95.19      83.93      90.38\n",
      "      2       81.77     87.50     64.13     66.25     59.62     79.88     79.79     79.81     78.05     78.72      93.18      93.27      93.89      96.15      83.93      92.31\n",
      "      3       81.63     87.50     61.96     67.50     56.73     79.27     82.45     81.73     79.27     78.19      93.18      92.31      93.89      94.23      83.93      92.31\n",
      "      4       81.98     87.50     61.96     67.50     58.65     81.10     82.98     79.81     77.44     78.19      94.09      92.31      93.89      97.12      83.93      93.27\n",
      "      5       81.80     88.75     63.04     65.62     55.77     82.32     79.79     80.77     78.66     78.72      96.36      93.27      93.33      96.15      82.14      92.31\n",
      "      6       82.31     87.50     64.67     65.62     59.62     82.93     81.38     79.81     79.27     79.26      95.00      93.27      93.89      96.15      83.93      92.31\n",
      "      7       82.23     88.12     63.04     68.12     55.77     82.93     79.79     82.69     78.66     79.79      95.45      92.31      94.44      96.15      83.93      92.31\n",
      "      8       81.64     86.25     61.41     65.62     56.73     82.32     79.79     81.73     79.88     78.72      94.09      91.35      93.33      97.12      83.93      92.31\n",
      "      9       82.50     86.25     65.76     66.88     57.69     84.15     80.85     80.77     80.49     78.19      95.45      93.27      94.44      97.12      83.93      92.31\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0011 0.0097 0.0584]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8250 \n",
      "Accuracies (test): [0.8625, 0.6576, 0.6688, 0.5769, 0.8415, 0.8085, 0.8077, 0.8049, 0.7819, 0.9545, 0.9327, 0.9444, 0.9712, 0.8393, 0.9231]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 16\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       82.67     88.12     61.96     67.50     64.42     80.49     81.91     81.73     78.05     77.13      94.09      91.35      92.78      95.19      87.50      91.35      89.13\n",
      "      1       82.58     89.38     62.50     66.88     59.62     81.71     80.32     80.77     77.44     78.19      94.09      92.31      91.67      94.23      87.50      92.31      92.39\n",
      "      2       82.34     88.12     61.96     63.75     59.62     81.71     80.85     78.85     78.05     78.19      95.00      92.31      93.33      94.23      85.71      92.31      93.48\n",
      "      3       82.33     88.75     60.33     65.62     59.62     79.88     81.38     81.73     78.05     76.60      95.00      92.31      91.11      95.19      85.71      91.35      94.57\n",
      "      4       82.70     89.38     61.96     66.25     59.62     82.32     82.98     79.81     76.22     76.60      95.00      91.35      92.78      95.19      85.71      92.31      95.65\n",
      "      5       81.94     88.75     60.33     66.25     56.73     80.49     84.57     77.88     77.44     76.60      95.00      90.38      91.67      94.23      83.93      93.27      93.48\n",
      "      6       82.64     88.75     60.33     66.88     60.58     81.71     83.51     80.77     76.83     77.13      95.00      91.35      92.78      95.19      85.71      92.31      93.48\n",
      "      7       82.41     88.75     59.78     63.75     57.69     82.32     82.98     80.77     76.83     76.60      95.45      92.31      93.33      96.15      83.93      93.27      94.57\n",
      "      8       82.06     88.75     59.24     64.38     54.81     80.49     82.98     79.81     79.27     75.53      94.09      90.38      92.78      95.19      87.50      94.23      93.48\n",
      "      9       82.41     88.75     60.33     66.88     58.65     81.71     83.51     79.81     78.05     74.47      94.55      91.35      92.78      95.19      85.71      93.27      93.48\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0021 0.0189 0.1066]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8241 \n",
      "Accuracies (test): [0.8875, 0.6033, 0.6688, 0.5865, 0.8171, 0.8351, 0.7981, 0.7805, 0.7447, 0.9455, 0.9135, 0.9278, 0.9519, 0.8571, 0.9327, 0.9348]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 17\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       80.46     88.12     59.78     64.38     57.69     79.88     81.38     80.77     76.83     72.87      94.09      89.42      90.56      91.35      80.36      94.23      90.22      75.89\n",
      "      1       81.57     88.75     60.87     67.50     56.73     78.66     84.57     81.73     77.44     72.87      94.09      90.38      91.67      95.19      82.14      97.12      90.22      76.79\n",
      "      2       81.36     88.12     63.04     66.88     55.77     80.49     80.32     82.69     76.22     72.87      93.18      90.38      90.56      94.23      82.14      95.19      92.39      78.57\n",
      "      3       81.28     88.75     60.33     65.00     57.69     79.27     79.79     81.73     78.66     75.00      93.64      89.42      89.44      93.27      83.93      97.12      90.22      78.57\n",
      "      4       81.02     88.12     60.33     66.88     58.65     79.27     79.79     81.73     78.05     73.94      94.09      87.50      91.67      93.27      80.36      94.23      89.13      80.36\n",
      "      5       81.46     88.75     61.96     66.25     54.81     79.88     81.38     82.69     77.44     75.00      94.09      88.46      91.11      92.31      82.14      94.23      91.30      83.04\n",
      "      6       81.26     89.38     60.87     65.62     58.65     77.44     80.32     81.73     75.61     73.94      94.09      90.38      90.56      93.27      80.36      94.23      90.22      84.82\n",
      "      7       81.45     88.75     58.15     68.12     59.62     79.27     81.38     81.73     78.66     72.34      93.18      89.42      90.00      93.27      80.36      95.19      91.30      83.93\n",
      "      8       81.53     88.75     59.78     66.25     60.58     77.44     78.72     79.81     75.61     73.40      95.00      86.54      91.11      94.23      85.71      95.19      91.30      86.61\n",
      "      9       82.11     88.12     61.96     68.75     62.50     78.05     79.26     81.73     78.05     71.81      94.55      87.50      90.56      94.23      85.71      95.19      91.30      86.61\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0045 0.0355 0.1588]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8211 \n",
      "Accuracies (test): [0.8812, 0.6196, 0.6875, 0.625, 0.7805, 0.7926, 0.8173, 0.7805, 0.7181, 0.9455, 0.875, 0.9056, 0.9423, 0.8571, 0.9519, 0.913, 0.8661]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 18\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       81.37     88.12     59.78     70.00     58.65     77.44     78.72     79.81     78.05     68.09      93.18      87.50      90.56      93.27      82.14      93.27      92.39      84.82      88.89\n",
      "      1       82.24     88.75     59.78     71.88     59.62     78.05     78.72     79.81     75.61     70.74      94.09      88.46      92.22      95.19      85.71      94.23      90.22      86.61      90.56\n",
      "      2       81.51     88.75     60.33     68.12     59.62     78.05     78.72     80.77     73.78     71.81      94.09      87.50      90.00      93.27      83.93      94.23      90.22      83.93      90.00\n",
      "      3       81.63     89.38     59.78     68.75     57.69     78.66     78.72     78.85     75.00     70.74      94.09      88.46      91.67      94.23      85.71      92.31      90.22      83.93      91.11\n",
      "      4       81.81     89.38     58.15     71.88     58.65     78.05     78.19     77.88     76.22     70.74      95.00      88.46      91.67      93.27      85.71      94.23      90.22      84.82      90.00\n",
      "      5       81.75     89.38     58.15     71.25     56.73     76.83     78.19     78.85     75.00     71.28      94.55      89.42      92.78      93.27      87.50      94.23      90.22      82.14      91.67\n",
      "      6       81.66     89.38     58.70     68.12     56.73     77.44     78.19     77.88     75.61     69.15      95.00      88.46      93.89      94.23      87.50      93.27      90.22      83.93      92.22\n",
      "      7       80.95     89.38     62.50     68.12     54.81     76.22     78.19     77.88     74.39     69.68      94.09      89.42      93.33      94.23      82.14      92.31      89.13      81.25      90.00\n",
      "      8       81.42     88.75     56.52     68.75     57.69     81.10     78.72     76.92     73.17     70.21      94.55      89.42      93.89      93.27      85.71      92.31      90.22      82.14      92.22\n",
      "      9       81.26     88.12     60.33     68.75     56.73     76.83     78.19     77.88     74.39     69.68      95.45      86.54      92.78      93.27      82.14      94.23      91.30      83.93      92.22\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0015 0.0197 0.1217]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8126 \n",
      "Accuracies (test): [0.8812, 0.6033, 0.6875, 0.5673, 0.7683, 0.7819, 0.7788, 0.7439, 0.6968, 0.9545, 0.8654, 0.9278, 0.9327, 0.8214, 0.9423, 0.913, 0.8393, 0.9222]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 19\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       81.29     87.50     59.78     68.75     59.62     78.05     79.26     76.92     78.05     70.21      94.55      87.50      90.56      93.27      85.71      93.27      89.13      81.25      90.56      80.56\n",
      "      1       81.46     88.75     57.61     70.00     58.65     80.49     77.13     77.88     78.66     73.40      94.55      84.62      91.11      92.31      85.71      95.19      88.04      84.82      91.11      77.78\n",
      "      2       81.08     88.75     55.98     69.38     56.73     79.88     78.19     76.92     77.44     70.74      94.09      87.50      90.56      92.31      83.93      95.19      89.13      82.14      92.22      79.44\n",
      "      3       81.06     88.75     56.52     71.25     57.69     77.44     79.79     77.88     77.44     68.62      95.45      85.58      88.89      92.31      87.50      94.23      88.04      82.14      90.56      80.00\n",
      "      4       80.89     89.38     57.61     68.12     56.73     80.49     78.19     75.96     77.44     70.74      95.45      87.50      89.44      92.31      83.93      94.23      85.87      83.04      90.56      80.00\n",
      "      5       80.64     90.62     54.89     68.12     54.81     76.22     79.79     75.00     78.05     70.21      94.55      85.58      90.56      93.27      83.93      97.12      85.87      81.25      91.67      80.56\n",
      "      6       80.70     90.00     54.89     66.88     56.73     79.27     78.19     75.96     78.05     71.81      94.55      83.65      88.33      92.31      83.93      96.15      85.87      83.93      91.67      81.11\n",
      "      7       80.66     89.38     57.07     66.88     56.73     75.61     79.26     75.96     78.05     69.15      95.00      86.54      88.89      92.31      83.93      95.19      88.04      83.93      90.56      80.00\n",
      "      8       80.78     89.38     53.80     66.25     56.73     78.66     78.19     76.92     77.44     68.09      95.45      86.54      89.44      91.35      83.93      96.15      88.04      83.93      91.67      82.78\n",
      "      9       80.47     90.62     56.52     66.25     56.73     75.61     78.72     75.96     76.22     68.62      95.00      88.46      87.78      91.35      83.93      96.15      89.13      83.04      90.00      78.89\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0042 0.0387 0.1814]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8047 \n",
      "Accuracies (test): [0.9062, 0.5652, 0.6625, 0.5673, 0.7561, 0.7872, 0.7596, 0.7622, 0.6862, 0.95, 0.8846, 0.8778, 0.9135, 0.8393, 0.9615, 0.8913, 0.8304, 0.9, 0.7889]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 20\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       79.86     81.88     56.52     65.62     55.77     75.61     78.19     74.04     74.39     67.02      94.55      85.58      92.22      90.38      78.57      91.35      86.96      82.14      90.00      80.00      96.43\n",
      "      1       81.10     87.50     60.87     66.88     55.77     78.05     78.72     75.00     74.39     69.68      95.00      84.62      89.44      92.31      80.36      95.19      86.96      83.04      90.00      80.00      98.21\n",
      "      2       80.93     86.88     60.33     65.62     57.69     77.44     79.26     77.88     73.78     67.02      95.45      84.62      90.56      91.35      78.57      94.23      85.87      83.93      91.11      78.89      98.21\n",
      "      3       81.33     87.50     59.78     65.00     56.73     79.88     81.38     75.96     74.39     70.74      94.55      87.50      88.33      91.35      80.36      94.23      86.96      82.14      91.11      80.56      98.21\n",
      "      4       81.50     88.75     60.87     68.75     59.62     80.49     80.32     75.96     74.39     69.68      95.45      84.62      89.44      91.35      80.36      95.19      86.96      80.36      90.00      81.11      96.43\n",
      "      5       80.96     89.38     57.07     65.00     57.69     78.05     78.72     76.92     74.39     70.21      94.55      86.54      89.44      91.35      80.36      94.23      86.96      81.25      90.00      78.89      98.21\n",
      "      6       80.73     89.38     56.52     66.25     56.73     79.88     77.66     75.96     74.39     68.09      94.55      84.62      87.78      91.35      80.36      94.23      86.96      82.14      90.56      78.89      98.21\n",
      "      7       81.20     88.12     57.07     66.25     56.73     77.44     80.32     75.96     73.78     70.74      94.55      84.62      89.44      91.35      82.14      95.19      86.96      83.93      91.11      80.00      98.21\n",
      "      8       81.12     88.75     59.24     66.88     55.77     77.44     77.66     75.96     73.78     69.68      94.55      87.50      89.44      92.31      80.36      94.23      86.96      83.04      90.56      80.00      98.21\n",
      "      9       81.18     87.50     57.07     69.38     57.69     78.05     79.26     75.96     75.00     68.09      95.91      87.50      87.78      91.35      80.36      93.27      88.04      82.14      90.00      81.11      98.21\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0008 0.0073 0.0423]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8118 \n",
      "Accuracies (test): [0.875, 0.5707, 0.6938, 0.5769, 0.7805, 0.7926, 0.7596, 0.75, 0.6809, 0.9591, 0.875, 0.8778, 0.9135, 0.8036, 0.9327, 0.8804, 0.8214, 0.9, 0.8111, 0.9821]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 21\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       81.44     88.75     60.33     66.25     54.81     75.61     80.32     76.92     73.17     70.74      93.64      86.54      90.56      91.35      83.93      93.27      86.96      83.04      89.44      79.44      98.21      87.00\n",
      "      1       81.70     88.12     58.70     68.75     56.73     75.61     79.26     75.96     76.22     69.15      94.55      88.46      90.56      91.35      82.14      93.27      89.13      81.25      90.56      77.78      98.21      90.00\n",
      "      2       81.57     88.75     59.24     65.62     54.81     79.88     80.85     76.92     75.00     70.74      95.91      84.62      89.44      91.35      78.57      94.23      86.96      83.04      90.00      78.89      98.21      90.00\n",
      "      3       81.79     89.38     61.41     65.62     53.85     76.83     78.72     76.92     76.22     69.15      94.09      86.54      91.11      93.27      82.14      95.19      86.96      83.04      90.56      78.33      98.21      90.00\n",
      "      4       82.02     90.62     59.24     67.50     56.73     73.78     79.79     76.92     76.22     68.62      95.45      85.58      91.11      92.31      83.93      95.19      88.04      84.82      88.89      79.44      98.21      90.00\n",
      "      5       82.47     90.00     60.33     66.88     56.73     76.83     79.26     78.85     77.44     68.09      95.00      85.58      88.89      91.35      87.50      94.23      90.22      84.82      91.67      78.89      98.21      91.00\n",
      "      6       82.13     90.00     59.24     68.12     55.77     74.39     79.79     77.88     77.44     68.09      95.45      86.54      91.67      94.23      82.14      94.23      86.96      83.04      90.56      80.00      98.21      91.00\n",
      "      7       81.82     90.00     59.24     67.50     53.85     75.00     79.79     78.85     77.44     67.02      95.00      83.65      91.11      93.27      83.93      94.23      88.04      82.14      90.00      80.00      98.21      90.00\n",
      "      8       82.05     90.00     59.24     65.62     55.77     73.78     80.32     79.81     76.83     68.62      95.45      84.62      91.67      92.31      82.14      94.23      88.04      83.93      90.00      80.56      98.21      92.00\n",
      "      9       82.15     90.00     59.78     66.88     55.77     75.61     80.32     78.85     78.66     66.49      95.91      85.58      88.89      92.31      85.71      94.23      88.04      83.93      90.00      80.00      98.21      90.00\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0036 0.0284 0.1293]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8215 \n",
      "Accuracies (test): [0.9, 0.5978, 0.6688, 0.5577, 0.7561, 0.8032, 0.7885, 0.7866, 0.6649, 0.9591, 0.8558, 0.8889, 0.9231, 0.8571, 0.9423, 0.8804, 0.8393, 0.9, 0.8, 0.9821, 0.9]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 22\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       81.31     89.38     58.70     64.38     55.77     74.39     79.26     79.81     75.00     68.62      95.00      85.58      90.00      93.27      83.93      94.23      90.22      81.25      91.11      77.22      98.21      89.00      74.42\n",
      "      1       81.75     86.25     58.70     65.62     57.69     75.00     78.72     80.77     76.83     71.81      95.45      84.62      90.56      93.27      83.93      93.27      85.87      83.93      92.22      80.00      98.21      89.00      76.74\n",
      "      2       81.56     86.25     59.78     66.25     53.85     76.22     78.72     79.81     77.44     71.28      94.55      85.58      90.56      91.35      82.14      93.27      88.04      83.93      91.11      78.89      98.21      88.00      79.07\n",
      "      3       82.38     88.12     61.41     68.75     56.73     76.83     79.79     79.81     77.44     72.87      95.00      84.62      89.44      92.31      83.93      92.31      88.04      84.82      91.11      80.00      98.21      87.00      83.72\n",
      "      4       81.21     87.50     60.33     68.75     56.73     75.00     75.53     76.92     77.44     70.74      94.09      85.58      88.89      91.35      83.93      93.27      84.78      83.93      90.00      78.33      98.21      84.00      81.40\n",
      "      5       81.51     86.88     59.24     70.00     52.88     75.00     79.79     75.96     78.05     70.21      93.18      85.58      90.56      92.31      80.36      93.27      89.13      82.14      91.11      80.00      98.21      88.00      81.40\n",
      "      6       81.31     88.75     59.78     69.38     53.85     77.44     76.60     75.96     75.61     72.34      94.09      85.58      91.11      92.31      82.14      93.27      84.78      83.93      88.89      80.00      98.21      84.00      80.81\n",
      "      7       81.43     87.50     58.70     69.38     55.77     76.83     80.85     75.96     74.39     67.02      95.91      86.54      90.56      92.31      80.36      91.35      86.96      82.14      90.00      82.22      98.21      86.00      82.56\n",
      "      8       81.24     85.62     59.24     70.00     56.73     76.22     77.13     75.96     72.56     69.68      95.00      87.50      88.89      92.31      83.93      92.31      86.96      80.36      91.11      80.00      98.21      85.00      82.56\n",
      "      9       80.91     86.25     58.70     70.62     54.81     76.22     75.00     74.04     73.78     69.68      93.64      85.58      91.67      91.35      83.93      91.35      88.04      79.46      90.00      80.56      98.21      84.00      83.14\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.007  0.0553 0.2549]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8091 \n",
      "Accuracies (test): [0.8625, 0.587, 0.7062, 0.5481, 0.7622, 0.75, 0.7404, 0.7378, 0.6968, 0.9364, 0.8558, 0.9167, 0.9135, 0.8393, 0.9135, 0.8804, 0.7946, 0.9, 0.8056, 0.9821, 0.84, 0.8314]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 23\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       80.46     88.12     57.07     68.75     54.81     74.39     73.94     75.96     73.78     71.28      94.55      82.69      88.33      87.50      83.93      90.38      88.04      82.14      89.44      77.22      98.21      84.00      80.81      85.23\n",
      "      1       81.22     90.00     59.78     68.75     54.81     76.22     78.72     76.92     72.56     68.62      95.00      83.65      89.44      91.35      83.93      93.27      90.22      79.46      89.44      78.89      98.21      85.00      79.65      84.09\n",
      "      2       80.94     88.12     60.33     66.25     54.81     73.17     76.60     75.96     73.78     69.68      95.00      83.65      90.00      89.42      82.14      93.27      90.22      79.46      90.00      78.89      98.21      86.00      82.56      84.09\n",
      "      3       80.76     86.25     55.43     67.50     53.85     73.78     75.53     77.88     76.22     67.55      95.00      83.65      90.00      88.46      82.14      91.35      92.39      78.57      91.11      78.33      98.21      86.00      81.98      86.36\n",
      "      4       80.99     88.12     60.33     68.75     55.77     75.61     78.19     76.92     72.56     68.62      94.09      83.65      90.00      88.46      82.14      92.31      91.30      78.57      88.89      78.33      98.21      86.00      80.81      85.23\n",
      "      5       81.24     88.12     58.15     66.88     52.88     74.39     77.66     75.96     76.22     69.15      95.45      84.62      90.56      90.38      83.93      92.31      92.39      80.36      88.89      77.78      98.21      86.00      81.98      86.36\n",
      "      6       81.42     88.75     60.33     63.75     53.85     75.61     77.13     76.92     76.83     71.28      94.09      85.58      90.56      88.46      82.14      93.27      92.39      80.36      89.44      77.78      98.21      87.00      82.56      86.36\n",
      "      7       81.26     89.38     58.70     66.25     55.77     74.39     78.72     77.88     75.61     68.09      95.45      84.62      90.00      90.38      83.93      91.35      92.39      77.68      88.89      77.22      98.21      87.00      80.81      86.36\n",
      "      8       81.17     88.75     58.70     66.25     52.88     74.39     80.32     76.92     74.39     69.15      94.55      84.62      90.56      89.42      82.14      92.31      91.30      79.46      89.44      78.89      98.21      86.00      81.98      86.36\n",
      "      9       81.10     88.12     59.24     65.62     50.96     73.78     75.53     76.92     75.00     69.15      95.00      83.65      91.67      90.38      83.93      93.27      91.30      78.57      90.00      78.33      98.21      86.00      84.30      86.36\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0029 0.0286 0.1586]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8110 \n",
      "Accuracies (test): [0.8812, 0.5924, 0.6562, 0.5096, 0.7378, 0.7553, 0.7692, 0.75, 0.6915, 0.95, 0.8365, 0.9167, 0.9038, 0.8393, 0.9327, 0.913, 0.7857, 0.9, 0.7833, 0.9821, 0.86, 0.843, 0.8636]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 24\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       80.62     88.75     59.78     67.50     48.08     76.83     75.53     76.92     73.17     69.15      93.18      83.65      90.00      90.38      76.79      89.42      88.04      75.00      90.00      77.78      98.21      86.00      80.81      87.50      92.31\n",
      "      1       80.96     90.62     59.78     64.38     50.00     77.44     76.60     76.92     70.73     69.15      93.64      83.65      90.56      90.38      78.57      90.38      90.22      75.00      90.00      77.22      98.21      84.00      83.14      85.23      97.12\n",
      "      2       81.68     90.00     59.78     68.12     49.04     76.22     77.66     76.92     71.95     70.21      94.09      83.65      90.00      91.35      80.36      93.27      91.30      76.79      90.00      77.78      98.21      86.00      83.14      86.36      98.08\n",
      "      3       81.19     88.75     60.33     67.50     47.12     76.83     76.60     75.96     70.12     69.15      94.55      82.69      88.89      90.38      80.36      94.23      91.30      77.68      91.11      75.56      98.21      87.00      81.98      85.23      97.12\n",
      "      4       81.47     91.25     59.78     67.50     47.12     78.05     77.66     75.00     71.34     72.34      93.64      83.65      90.56      90.38      78.57      90.38      91.30      78.57      91.11      76.11      98.21      87.00      83.14      86.36      96.15\n",
      "      5       81.65     90.00     59.24     67.50     49.04     76.22     77.66     75.00     69.51     68.62      95.00      84.62      90.56      90.38      82.14      95.19      91.30      77.68      91.11      77.22      98.21      87.00      83.14      85.23      98.08\n",
      "      6       81.18     90.00     60.33     68.12     49.04     76.22     77.13     74.04     68.90     70.74      92.27      83.65      90.56      90.38      82.14      90.38      90.22      76.79      91.11      77.78      98.21      84.00      81.98      86.36      98.08\n",
      "      7       81.07     89.38     60.87     66.25     52.88     75.00     76.60     75.96     68.29     68.09      92.73      83.65      91.11      88.46      82.14      89.42      92.39      75.00      91.11      78.33      98.21      86.00      82.56      84.09      97.12\n",
      "      8       81.27     89.38     61.41     67.50     48.08     74.39     77.13     75.00     71.34     69.68      93.64      84.62      91.67      89.42      80.36      89.42      91.30      77.68      92.78      78.89      98.21      86.00      82.56      82.95      97.12\n",
      "      9       81.40     90.00     59.78     65.00     50.00     74.39     77.13     75.00     69.51     67.55      94.09      84.62      90.56      89.42      83.93      93.27      91.30      76.79      91.11      77.22      98.21      88.00      83.14      86.36      97.12\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0007 0.0064 0.044 ]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8140 \n",
      "Accuracies (test): [0.9, 0.5978, 0.65, 0.5, 0.7439, 0.7713, 0.75, 0.6951, 0.6755, 0.9409, 0.8462, 0.9056, 0.8942, 0.8393, 0.9327, 0.913, 0.7679, 0.9111, 0.7722, 0.9821, 0.88, 0.8314, 0.8636, 0.9712]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 25\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       81.48     86.88     58.70     66.88     50.00     76.22     78.19     75.00     70.12     71.28      95.00      83.65      91.11      91.35      82.14      92.31      91.30      77.68      91.11      77.78      96.43      86.00      79.07      86.36      98.08      84.24\n",
      "      1       82.57     86.25     60.87     68.12     52.88     81.10     80.32     75.00     75.00     73.40      95.00      86.54      92.22      91.35      82.14      92.31      92.39      78.57      89.44      79.44      96.43      86.00      79.07      86.36      98.08      85.87\n",
      "      2       82.65     87.50     61.41     68.75     53.85     78.05     79.26     79.81     73.78     72.34      95.45      83.65      92.22      90.38      80.36      93.27      91.30      78.57      90.00      78.89      94.64      88.00      79.65      86.36      99.04      89.67\n",
      "      3       81.82     86.25     59.24     66.88     53.85     75.61     78.19     77.88     74.39     71.81      94.55      83.65      92.22      89.42      80.36      94.23      91.30      75.89      89.44      78.89      94.64      86.00      79.07      86.36      96.15      89.13\n",
      "      4       82.27     87.50     58.70     68.12     51.92     79.88     79.79     78.85     73.78     72.34      94.55      84.62      92.22      88.46      76.79      91.35      93.48      77.68      91.11      80.56      94.64      88.00      79.07      87.50      96.15      89.67\n",
      "      5       81.53     86.88     58.70     65.62     52.88     80.49     78.72     77.88     75.61     71.28      93.18      85.58      92.22      88.46      76.79      92.31      90.22      74.11      90.56      76.11      94.64      86.00      78.49      86.36      96.15      89.13\n",
      "      6       81.71     87.50     59.24     67.50     50.96     79.27     79.79     80.77     75.00     70.74      95.00      84.62      91.67      88.46      75.00      92.31      91.30      73.21      90.00      78.33      96.43      85.00      79.07      86.36      96.15      89.13\n",
      "      7       82.13     86.25     59.78     64.38     50.00     79.27     78.72     79.81     76.83     71.28      93.18      84.62      92.78      89.42      82.14      91.35      89.13      77.68      90.56      78.33      96.43      86.00      77.91      88.64      98.08      90.76\n",
      "      8       81.78     88.12     58.15     66.25     51.92     79.27     77.66     77.88     75.00     71.81      93.64      84.62      88.89      89.42      78.57      92.31      92.39      75.89      90.56      76.11      94.64      88.00      79.07      87.50      96.15      90.76\n",
      "      9       81.95     87.50     59.24     67.50     52.88     78.05     77.66     78.85     76.83     70.21      93.64      85.58      91.67      88.46      78.57      92.31      88.04      75.89      92.22      79.44      92.86      87.00      78.49      87.50      98.08      90.22\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0023 0.019  0.1025]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8195 \n",
      "Accuracies (test): [0.875, 0.5924, 0.675, 0.5288, 0.7805, 0.7766, 0.7885, 0.7683, 0.7021, 0.9364, 0.8558, 0.9167, 0.8846, 0.7857, 0.9231, 0.8804, 0.7589, 0.9222, 0.7944, 0.9286, 0.87, 0.7849, 0.875, 0.9808, 0.9022]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 26\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       81.48     86.88     55.43     65.62     49.04     78.66     76.60     79.81     75.61     70.74      94.09      81.73      90.56      89.42      78.57      86.54      90.22      78.57      91.11      82.22      94.64      87.00      79.07      82.95      95.19      91.85      86.31\n",
      "      1       81.86     88.12     56.52     66.88     48.08     76.22     77.66     75.00     75.61     69.68      95.45      86.54      91.11      90.38      76.79      90.38      89.13      79.46      92.22      78.33      94.64      85.00      80.23      86.36      99.04      90.76      88.69\n",
      "      2       81.87     87.50     58.15     66.88     57.69     74.39     76.60     75.00     74.39     71.81      93.64      83.65      90.00      91.35      78.57      87.50      88.04      80.36      90.56      81.67      92.86      85.00      77.91      86.36      98.08      91.30      89.29\n",
      "      3       82.38     88.75     59.78     65.62     52.88     77.44     76.06     76.92     76.83     73.40      93.64      83.65      89.44      87.50      78.57      89.42      92.39      79.46      91.67      81.11      96.43      88.00      77.91      87.50      98.08      90.76      88.69\n",
      "      4       81.69     86.25     58.70     66.88     52.88     75.00     78.72     75.00     74.39     71.28      94.55      82.69      90.00      89.42      75.00      88.46      90.22      76.79      90.56      82.22      96.43      86.00      79.07      87.50      97.12      91.30      87.50\n",
      "      5       82.19     86.88     59.78     66.25     54.81     73.78     79.26     75.00     78.66     72.34      95.00      81.73      90.00      88.46      80.36      88.46      90.22      79.46      90.56      82.22      96.43      86.00      79.07      85.23      97.12      91.85      88.10\n",
      "      6       81.92     87.50     59.78     67.50     50.96     75.00     80.32     75.96     75.00     68.62      93.64      84.62      91.67      89.42      75.00      87.50      90.22      80.36      92.78      82.22      94.64      87.00      80.23      86.36      97.12      89.13      87.50\n",
      "      7       81.99     86.88     60.87     65.62     53.85     73.78     79.79     75.00     74.39     71.81      94.55      84.62      91.67      90.38      78.57      88.46      86.96      77.68      91.67      80.56      94.64      87.00      81.98      84.09      97.12      92.39      87.50\n",
      "      8       82.00     87.50     60.87     67.50     53.85     74.39     78.19     75.00     71.95     69.68      94.55      84.62      89.44      90.38      78.57      89.42      91.30      78.57      91.67      80.00      94.64      87.00      80.23      85.23      98.08      91.85      87.50\n",
      "      9       82.17     88.75     60.87     67.50     53.85     73.78     80.32     75.00     73.17     70.21      92.73      84.62      90.00      90.38      80.36      88.46      88.04      81.25      92.22      80.56      94.64      89.00      80.23      84.09      97.12      91.30      88.10\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0036 0.0243 0.1184]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8217 \n",
      "Accuracies (test): [0.8875, 0.6087, 0.675, 0.5385, 0.7378, 0.8032, 0.75, 0.7317, 0.7021, 0.9273, 0.8462, 0.9, 0.9038, 0.8036, 0.8846, 0.8804, 0.8125, 0.9222, 0.8056, 0.9464, 0.89, 0.8023, 0.8409, 0.9712, 0.913, 0.881]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 27\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       81.75     88.12     58.70     66.25     49.04     73.17     76.60     75.00     75.61     69.15      92.27      79.81      90.00      90.38      76.79      88.46      86.96      78.57      90.56      82.22      94.64      85.00      81.98      85.23      97.12      89.67      87.50      98.48\n",
      "      1       81.81     86.88     58.70     61.88     44.23     72.56     78.19     75.96     75.00     72.87      93.18      81.73      90.56      91.35      78.57      89.42      89.13      83.04      89.44      80.00      94.64      86.00      79.07      85.23      96.15      89.13      88.10      97.73\n",
      "      2       82.16     88.12     61.41     65.00     44.23     75.00     78.72     73.08     74.39     73.94      92.73      81.73      88.33      89.42      78.57      88.46      91.30      81.25      91.67      82.22      94.64      85.00      81.98      85.23      98.08      88.59      87.50      97.73\n",
      "      3       82.69     88.12     61.41     66.25     53.85     75.61     81.38     76.92     72.56     70.21      92.73      84.62      90.56      90.38      75.00      88.46      89.13      79.46      91.67      83.33      94.64      86.00      80.81      85.23      98.08      89.67      88.69      97.73\n",
      "      4       82.76     89.38     61.41     65.00     50.96     75.00     80.32     78.85     74.39     70.21      92.27      84.62      89.44      89.42      78.57      88.46      90.22      79.46      92.78      82.22      94.64      87.00      80.23      86.36      96.15      90.76      88.69      97.73\n",
      "      5       82.66     88.75     59.24     65.00     51.92     73.78     79.79     77.88     75.00     70.21      93.64      84.62      89.44      88.46      76.79      88.46      91.30      80.36      91.67      81.11      96.43      85.00      80.23      86.36      98.08      92.39      87.50      98.48\n",
      "      6       82.91     89.38     57.61     65.62     51.92     75.61     79.79     75.96     73.78     68.09      94.09      83.65      91.11      87.50      78.57      88.46      90.22      82.14      92.22      81.67      96.43      89.00      81.40      88.64      97.12      90.76      89.29      98.48\n",
      "      7       82.16     87.50     58.70     65.62     49.04     72.56     77.66     75.96     74.39     70.74      92.27      83.65      90.56      89.42      80.36      88.46      91.30      80.36      90.00      79.44      94.64      87.00      79.65      86.36      97.12      90.22      87.50      97.73\n",
      "      8       82.55     89.38     59.78     65.62     51.92     75.00     76.60     75.96     72.56     69.15      94.09      82.69      89.44      89.42      80.36      90.38      91.30      79.46      91.11      81.11      96.43      87.00      81.40      85.23      97.12      90.76      88.69      96.97\n",
      "      9       82.36     88.75     59.24     66.88     49.04     73.78     76.06     75.96     75.00     72.87      95.00      84.62      88.89      89.42      76.79      87.50      89.13      80.36      91.67      80.56      96.43      86.00      78.49      86.36      98.08      90.22      88.10      98.48\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0015 0.0099 0.0567]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8236 \n",
      "Accuracies (test): [0.8875, 0.5924, 0.6688, 0.4904, 0.7378, 0.7606, 0.7596, 0.75, 0.7287, 0.95, 0.8462, 0.8889, 0.8942, 0.7679, 0.875, 0.8913, 0.8036, 0.9167, 0.8056, 0.9643, 0.86, 0.7849, 0.8636, 0.9808, 0.9022, 0.881, 0.9848]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 28\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       81.46     83.75     57.07     61.88     45.19     73.17     75.00     75.00     72.56     69.15      91.82      80.77      88.33      91.35      80.36      86.54      90.22      76.79      89.44      77.78      96.43      85.00      80.81      85.23      97.12      87.50      86.90      97.73      98.08\n",
      "      1       82.36     86.25     60.87     63.75     52.88     72.56     76.06     75.00     71.95     72.87      94.09      84.62      88.89      87.50      80.36      90.38      91.30      76.79      88.89      79.44      96.43      84.00      75.00      85.23      96.15      90.76      87.50      98.48      98.08\n",
      "      2       82.54     87.50     58.70     63.12     53.85     75.61     77.66     74.04     74.39     72.34      92.27      84.62      90.00      88.46      78.57      88.46      90.22      78.57      91.11      80.56      96.43      85.00      75.00      85.23      96.15      89.13      89.29      97.73      97.12\n",
      "      3       82.85     86.88     58.15     65.62     49.04     76.22     76.06     78.85     73.78     73.94      92.73      82.69      91.11      89.42      80.36      90.38      89.13      77.68      90.56      81.11      96.43      88.00      76.16      85.23      96.15      89.13      89.29      98.48      97.12\n",
      "      4       82.19     86.25     58.70     65.00     48.08     75.00     75.53     73.08     72.56     69.68      94.55      82.69      88.33      89.42      78.57      91.35      88.04      76.79      91.67      79.44      96.43      87.00      76.16      85.23      96.15      89.67      89.29      98.48      98.08\n",
      "      5       82.71     86.88     59.78     66.25     51.92     72.56     76.06     75.96     73.17     73.40      93.18      83.65      89.44      90.38      76.79      91.35      88.04      78.57      91.11      80.56      96.43      88.00      77.91      84.09      96.15      89.13      88.69      98.48      98.08\n",
      "      6       82.76     87.50     61.41     65.00     50.96     75.61     77.13     75.96     73.17     71.81      93.64      83.65      90.00      88.46      78.57      89.42      90.22      79.46      91.11      78.89      96.43      84.00      79.65      82.95      97.12      89.67      89.88      98.48      97.12\n",
      "      7       82.63     87.50     59.24     65.00     48.08     73.17     78.72     76.92     74.39     69.15      95.00      86.54      90.00      88.46      78.57      90.38      88.04      78.57      91.67      79.44      98.21      84.00      76.16      85.23      96.15      88.59      89.88      98.48      98.08\n",
      "      8       82.41     87.50     57.07     65.62     47.12     75.61     74.47     75.96     76.22     69.15      93.64      84.62      90.56      90.38      75.00      89.42      90.22      80.36      89.44      80.00      96.43      86.00      77.91      84.09      96.15      89.67      89.29      98.48      97.12\n",
      "      9       82.62     88.12     58.15     66.25     49.04     73.78     76.06     73.08     73.78     69.68      94.09      84.62      91.11      88.46      75.00      90.38      91.30      79.46      90.56      80.00      98.21      88.00      78.49      84.09      96.15      89.67      89.29      98.48      98.08\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0029 0.0178 0.0905]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8262 \n",
      "Accuracies (test): [0.8812, 0.5815, 0.6625, 0.4904, 0.7378, 0.7606, 0.7308, 0.7378, 0.6968, 0.9409, 0.8462, 0.9111, 0.8846, 0.75, 0.9038, 0.913, 0.7946, 0.9056, 0.8, 0.9821, 0.88, 0.7849, 0.8409, 0.9615, 0.8967, 0.8929, 0.9848, 0.9808]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 29\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       82.43     85.00     58.15     66.88     51.92     75.00     76.60     75.96     72.56     68.09      92.27      80.77      90.00      89.42      78.57      90.38      86.96      76.79      90.56      81.67      96.43      83.00      74.42      84.09      97.12      90.76      87.50      98.48      97.12      93.94\n",
      "      1       82.81     86.25     57.61     66.88     50.00     75.00     75.00     75.00     73.78     69.68      92.27      82.69      91.11      87.50      80.36      88.46      89.13      78.57      91.11      81.67      96.43      85.00      77.33      82.95      96.15      89.67      87.50      98.48      98.08      97.73\n",
      "      2       82.73     86.88     59.78     66.88     47.12     72.56     77.66     75.00     74.39     69.15      93.64      82.69      91.11      90.38      78.57      90.38      88.04      79.46      89.44      81.11      96.43      86.00      75.00      81.82      96.15      90.76      86.90      98.48      98.08      95.45\n",
      "      3       82.73     87.50     57.07     65.62     50.96     76.22     75.00     75.96     71.95     69.68      91.82      83.65      91.11      89.42      78.57      88.46      86.96      79.46      90.56      81.67      94.64      86.00      75.00      84.09      96.15      90.76      88.10      98.48      98.08      96.21\n",
      "      4       83.15     88.12     57.61     68.75     50.00     75.61     77.13     74.04     72.56     71.81      94.09      83.65      89.44      88.46      78.57      91.35      90.22      77.68      91.67      82.22      94.64      85.00      76.74      84.09      97.12      89.67      87.50      98.48      98.08      96.97\n",
      "      5       82.81     85.62     58.70     67.50     50.00     75.00     77.13     75.00     73.17     68.62      93.18      81.73      91.11      88.46      75.00      91.35      88.04      80.36      91.67      80.56      94.64      86.00      76.16      84.09      97.12      89.67      88.10      98.48      98.08      96.97\n",
      "      6       83.01     86.25     57.61     68.75     46.15     75.00     77.66     75.96     71.34     69.68      91.82      84.62      90.00      90.38      78.57      91.35      88.04      79.46      92.22      82.22      96.43      86.00      75.58      82.95      96.15      90.76      88.69      98.48      98.08      96.97\n",
      "      7       82.62     86.25     55.43     69.38     50.00     70.73     78.19     76.92     71.34     69.15      92.27      81.73      91.11      90.38      78.57      91.35      85.87      77.68      90.56      80.00      96.43      87.00      75.00      85.23      97.12      89.67      87.50      98.48      98.08      94.70\n",
      "      8       82.69     86.88     57.61     67.50     50.00     72.56     76.06     75.00     75.61     69.68      92.27      83.65      90.00      89.42      78.57      89.42      88.04      75.89      90.56      80.56      96.43      86.00      75.00      82.95      97.12      88.59      89.88      98.48      98.08      96.21\n",
      "      9       82.85     86.88     57.61     68.12     50.96     71.34     77.13     74.04     73.78     70.21      92.27      81.73      89.44      90.38      76.79      90.38      88.04      75.89      90.56      82.22      96.43      87.00      77.33      84.09      97.12      89.67      90.48      98.48      98.08      96.21\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0013 0.0089 0.0543]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8285 \n",
      "Accuracies (test): [0.8688, 0.5761, 0.6812, 0.5096, 0.7134, 0.7713, 0.7404, 0.7378, 0.7021, 0.9227, 0.8173, 0.8944, 0.9038, 0.7679, 0.9038, 0.8804, 0.7589, 0.9056, 0.8222, 0.9643, 0.87, 0.7733, 0.8409, 0.9712, 0.8967, 0.9048, 0.9848, 0.9808, 0.9621]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 30\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.21     86.25     57.07     66.88     51.92     68.29     72.87     74.04     73.78     73.40      93.64      84.62      88.89      88.46      76.79      93.27      89.13      76.79      89.44      80.00      96.43      89.00      80.81      82.95      98.08      88.59      88.10      97.73      97.12      96.21      95.83\n",
      "      1       83.57     86.88     57.07     68.75     51.92     75.00     75.53     73.08     75.00     73.94      93.18      85.58      90.56      88.46      76.79      90.38      86.96      78.57      88.89      79.44      96.43      87.00      76.74      84.09      96.15      90.76      89.88      98.48      98.08      97.73      95.83\n",
      "      2       82.77     88.12     57.61     69.38     49.04     70.73     75.00     73.08     71.34     70.21      93.18      83.65      90.00      88.46      78.57      91.35      85.87      76.79      86.11      80.00      96.43      83.00      76.74      82.95      97.12      89.13      88.69      98.48      98.08      96.97      96.88\n",
      "      3       83.41     86.88     57.61     66.88     51.92     77.44     76.06     74.04     73.17     70.74      92.73      84.62      91.67      89.42      76.79      93.27      88.04      78.57      87.78      80.56      92.86      86.00      75.58      82.95      97.12      91.30      89.29      98.48      98.08      95.45      96.88\n",
      "      4       83.30     88.75     55.43     68.75     49.04     74.39     75.53     75.00     72.56     69.68      92.27      83.65      90.56      89.42      78.57      91.35      85.87      78.57      88.89      80.00      96.43      89.00      76.74      82.95      97.12      88.04      89.29      98.48      98.08      97.73      96.88\n",
      "      5       83.35     88.12     58.70     66.88     49.04     70.73     74.47     75.00     73.78     70.74      91.82      87.50      90.00      89.42      82.14      90.38      86.96      76.79      91.11      79.44      96.43      86.00      76.16      84.09      96.15      89.67      89.29      98.48      98.08      96.21      96.88\n",
      "      6       83.32     87.50     55.98     69.38     49.04     72.56     78.19     74.04     74.39     70.74      94.09      84.62      89.44      89.42      80.36      89.42      90.22      77.68      88.89      80.56      94.64      86.00      76.16      82.95      97.12      89.13      87.50      98.48      98.08      96.21      96.88\n",
      "      7       83.41     86.88     56.52     68.75     49.04     73.78     75.53     73.08     75.61     69.68      91.82      84.62      91.11      89.42      80.36      92.31      89.13      78.57      88.33      78.89      94.64      88.00      76.74      85.23      97.12      89.67      88.69      98.48      98.08      95.45      96.88\n",
      "      8       83.16     86.88     57.61     68.75     50.96     73.78     73.94     74.04     75.00     69.15      92.73      84.62      90.56      88.46      78.57      92.31      90.22      75.89      88.89      80.56      94.64      86.00      72.09      82.95      98.08      90.22      88.10      98.48      98.08      96.21      96.88\n",
      "      9       83.31     86.88     57.07     68.75     51.92     73.78     75.00     74.04     71.95     72.34      93.18      83.65      88.33      89.42      80.36      92.31      88.04      75.00      87.78      82.78      92.86      88.00      77.91      84.09      97.12      88.59      86.90      98.48      98.08      97.73      96.88\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0021 0.013  0.0671]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8331 \n",
      "Accuracies (test): [0.8688, 0.5707, 0.6875, 0.5192, 0.7378, 0.75, 0.7404, 0.7195, 0.7234, 0.9318, 0.8365, 0.8833, 0.8942, 0.8036, 0.9231, 0.8804, 0.75, 0.8778, 0.8278, 0.9286, 0.88, 0.7791, 0.8409, 0.9712, 0.8859, 0.869, 0.9848, 0.9808, 0.9773, 0.9688]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 31\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       82.33     83.75     54.89     66.25     50.96     68.90     75.53     72.12     71.95     71.28      93.18      80.77      87.22      88.46      82.14      89.42      86.96      75.89      87.22      78.33      94.64      85.00      77.91      85.23      95.19      89.13      85.12      98.48      96.15      96.97      94.79      88.46\n",
      "      1       82.20     83.75     58.70     65.00     54.81     71.95     72.34     74.04     70.12     69.68      94.09      81.73      83.89      85.58      78.57      89.42      85.87      75.89      85.56      78.89      94.64      85.00      74.42      82.95      95.19      90.76      87.50      98.48      96.15      96.97      95.83      90.38\n",
      "      2       83.12     85.62     58.70     65.62     47.12     71.34     77.13     74.04     71.95     69.15      94.55      85.58      87.78      88.46      78.57      90.38      84.78      80.36      85.56      80.00      94.64      84.00      79.07      84.09      96.15      90.76      88.69      98.48      97.12      97.73      96.88      92.31\n",
      "      3       82.67     86.88     59.78     67.50     46.15     71.95     73.40     73.08     71.95     71.28      92.73      81.73      86.67      88.46      80.36      89.42      84.78      77.68      86.67      79.44      94.64      85.00      76.74      84.09      95.19      89.67      88.69      98.48      96.15      96.97      96.88      90.38\n",
      "      4       82.86     86.25     57.07     69.38     50.96     71.34     75.00     74.04     72.56     70.74      93.64      84.62      87.78      89.42      78.57      89.42      83.70      77.68      86.67      81.11      94.64      83.00      79.07      82.95      95.19      89.13      86.31      98.48      96.15      95.45      96.88      91.35\n",
      "      5       83.82     87.50     58.70     70.62     52.88     73.78     75.00     74.04     73.78     71.81      95.45      84.62      88.33      89.42      78.57      90.38      85.87      79.46      86.67      80.00      96.43      85.00      77.33      84.09      95.19      90.76      89.88      98.48      97.12      96.21      96.88      94.23\n",
      "      6       83.17     87.50     53.26     68.75     50.96     74.39     73.94     73.08     73.78     67.55      93.64      84.62      87.78      88.46      80.36      89.42      84.78      75.00      86.67      82.78      94.64      87.00      77.91      84.09      96.15      88.59      88.69      98.48      96.15      97.73      96.88      95.19\n",
      "      7       82.99     87.50     57.07     67.50     51.92     70.73     75.00     73.08     71.95     69.68      92.73      82.69      88.33      88.46      76.79      88.46      85.87      80.36      85.00      81.11      96.43      85.00      77.33      82.95      96.15      90.22      88.69      98.48      98.08      96.97      95.83      92.31\n",
      "      8       83.19     86.88     57.07     68.75     52.88     73.17     75.00     71.15     73.17     70.21      93.64      81.73      86.11      88.46      76.79      92.31      85.87      78.57      86.67      81.11      94.64      87.00      78.49      84.09      95.19      91.30      88.10      98.48      96.15      97.73      95.83      92.31\n",
      "      9       83.51     87.50     57.61     68.12     52.88     68.90     73.40     73.08     73.78     69.68      92.73      87.50      90.00      90.38      78.57      91.35      84.78      78.57      88.89      81.67      94.64      86.00      79.07      82.95      96.15      90.22      88.69      98.48      97.12      97.73      96.88      91.35\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0009 0.0096 0.0672]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8351 \n",
      "Accuracies (test): [0.875, 0.5761, 0.6812, 0.5288, 0.689, 0.734, 0.7308, 0.7378, 0.6968, 0.9273, 0.875, 0.9, 0.9038, 0.7857, 0.9135, 0.8478, 0.7857, 0.8889, 0.8167, 0.9464, 0.86, 0.7907, 0.8295, 0.9615, 0.9022, 0.8869, 0.9848, 0.9712, 0.9773, 0.9688, 0.9135]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 32\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.32     86.88     57.07     70.62     50.96     70.73     74.47     75.96     75.61     69.68      93.18      85.58      86.67      89.42      78.57      90.38      84.78      77.68      85.56      80.00      96.43      83.00      77.91      84.09      96.15      91.85      88.69      97.73      96.15      94.70      96.88      92.31      86.54\n",
      "      1       83.22     86.88     54.35     68.12     48.08     71.95     76.60     71.15     75.00     69.68      92.73      85.58      88.33      88.46      78.57      91.35      86.96      75.89      87.78      84.44      94.64      88.00      77.91      84.09      97.12      90.22      88.10      97.73      97.12      96.21      96.88      90.38      82.69\n",
      "      2       83.03     86.88     56.52     63.75     52.88     68.29     76.06     73.08     72.56     70.21      93.18      84.62      88.33      88.46      80.36      89.42      86.96      74.11      86.67      81.11      94.64      86.00      79.07      84.09      96.15      89.13      88.69      98.48      97.12      95.45      96.88      93.27      84.62\n",
      "      3       82.58     86.25     54.35     66.88     48.08     71.34     74.47     70.19     74.39     70.21      93.64      84.62      88.33      88.46      78.57      90.38      88.04      73.21      86.11      80.56      92.86      86.00      77.33      82.95      96.15      89.13      89.29      97.73      96.15      96.97      95.83      90.38      83.65\n",
      "      4       82.54     86.25     57.61     67.50     44.23     67.07     75.00     72.12     72.56     69.68      92.73      81.73      87.78      88.46      76.79      88.46      88.04      76.79      86.11      81.67      94.64      86.00      77.91      85.23      96.15      89.67      88.69      98.48      95.19      96.97      96.88      93.27      81.73\n",
      "      5       82.73     87.50     59.78     66.25     48.08     69.51     72.34     74.04     73.17     71.81      93.18      81.73      88.33      89.42      76.79      89.42      85.87      73.21      86.11      82.22      94.64      85.00      76.74      84.09      96.15      89.13      89.29      97.73      97.12      96.97      96.88      91.35      83.65\n",
      "      6       82.99     86.25     58.15     68.75     50.00     73.78     74.47     74.04     73.78     70.21      94.09      81.73      88.89      88.46      76.79      90.38      89.13      75.00      85.56      80.56      94.64      83.00      77.91      80.68      96.15      89.67      89.29      98.48      97.12      96.97      95.83      92.31      83.65\n",
      "      7       83.07     86.25     54.35     68.12     51.92     71.95     75.00     75.00     72.56     70.21      93.64      82.69      89.44      89.42      76.79      90.38      86.96      76.79      86.67      83.89      94.64      83.00      77.91      82.95      97.12      88.59      87.50      98.48      98.08      96.97      96.88      91.35      82.69\n",
      "      8       82.85     86.25     56.52     69.38     50.00     70.12     70.74     72.12     70.73     69.68      94.09      81.73      88.33      88.46      80.36      89.42      88.04      74.11      86.67      82.22      94.64      83.00      79.65      84.09      97.12      88.59      89.88      98.48      97.12      96.97      95.83      91.35      85.58\n",
      "      9       83.11     86.88     54.89     68.75     50.96     69.51     74.47     71.15     73.78     68.62      94.55      80.77      88.33      89.42      78.57      90.38      88.04      75.89      88.33      81.11      94.64      87.00      76.74      82.95      97.12      89.67      88.69      98.48      98.08      96.97      96.88      92.31      85.58\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.004  0.0418 0.2263]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8311 \n",
      "Accuracies (test): [0.8688, 0.5489, 0.6875, 0.5096, 0.6951, 0.7447, 0.7115, 0.7378, 0.6862, 0.9455, 0.8077, 0.8833, 0.8942, 0.7857, 0.9038, 0.8804, 0.7589, 0.8833, 0.8111, 0.9464, 0.87, 0.7674, 0.8295, 0.9712, 0.8967, 0.8869, 0.9848, 0.9808, 0.9697, 0.9688, 0.9231, 0.8558]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 33\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       82.55     89.38     54.89     66.88     46.15     71.34     75.00     70.19     70.12     71.28      92.27      80.77      85.56      89.42      78.57      90.38      85.87      81.25      86.67      80.56      94.64      85.00      75.58      85.23      97.12      91.85      87.50      98.48      95.19      96.97      96.88      89.42      84.62      79.27\n",
      "      1       82.63     88.75     57.61     70.62     49.04     72.56     75.00     69.23     71.34     75.00      94.55      79.81      86.67      89.42      75.00      90.38      86.96      74.11      85.56      82.22      96.43      83.00      75.00      85.23      96.15      90.22      86.90      98.48      95.19      96.97      94.79      89.42      84.62      80.49\n",
      "      2       82.31     88.12     54.35     66.25     49.04     69.51     77.13     70.19     70.12     70.21      93.18      80.77      87.22      88.46      75.00      89.42      88.04      75.89      86.11      78.89      96.43      84.00      75.00      85.23      97.12      90.76      86.90      97.73      96.15      96.97      95.83      90.38      83.65      82.32\n",
      "      3       82.59     90.00     56.52     69.38     47.12     68.90     75.53     70.19     70.12     67.02      93.64      81.73      87.22      87.50      75.00      87.50      86.96      76.79      86.67      82.22      96.43      87.00      76.74      84.09      96.15      90.76      87.50      98.48      96.15      96.97      96.88      91.35      82.69      84.15\n",
      "      4       82.58     88.12     58.15     67.50     50.00     69.51     74.47     72.12     69.51     70.21      93.18      79.81      87.22      90.38      75.00      90.38      86.96      74.11      85.56      81.11      94.64      86.00      77.91      84.09      97.12      91.30      88.10      98.48      95.19      94.70      94.79      90.38      83.65      85.37\n",
      "      5       83.28     89.38     53.80     67.50     48.08     69.51     75.00     75.00     71.34     71.28      94.09      85.58      90.00      89.42      80.36      88.46      88.04      76.79      84.44      80.00      94.64      85.00      77.91      86.36      95.19      93.48      88.10      98.48      96.15      96.97      95.83      90.38      85.58      85.98\n",
      "      6       83.22     89.38     55.43     68.12     52.88     67.68     75.53     73.08     70.12     72.34      92.27      83.65      89.44      89.42      76.79      88.46      88.04      76.79      84.44      82.78      94.64      85.00      77.33      86.36      96.15      92.39      88.10      98.48      96.15      96.97      94.79      93.27      84.62      85.37\n",
      "      7       83.62     89.38     56.52     70.00     52.88     71.95     75.00     72.12     71.95     70.74      92.73      82.69      87.78      89.42      82.14      90.38      86.96      76.79      85.56      80.56      96.43      85.00      79.65      85.23      95.19      91.30      89.88      98.48      96.15      96.97      95.83      92.31      83.65      87.80\n",
      "      8       82.70     86.88     54.35     67.50     46.15     71.34     73.94     70.19     70.73     71.81      93.64      80.77      86.11      87.50      80.36      88.46      88.04      78.57      82.22      80.56      96.43      85.00      76.16      85.23      95.19      91.85      87.50      98.48      97.12      97.73      95.83      90.38      84.62      88.41\n",
      "      9       82.57     87.50     53.26     68.12     47.12     68.90     71.81     70.19     73.17     70.21      92.27      82.69      87.78      90.38      78.57      87.50      89.13      75.00      86.67      79.44      92.86      82.00      77.91      84.09      96.15      91.85      86.31      98.48      96.15      96.97      95.83      92.31      86.54      87.80\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0119 0.0795 0.3126]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8257 \n",
      "Accuracies (test): [0.875, 0.5326, 0.6812, 0.4712, 0.689, 0.7181, 0.7019, 0.7317, 0.7021, 0.9227, 0.8269, 0.8778, 0.9038, 0.7857, 0.875, 0.8913, 0.75, 0.8667, 0.7944, 0.9286, 0.82, 0.7791, 0.8409, 0.9615, 0.9185, 0.8631, 0.9848, 0.9615, 0.9697, 0.9583, 0.9231, 0.8654, 0.878]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 34\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       82.60     87.50     57.61     68.12     50.96     68.90     72.87     67.31     72.56     70.21      92.73      82.69      87.78      89.42      78.57      89.42      89.13      77.68      83.89      78.33      89.29      83.00      73.84      84.09      95.19      92.39      85.71      97.73      96.15      97.73      95.83      94.23      82.69      86.59      88.33\n",
      "      1       82.61     89.38     55.98     66.25     50.00     71.34     71.28     71.15     70.73     72.87      92.27      81.73      87.22      89.42      80.36      88.46      86.96      75.89      86.11      77.78      91.07      81.00      74.42      84.09      95.19      91.85      87.50      98.48      96.15      97.73      93.75      91.35      84.62      84.76      91.67\n",
      "      2       82.75     90.00     55.43     66.25     50.96     70.73     72.87     70.19     70.73     71.81      93.64      83.65      86.11      88.46      76.79      90.38      88.04      75.00      85.00      80.00      89.29      83.00      75.00      84.09      96.15      90.76      85.12      98.48      96.15      96.97      96.88      91.35      85.58      85.37      93.33\n",
      "      3       82.35     89.38     53.26     65.00     47.12     70.12     72.87     70.19     69.51     70.74      92.73      81.73      88.33      90.38      75.00      88.46      86.96      75.00      85.56      77.78      91.07      81.00      77.33      85.23      96.15      90.22      86.90      97.73      96.15      96.21      94.79      91.35      84.62      85.98      95.00\n",
      "      4       82.66     90.00     51.09     69.38     52.88     68.29     71.81     74.04     70.73     68.62      92.73      81.73      88.89      86.54      78.57      89.42      88.04      74.11      85.00      76.67      92.86      85.00      75.00      84.09      95.19      91.30      86.90      98.48      95.19      98.48      93.75      90.38      83.65      86.59      95.00\n",
      "      5       82.65     88.75     53.80     68.12     48.08     69.51     71.81     69.23     70.73     70.21      92.27      80.77      88.89      89.42      78.57      89.42      89.13      75.00      85.56      77.78      91.07      83.00      73.84      86.36      97.12      90.76      86.90      98.48      96.15      98.48      93.75      93.27      83.65      85.37      95.00\n",
      "      6       83.03     88.12     53.26     66.88     47.12     70.12     72.34     73.08     71.34     65.96      94.09      82.69      87.78      89.42      80.36      89.42      89.13      75.89      86.67      80.00      92.86      86.00      75.00      86.36      96.15      91.30      86.31      97.73      95.19      97.73      95.83      92.31      84.62      85.98      95.83\n",
      "      7       82.67     90.62     52.17     65.00     47.12     68.29     70.74     70.19     71.34     70.74      92.73      81.73      87.78      88.46      80.36      90.38      88.04      75.00      87.78      75.56      92.86      81.00      76.74      85.23      96.15      91.85      86.31      97.73      95.19      98.48      94.79      93.27      85.58      86.59      95.00\n",
      "      8       82.70     86.88     52.17     65.00     49.04     69.51     71.81     71.15     73.78     69.68      92.73      80.77      87.78      89.42      78.57      92.31      88.04      75.89      86.67      79.44      92.86      84.00      75.00      82.95      97.12      90.22      86.31      97.73      96.15      96.97      95.83      91.35      84.62      85.98      94.17\n",
      "      9       82.86     88.12     55.98     63.75     45.19     68.90     72.34     73.08     71.95     71.81      92.73      83.65      88.89      89.42      78.57      89.42      89.13      75.00      85.56      77.78      94.64      83.00      73.84      86.36      96.15      89.67      87.50      97.73      95.19      97.73      95.83      91.35      86.54      85.37      95.00\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0029 0.0241 0.1346]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8286 \n",
      "Accuracies (test): [0.8812, 0.5598, 0.6375, 0.4519, 0.689, 0.7234, 0.7308, 0.7195, 0.7181, 0.9273, 0.8365, 0.8889, 0.8942, 0.7857, 0.8942, 0.8913, 0.75, 0.8556, 0.7778, 0.9464, 0.83, 0.7384, 0.8636, 0.9615, 0.8967, 0.875, 0.9773, 0.9519, 0.9773, 0.9583, 0.9135, 0.8654, 0.8537, 0.95]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 35\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.21     86.25     52.72     66.25     47.12     70.73     71.81     76.92     71.95     72.87      92.27      82.69      85.56      90.38      76.79      87.50      88.04      75.89      83.33      77.78      92.86      87.00      77.33      85.23      97.12      89.67      88.69      98.48      96.15      97.73      96.88      92.31      84.62      84.15      95.00      92.31\n",
      "      1       83.60     88.12     54.89     64.38     52.88     71.95     74.47     72.12     72.56     75.00      94.09      81.73      87.22      91.35      78.57      91.35      86.96      76.79      85.56      76.67      92.86      83.00      77.33      85.23      95.19      91.30      86.90      97.73      95.19      97.73      94.79      93.27      84.62      84.15      95.83      94.23\n",
      "      2       83.45     89.38     55.98     65.62     50.00     68.90     73.40     75.00     70.73     73.40      93.18      79.81      90.00      89.42      78.57      92.31      85.87      72.32      84.44      75.56      96.43      84.00      77.91      84.09      97.12      91.85      86.31      97.73      94.23      97.73      94.79      93.27      84.62      86.59      95.83      94.23\n",
      "      3       83.76     91.25     54.89     68.12     45.19     71.34     73.94     74.04     71.34     69.68      93.18      83.65      89.44      90.38      80.36      91.35      89.13      74.11      87.78      77.78      96.43      84.00      79.07      85.23      97.12      91.85      87.50      97.73      95.19      97.73      93.75      93.27      82.69      84.76      94.17      94.23\n",
      "      4       83.39     89.38     56.52     66.25     49.04     70.73     73.94     73.08     70.73     74.47      93.64      84.62      87.78      91.35      78.57      89.42      88.04      75.00      86.11      76.11      89.29      83.00      77.91      85.23      96.15      90.22      87.50      97.73      93.27      97.73      93.75      91.35      85.58      85.98      95.00      94.23\n",
      "      5       83.77     90.00     55.98     63.75     47.12     70.12     74.47     74.04     72.56     72.87      93.18      83.65      86.67      90.38      76.79      91.35      90.22      75.89      86.11      78.89      92.86      83.00      76.16      86.36      98.08      91.30      88.10      98.48      96.15      98.48      93.75      93.27      86.54      85.98      94.17      95.19\n",
      "      6       83.38     86.88     55.98     68.12     45.19     73.17     73.94     74.04     70.12     71.28      93.64      82.69      87.78      90.38      80.36      89.42      88.04      75.00      84.44      76.67      94.64      83.00      75.58      87.50      96.15      90.76      88.10      97.73      95.19      97.73      93.75      91.35      84.62      84.76      95.00      95.19\n",
      "      7       83.16     89.38     53.80     65.00     48.08     71.34     70.74     73.08     71.95     70.21      93.18      82.69      86.67      90.38      76.79      90.38      88.04      74.11      86.67      76.67      91.07      82.00      76.74      86.36      95.19      89.67      88.10      98.48      95.19      97.73      93.75      95.19      85.58      85.37      95.00      96.15\n",
      "      8       82.83     87.50     58.15     61.88     40.38     70.73     73.94     75.00     68.90     70.74      91.82      81.73      87.78      91.35      76.79      88.46      88.04      78.57      84.44      78.89      92.86      83.00      75.00      85.23      96.15      91.30      87.50      97.73      93.27      96.97      92.71      93.27      85.58      84.15      94.17      95.19\n",
      "      9       83.28     88.75     57.07     67.50     48.08     72.56     70.21     71.15     68.90     70.74      95.00      83.65      86.67      92.31      76.79      88.46      86.96      73.21      85.00      78.89      94.64      84.00      75.58      86.36      96.15      90.76      88.10      98.48      95.19      97.73      93.75      93.27      85.58      84.76      93.33      95.19\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0014 0.0126 0.0797]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8328 \n",
      "Accuracies (test): [0.8875, 0.5707, 0.675, 0.4808, 0.7256, 0.7021, 0.7115, 0.689, 0.7074, 0.95, 0.8365, 0.8667, 0.9231, 0.7679, 0.8846, 0.8696, 0.7321, 0.85, 0.7889, 0.9464, 0.84, 0.7558, 0.8636, 0.9615, 0.9076, 0.881, 0.9848, 0.9519, 0.9773, 0.9375, 0.9327, 0.8558, 0.8476, 0.9333, 0.9519]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 36\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       82.71     88.75     53.26     63.75     48.08     70.12     73.40     69.23     71.34     69.15      93.18      80.77      85.56      90.38      80.36      88.46      86.96      74.11      86.11      78.89      92.86      80.00      75.58      87.50      96.15      91.30      87.50      97.73      97.12      97.73      92.71      92.31      79.81      84.15      94.17      93.27      85.71\n",
      "      1       83.36     90.00     53.80     65.00     45.19     72.56     71.28     74.04     73.17     70.74      93.18      80.77      86.67      90.38      83.93      88.46      86.96      71.43      86.11      80.56      92.86      83.00      77.33      87.50      95.19      91.30      88.10      96.97      97.12      96.97      92.71      91.35      83.65      86.59      93.33      94.23      88.69\n",
      "      2       82.73     88.75     54.89     63.75     45.19     71.95     71.81     70.19     71.34     71.28      93.18      81.73      86.11      90.38      76.79      89.42      88.04      73.21      84.44      78.33      94.64      84.00      70.93      87.50      96.15      91.30      88.69      96.97      96.15      97.73      90.62      88.46      81.73      85.37      93.33      95.19      88.69\n",
      "      3       82.98     90.00     54.89     66.25     46.15     71.34     69.15     73.08     71.95     71.28      93.64      77.88      86.11      92.31      80.36      89.42      85.87      72.32      86.11      78.33      89.29      84.00      73.84      88.64      96.15      91.85      86.90      96.97      98.08      96.97      91.67      89.42      81.73      84.76      94.17      95.19      91.07\n",
      "      4       83.16     87.50     54.35     65.00     49.04     72.56     71.81     70.19     68.90     71.81      92.73      78.85      87.22      91.35      82.14      88.46      86.96      73.21      86.11      78.33      92.86      84.00      73.84      85.23      98.08      92.39      88.10      97.73      96.15      96.21      91.67      91.35      85.58      85.37      91.67      95.19      91.67\n",
      "      5       82.84     87.50     52.17     64.38     48.08     71.34     68.62     72.12     71.34     71.28      93.64      81.73      85.56      90.38      78.57      87.50      86.96      69.64      82.78      80.00      92.86      86.00      76.74      87.50      97.12      91.85      86.90      97.73      95.19      97.73      91.67      91.35      84.62      82.93      94.17      94.23      89.88\n",
      "      6       83.07     88.75     53.80     65.00     45.19     69.51     70.21     74.04     69.51     70.21      94.55      80.77      86.11      90.38      82.14      87.50      89.13      75.00      86.11      79.44      92.86      84.00      74.42      86.36      96.15      90.76      85.71      97.73      99.04      97.73      90.62      90.38      83.65      84.76      94.17      94.23      90.48\n",
      "      7       82.91     88.75     51.63     66.25     45.19     71.95     73.94     72.12     69.51     69.15      93.18      83.65      85.56      89.42      76.79      90.38      86.96      72.32      84.44      75.56      89.29      84.00      76.74      88.64      97.12      91.85      87.50      97.73      97.12      96.97      93.75      90.38      83.65      82.93      93.33      95.19      91.67\n",
      "      8       83.44     89.38     55.43     65.00     43.27     73.78     73.94     72.12     70.12     73.40      93.64      82.69      85.56      91.35      75.00      92.31      89.13      72.32      85.56      78.33      91.07      85.00      75.00      89.77      96.15      92.39      85.71      98.48      98.08      96.97      90.62      92.31      84.62      85.98      94.17      94.23      91.07\n",
      "      9       83.32     88.12     54.35     68.75     47.12     71.95     71.81     74.04     70.12     71.81      94.55      80.77      85.56      92.31      76.79      89.42      88.04      72.32      84.44      78.89      91.07      85.00      75.00      87.50      95.19      91.85      85.12      97.73      98.08      96.21      91.67      92.31      84.62      85.98      94.17      95.19      91.67\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0046 0.0373 0.1739]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8332 \n",
      "Accuracies (test): [0.8812, 0.5435, 0.6875, 0.4712, 0.7195, 0.7181, 0.7404, 0.7012, 0.7181, 0.9455, 0.8077, 0.8556, 0.9231, 0.7679, 0.8942, 0.8804, 0.7232, 0.8444, 0.7889, 0.9107, 0.85, 0.75, 0.875, 0.9519, 0.9185, 0.8512, 0.9773, 0.9808, 0.9621, 0.9167, 0.9231, 0.8462, 0.8598, 0.9417, 0.9519, 0.9167]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 37\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.16     90.62     57.61     66.25     47.12     70.73     71.81     75.96     73.17     69.15      92.27      80.77      85.56      87.50      83.93      88.46      88.04      74.11      86.67      77.78      89.29      86.00      76.74      87.50      96.15      90.76      86.31      96.97      97.12      96.97      90.62      90.38      81.73      80.49      92.50      95.19      90.48      84.38\n",
      "      1       83.48     89.38     55.98     67.50     49.04     74.39     71.81     74.04     71.95     70.74      93.18      79.81      86.67      91.35      80.36      91.35      88.04      73.21      85.00      77.22      89.29      85.00      73.84      87.50      94.23      91.85      85.71      96.97      99.04      97.73      90.62      92.31      83.65      84.15      93.33      95.19      90.48      86.88\n",
      "      2       83.06     89.38     53.26     65.00     48.08     69.51     69.68     73.08     73.17     71.28      94.09      83.65      84.44      91.35      80.36      90.38      88.04      70.54      85.00      77.22      92.86      83.00      76.16      88.64      94.23      91.85      85.71      97.73      98.08      96.97      90.62      91.35      81.73      83.54      92.50      95.19      89.29      86.25\n",
      "      3       83.49     90.00     54.89     68.12     44.23     70.12     71.81     75.00     71.95     68.09      94.09      82.69      87.22      90.38      80.36      89.42      88.04      72.32      85.00      76.67      92.86      85.00      76.74      89.77      95.19      91.85      84.52      97.73      99.04      97.73      89.58      92.31      85.58      84.15      93.33      95.19      90.48      87.50\n",
      "      4       83.42     87.50     54.35     68.75     47.12     73.17     71.81     75.00     70.73     71.28      95.00      80.77      85.56      92.31      78.57      88.46      88.04      71.43      85.00      79.44      92.86      86.00      76.16      88.64      95.19      91.30      85.12      97.73      99.04      96.97      90.62      91.35      82.69      83.54      92.50      95.19      90.48      86.88\n",
      "      5       83.21     87.50     53.26     64.38     46.15     71.34     70.21     73.08     70.73     71.81      92.73      79.81      86.11      90.38      80.36      89.42      89.13      74.11      87.22      79.44      91.07      85.00      77.33      89.77      95.19      90.22      85.71      97.73      99.04      97.73      90.62      91.35      81.73      82.93      95.00      95.19      89.29      86.88\n",
      "      6       83.38     88.75     51.63     71.25     48.08     69.51     75.00     72.12     71.34     70.21      93.64      82.69      86.67      92.31      82.14      89.42      89.13      71.43      86.11      76.67      89.29      83.00      75.58      88.64      95.19      89.13      84.52      97.73      99.04      97.73      90.62      91.35      85.58      83.54      93.33      95.19      89.29      88.12\n",
      "      7       83.16     89.38     55.98     68.75     46.15     70.12     71.28     73.08     70.73     69.15      92.73      80.77      86.11      90.38      80.36      87.50      88.04      75.89      85.56      78.89      91.07      83.00      75.00      89.77      94.23      90.76      84.52      98.48      99.04      96.97      89.58      92.31      81.73      82.93      93.33      95.19      89.88      88.12\n",
      "      8       83.41     88.12     54.89     69.38     47.12     70.12     69.68     75.96     72.56     69.68      91.82      83.65      85.56      90.38      83.93      87.50      89.13      71.43      83.89      81.11      91.07      85.00      78.49      89.77      95.19      90.76      83.33      98.48      99.04      96.97      90.62      91.35      81.73      83.54      92.50      95.19      89.88      87.50\n",
      "      9       83.25     90.00     52.17     68.75     48.08     70.73     71.81     71.15     71.95     68.62      93.64      79.81      83.33      91.35      78.57      90.38      88.04      72.32      88.89      78.89      89.29      83.00      76.74      88.64      95.19      91.85      84.52      98.48      99.04      97.73      90.62      91.35      82.69      85.98      93.33      95.19      89.88      88.12\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.003  0.0278 0.1568]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8325 \n",
      "Accuracies (test): [0.9, 0.5217, 0.6875, 0.4808, 0.7073, 0.7181, 0.7115, 0.7195, 0.6862, 0.9364, 0.7981, 0.8333, 0.9135, 0.7857, 0.9038, 0.8804, 0.7232, 0.8889, 0.7889, 0.8929, 0.83, 0.7674, 0.8864, 0.9519, 0.9185, 0.8452, 0.9848, 0.9904, 0.9773, 0.9062, 0.9135, 0.8269, 0.8598, 0.9333, 0.9519, 0.8988, 0.8812]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 38\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       82.88     88.75     53.26     69.38     43.27     70.12     71.81     74.04     71.95     69.68      92.27      78.85      84.44      88.46      78.57      89.42      84.78      73.21      83.89      77.78      92.86      85.00      76.74      88.64      95.19      90.76      83.93      97.73      98.08      96.97      88.54      89.42      81.73      85.98      93.33      95.19      91.07      86.88      87.50\n",
      "      1       83.02     87.50     53.26     68.12     46.15     73.17     72.87     71.15     73.17     68.09      92.73      76.92      83.33      90.38      80.36      89.42      89.13      74.11      85.56      75.56      91.07      84.00      77.91      87.50      94.23      89.67      86.31      97.73      98.08      97.73      91.67      88.46      81.73      85.37      93.33      94.23      89.88      86.25      88.46\n",
      "      2       83.27     88.75     52.72     66.88     45.19     70.73     75.53     75.00     71.34     70.21      91.36      79.81      85.00      89.42      82.14      88.46      89.13      73.21      86.67      78.33      91.07      85.00      78.49      87.50      95.19      90.22      83.33      96.97      98.08      96.97      91.67      90.38      82.69      83.54      93.33      94.23      91.07      86.25      88.46\n",
      "      3       83.16     89.38     54.89     70.00     44.23     71.95     73.40     73.08     73.17     70.21      93.64      78.85      85.00      90.38      82.14      86.54      86.96      70.54      83.89      77.22      91.07      84.00      76.16      89.77      95.19      90.76      84.52      97.73      98.08      97.73      90.62      89.42      81.73      85.37      93.33      94.23      89.88      86.25      88.94\n",
      "      4       83.10     86.88     53.26     66.25     45.19     70.73     72.34     73.08     71.95     68.09      93.18      78.85      86.11      90.38      78.57      87.50      85.87      72.32      87.22      79.44      94.64      84.00      76.16      87.50      95.19      91.30      85.71      97.73      98.08      96.97      91.67      89.42      81.73      84.76      93.33      95.19      88.69      86.25      92.31\n",
      "      5       83.35     86.88     53.26     66.88     45.19     71.95     72.34     74.04     71.95     68.62      93.18      77.88      86.67      91.35      82.14      88.46      86.96      70.54      88.33      78.33      91.07      86.00      79.07      88.64      94.23      91.85      85.12      97.73      98.08      97.73      91.67      90.38      81.73      82.93      93.33      95.19      90.48      86.88      90.38\n",
      "      6       83.69     89.38     51.09     68.12     41.35     71.95     71.28     72.12     71.95     70.74      94.55      84.62      86.11      90.38      82.14      91.35      86.96      72.32      86.67      79.44      91.07      84.00      79.07      89.77      94.23      92.93      85.71      98.48      98.08      97.73      92.71      90.38      84.62      83.54      93.33      95.19      89.88      86.25      90.87\n",
      "      7       83.53     88.75     53.26     68.12     43.27     71.34     74.47     74.04     71.95     70.21      95.00      79.81      86.11      90.38      82.14      91.35      86.96      68.75      86.11      78.33      91.07      84.00      78.49      86.36      94.23      91.30      86.31      97.73      99.04      98.48      91.67      91.35      83.65      80.49      94.17      95.19      89.88      87.50      92.79\n",
      "      8       83.82     89.38     52.72     71.88     43.27     71.95     73.94     74.04     71.34     70.21      92.73      84.62      85.56      92.31      83.93      89.42      86.96      70.54      88.33      79.44      92.86      84.00      79.07      87.50      95.19      91.30      84.52      97.73      99.04      97.73      91.67      89.42      83.65      83.54      93.33      94.23      89.88      86.88      90.87\n",
      "      9       83.07     86.25     52.17     69.38     46.15     70.73     73.40     72.12     72.56     67.02      92.73      77.88      86.11      90.38      80.36      88.46      85.87      75.89      85.00      78.33      91.07      83.00      76.16      88.64      94.23      90.76      86.31      96.97      99.04      97.73      93.75      89.42      83.65      81.10      91.67      94.23      91.07      86.25      90.87\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0026 0.0225 0.136 ]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8307 \n",
      "Accuracies (test): [0.8625, 0.5217, 0.6938, 0.4615, 0.7073, 0.734, 0.7212, 0.7256, 0.6702, 0.9273, 0.7788, 0.8611, 0.9038, 0.8036, 0.8846, 0.8587, 0.7589, 0.85, 0.7833, 0.9107, 0.83, 0.7616, 0.8864, 0.9423, 0.9076, 0.8631, 0.9697, 0.9904, 0.9773, 0.9375, 0.8942, 0.8365, 0.811, 0.9167, 0.9423, 0.9107, 0.8625, 0.9087]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 39\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.22     86.88     50.00     66.25     49.04     67.07     71.28     75.00     74.39     68.62      93.18      75.96      86.67      89.42      80.36      89.42      88.04      75.89      85.00      78.33      92.86      82.00      78.49      84.09      92.31      92.39      83.93      97.73      96.15      97.73      91.67      89.42      81.73      82.32      92.50      93.27      91.07      86.88      91.83      96.55\n",
      "      1       83.43     87.50     51.63     68.75     45.19     71.95     71.28     73.08     73.17     69.68      92.27      75.00      87.22      90.38      82.14      88.46      85.87      72.32      85.56      80.00      91.07      84.00      77.33      88.64      93.27      89.67      85.12      97.73      97.12      98.48      92.71      90.38      79.81      82.32      92.50      93.27      91.07      88.12      92.31      97.41\n",
      "      2       83.24     88.12     52.17     68.12     45.19     71.34     69.15     73.08     71.34     70.21      93.64      75.96      86.11      91.35      82.14      88.46      88.04      71.43      84.44      76.11      91.07      86.00      79.65      88.64      93.27      90.22      83.93      97.73      96.15      97.73      91.67      90.38      78.85      82.93      92.50      94.23      90.48      86.25      90.87      97.41\n",
      "      3       83.68     89.38     51.63     68.75     48.08     68.90     70.21     74.04     72.56     69.15      93.64      78.85      87.22      89.42      83.93      88.46      88.04      73.21      86.11      79.44      92.86      85.00      79.65      87.50      94.23      89.67      83.93      97.73      97.12      98.48      91.67      90.38      80.77      82.32      92.50      94.23      88.69      86.88      92.31      96.55\n",
      "      4       83.46     86.88     52.72     69.38     47.12     68.29     72.34     73.08     73.17     68.62      92.73      75.96      85.56      90.38      82.14      90.38      88.04      75.00      87.78      75.56      92.86      84.00      77.33      89.77      94.23      90.22      83.93      97.73      96.15      97.73      91.67      90.38      80.77      81.10      94.17      94.23      89.88      85.62      91.35      96.55\n",
      "      5       83.79     88.75     51.63     70.62     45.19     70.73     71.81     74.04     73.78     68.62      92.73      80.77      87.22      90.38      82.14      89.42      86.96      72.32      87.22      78.33      91.07      84.00      76.16      89.77      95.19      90.22      84.52      97.73      97.12      98.48      94.79      89.42      79.81      83.54      93.33      94.23      91.07      86.88      91.35      96.55\n",
      "      6       83.59     87.50     51.63     71.25     46.15     70.12     71.81     73.08     71.34     69.68      93.18      80.77      86.11      89.42      83.93      85.58      86.96      72.32      88.33      79.44      94.64      85.00      75.58      88.64      93.27      90.22      84.52      97.73      98.08      97.73      92.71      89.42      79.81      81.71      92.50      94.23      90.48      86.88      91.83      96.55\n",
      "      7       83.79     89.38     53.26     67.50     46.15     70.12     71.81     74.04     71.95     70.21      94.09      79.81      88.33      90.38      80.36      88.46      88.04      71.43      88.89      77.22      94.64      84.00      77.91      89.77      94.23      90.22      86.31      97.73      97.12      97.73      91.67      89.42      81.73      81.71      93.33      94.23      91.67      86.25      89.42      97.41\n",
      "      8       83.29     88.12     51.63     70.00     43.27     69.51     73.40     74.04     71.34     67.02      92.27      76.92      85.56      88.46      82.14      88.46      86.96      74.11      85.56      78.89      92.86      83.00      78.49      89.77      94.23      90.22      85.12      97.73      96.15      96.21      92.71      89.42      81.73      81.10      92.50      93.27      89.88      86.88      92.79      96.55\n",
      "      9       83.57     88.75     51.09     66.88     47.12     68.90     73.40     74.04     72.56     69.68      93.64      77.88      83.33      91.35      83.93      87.50      88.04      72.32      85.00      80.56      94.64      82.00      76.16      88.64      93.27      89.67      85.71      97.73      96.15      98.48      93.75      91.35      80.77      81.71      93.33      94.23      89.88      86.88      92.31      96.55\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0014 0.0099 0.0551]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8357 \n",
      "Accuracies (test): [0.8875, 0.5109, 0.6688, 0.4712, 0.689, 0.734, 0.7404, 0.7256, 0.6968, 0.9364, 0.7788, 0.8333, 0.9135, 0.8393, 0.875, 0.8804, 0.7232, 0.85, 0.8056, 0.9464, 0.82, 0.7616, 0.8864, 0.9327, 0.8967, 0.8571, 0.9773, 0.9615, 0.9848, 0.9375, 0.9135, 0.8077, 0.8171, 0.9333, 0.9423, 0.8988, 0.8688, 0.9231, 0.9655]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 40\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.28     85.62     51.63     68.75     50.96     68.29     70.21     71.15     73.78     68.62      90.45      77.88      86.11      90.38      80.36      88.46      89.13      74.11      84.44      78.33      96.43      84.00      79.07      89.77      94.23      91.30      82.74      96.21      95.19      97.73      90.62      90.38      80.77      81.10      92.50      93.27      88.10      86.25      90.87      96.55      85.42\n",
      "      1       83.41     85.00     51.09     68.75     50.00     71.95     71.81     75.00     73.17     66.49      93.64      77.88      83.89      91.35      80.36      89.42      85.87      73.21      83.89      78.89      96.43      82.00      77.33      86.36      91.35      89.13      83.33      97.73      95.19      97.73      93.75      90.38      81.73      81.71      93.33      94.23      91.07      86.88      91.83      97.41      85.94\n",
      "      2       83.49     86.25     48.37     68.12     49.04     67.68     71.28     75.00     73.78     70.74      91.82      79.81      86.67      90.38      82.14      88.46      86.96      72.32      86.67      77.22      96.43      83.00      76.74      88.64      93.27      90.76      85.12      96.97      96.15      98.48      92.71      88.46      83.65      79.27      91.67      94.23      88.69      87.50      91.83      94.83      88.54\n",
      "      3       83.67     86.88     50.54     66.88     49.04     70.73     69.68     75.96     76.83     70.21      91.82      78.85      88.89      91.35      82.14      88.46      86.96      74.11      87.22      77.22      92.86      83.00      75.58      88.64      95.19      91.30      84.52      97.73      96.15      98.48      92.71      88.46      81.73      80.49      92.50      94.23      89.29      86.88      90.38      96.55      86.46\n",
      "      4       83.46     85.62     48.91     63.12     50.00     69.51     71.28     73.08     75.00     68.62      92.73      81.73      86.11      90.38      80.36      87.50      86.96      76.79      84.44      79.44      94.64      82.00      77.33      87.50      92.31      90.22      85.71      97.73      98.08      98.48      93.75      90.38      80.77      79.27      92.50      94.23      90.48      88.12      90.87      96.55      85.94\n",
      "      5       83.59     85.62     52.17     67.50     47.12     73.17     73.40     73.08     76.22     69.68      93.64      78.85      87.22      89.42      82.14      87.50      85.87      75.00      85.00      77.78      94.64      82.00      76.16      88.64      93.27      91.30      86.31      97.73      96.15      96.97      92.71      88.46      78.85      82.93      91.67      96.15      89.29      88.12      90.38      96.55      84.90\n",
      "      6       83.75     86.25     52.72     65.00     51.92     68.90     72.87     75.00     73.17     70.74      91.36      79.81      83.89      90.38      82.14      88.46      89.13      75.89      85.56      80.56      94.64      81.00      77.33      87.50      93.27      90.22      85.71      97.73      94.23      97.73      94.79      89.42      80.77      83.54      92.50      94.23      90.48      87.50      90.38      97.41      85.94\n",
      "      7       83.55     85.00     52.17     63.75     47.12     70.12     71.28     75.96     75.00     70.21      94.09      76.92      85.56      90.38      82.14      88.46      86.96      75.00      86.11      78.33      94.64      85.00      79.07      88.64      94.23      89.67      85.12      97.73      96.15      97.73      91.67      88.46      79.81      83.54      93.33      93.27      89.88      87.50      91.83      94.83      85.42\n",
      "      8       83.69     88.12     51.63     66.25     48.08     72.56     72.34     75.00     76.22     67.55      93.64      83.65      86.67      92.31      78.57      88.46      85.87      75.89      85.56      80.00      92.86      84.00      76.74      86.36      94.23      90.22      85.12      97.73      94.23      98.48      94.79      84.62      80.77      83.54      92.50      93.27      89.88      87.50      89.90      96.55      85.94\n",
      "      9       83.26     85.00     51.63     65.62     48.08     69.51     69.68     74.04     74.39     68.62      90.00      77.88      83.33      91.35      80.36      88.46      88.04      74.11      87.78      80.56      94.64      82.00      79.07      88.64      94.23      90.76      86.31      97.73      96.15      97.73      93.75      87.50      79.81      80.49      93.33      92.31      89.88      86.25      89.42      96.55      85.42\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0071 0.0492 0.2221]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8326 \n",
      "Accuracies (test): [0.85, 0.5163, 0.6562, 0.4808, 0.6951, 0.6968, 0.7404, 0.7439, 0.6862, 0.9, 0.7788, 0.8333, 0.9135, 0.8036, 0.8846, 0.8804, 0.7411, 0.8778, 0.8056, 0.9464, 0.82, 0.7907, 0.8864, 0.9423, 0.9076, 0.8631, 0.9773, 0.9615, 0.9773, 0.9375, 0.875, 0.7981, 0.8049, 0.9333, 0.9231, 0.8988, 0.8625, 0.8942, 0.9655, 0.8542]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 41\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc    t41 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.43     84.38     54.35     65.62     49.04     69.51     70.74     77.88     71.95     64.89      93.18      76.92      84.44      88.46      80.36      90.38      88.04      74.11      85.00      80.56      92.86      86.00      75.00      89.77      91.35      90.22      86.31      96.97      95.19      98.48      92.71      90.38      79.81      81.71      93.33      92.31      91.07      86.88      88.46      95.69      86.46      90.00\n",
      "      1       82.90     85.62     57.07     66.25     44.23     68.90     70.74     74.04     70.73     64.36      93.64      75.00      82.78      89.42      82.14      88.46      86.96      72.32      85.00      78.89      91.07      82.00      76.74      86.36      92.31      89.67      87.50      96.21      95.19      96.97      91.67      88.46      79.81      81.71      92.50      92.31      91.67      88.75      91.35      95.69      86.98      87.50\n",
      "      2       83.23     85.62     53.26     65.00     44.23     70.73     67.55     74.04     73.78     71.28      92.27      72.12      85.00      88.46      82.14      88.46      86.96      77.68      87.22      80.00      91.07      83.00      77.33      87.50      93.27      90.22      88.69      97.73      96.15      96.21      93.75      87.50      79.81      81.71      91.67      93.27      91.07      86.25      89.90      95.69      85.94      88.75\n",
      "      3       83.47     85.00     52.72     64.38     45.19     68.29     69.15     75.96     73.78     68.09      92.73      79.81      85.00      90.38      80.36      92.31      88.04      77.68      84.44      77.22      94.64      84.00      75.00      87.50      92.31      91.30      85.71      97.73      97.12      96.21      94.79      89.42      79.81      82.32      92.50      92.31      91.07      86.88      90.38      95.69      84.90      90.00\n",
      "      4       83.39     86.88     54.35     68.12     45.19     70.12     72.34     75.00     73.78     70.21      91.82      78.85      84.44      90.38      82.14      88.46      88.04      75.89      85.00      79.44      94.64      82.00      72.67      86.36      91.35      88.04      86.90      97.73      97.12      96.21      91.67      87.50      80.77      79.27      91.67      95.19      91.07      85.62      90.38      95.69      88.02      88.75\n",
      "      5       83.14     87.50     53.26     63.75     48.08     68.90     69.15     71.15     75.61     69.68      92.27      76.92      83.89      88.46      82.14      87.50      86.96      76.79      82.78      81.11      91.07      83.00      75.00      88.64      92.31      89.67      85.71      97.73      97.12      96.97      93.75      91.35      76.92      81.71      93.33      94.23      90.48      86.25      90.38      94.83      85.94      86.25\n",
      "      6       83.08     86.88     52.17     64.38     46.15     68.90     68.09     75.00     73.17     67.55      94.55      78.85      85.00      89.42      80.36      87.50      89.13      72.32      83.33      78.89      92.86      84.00      75.58      88.64      92.31      89.13      86.31      97.73      97.12      97.73      92.71      89.42      77.88      79.88      94.17      94.23      91.07      86.25      88.46      95.69      84.90      88.75\n",
      "      7       83.20     85.00     52.17     65.00     45.19     72.56     70.21     76.92     75.00     69.68      91.82      77.88      82.78      88.46      82.14      85.58      88.04      76.79      83.33      78.89      92.86      84.00      76.16      86.36      90.38      90.76      86.90      96.97      97.12      96.97      91.67      89.42      76.92      80.49      94.17      94.23      90.48      85.62      90.87      96.55      85.94      88.75\n",
      "      8       83.63     86.88     52.17     65.62     41.35     71.95     71.28     76.92     71.95     69.15      93.64      78.85      85.00      89.42      82.14      87.50      88.04      77.68      86.11      78.89      94.64      83.00      75.58      88.64      93.27      89.67      86.31      96.97      97.12      96.97      94.79      88.46      78.85      81.71      93.33      95.19      91.07      86.88      90.38      95.69      86.98      88.75\n",
      "      9       83.40     87.50     51.63     61.88     50.00     73.17     69.68     71.15     71.95     68.09      91.82      76.92      86.67      89.42      82.14      90.38      86.96      78.57      83.89      80.56      92.86      83.00      76.74      88.64      92.31      89.67      86.31      96.97      97.12      96.97      91.67      88.46      79.81      81.10      92.50      95.19      89.88      86.88      90.38      95.69      87.50      87.50\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0032 0.026  0.1561]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8340 \n",
      "Accuracies (test): [0.875, 0.5163, 0.6188, 0.5, 0.7317, 0.6968, 0.7115, 0.7195, 0.6809, 0.9182, 0.7692, 0.8667, 0.8942, 0.8214, 0.9038, 0.8696, 0.7857, 0.8389, 0.8056, 0.9286, 0.83, 0.7674, 0.8864, 0.9231, 0.8967, 0.8631, 0.9697, 0.9712, 0.9697, 0.9167, 0.8846, 0.7981, 0.811, 0.925, 0.9519, 0.8988, 0.8688, 0.9038, 0.9569, 0.875, 0.875]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 42\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc    t41 acc    t42 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.22     85.62     49.46     62.50     50.00     73.17     69.15     73.08     70.73     67.02      92.73      75.00      84.44      90.38      80.36      88.46      88.04      75.89      84.44      78.89      94.64      83.00      73.26      86.36      94.23      88.59      83.93      97.73      96.15      96.21      93.75      89.42      77.88      80.49      93.33      94.23      91.07      87.50      89.90      95.69      84.38      88.75      95.59\n",
      "      1       83.71     87.50     50.54     65.00     49.04     69.51     69.68     73.08     71.95     68.09      91.36      78.85      87.78      90.38      78.57      88.46      88.04      75.00      85.56      80.00      94.64      83.00      76.16      86.36      95.19      88.04      84.52      96.97      96.15      95.45      95.83      91.35      79.81      83.54      92.50      95.19      90.48      86.25      89.90      96.55      84.90      87.50      97.06\n",
      "      2       83.55     85.62     51.63     63.12     47.12     70.12     70.21     73.08     71.95     70.21      91.36      76.92      87.78      89.42      80.36      88.46      88.04      75.00      84.44      80.00      94.64      82.00      75.58      85.23      94.23      88.59      83.93      97.73      97.12      97.73      91.67      90.38      78.85      82.32      93.33      94.23      90.48      87.50      90.38      96.55      84.90      91.25      95.59\n",
      "      3       83.47     85.62     50.54     64.38     45.19     70.73     70.21     72.12     74.39     66.49      93.64      74.04      85.00      88.46      82.14      88.46      86.96      75.00      86.67      80.00      94.64      84.00      73.84      86.36      93.27      89.13      84.52      97.73      96.15      95.45      93.75      90.38      77.88      80.49      93.33      95.19      90.48      87.50      91.83      97.41      86.46      88.75      97.06\n",
      "      4       83.80     86.25     52.17     65.00     49.04     72.56     70.21     71.15     71.34     65.96      91.36      76.92      85.56      90.38      80.36      90.38      88.04      75.00      83.89      80.00      92.86      84.00      74.42      88.64      95.19      90.22      86.90      97.73      97.12      97.73      93.75      92.31      77.88      81.71      92.50      94.23      90.48      88.75      90.38      96.55      85.42      87.50      97.79\n",
      "      5       83.99     86.88     52.72     65.00     48.08     71.95     70.74     74.04     71.95     68.09      91.36      78.85      86.11      91.35      82.14      90.38      86.96      75.00      86.11      79.44      94.64      83.00      76.74      87.50      93.27      89.13      84.52      97.73      95.19      95.45      93.75      92.31      78.85      82.93      93.33      94.23      89.88      88.75      91.35      95.69      84.38      90.00      97.79\n",
      "      6       83.65     85.00     52.17     65.62     50.96     71.34     68.62     72.12     72.56     63.30      91.82      76.92      85.00      89.42      80.36      90.38      88.04      75.00      85.00      78.33      94.64      83.00      75.00      86.36      94.23      89.67      83.93      97.73      97.12      96.21      93.75      91.35      78.85      81.71      94.17      94.23      89.29      86.25      91.35      97.41      87.50      90.00      97.79\n",
      "      7       83.97     85.62     51.63     65.62     50.00     73.17     71.28     75.00     71.34     67.02      93.64      75.00      85.00      89.42      82.14      89.42      86.96      77.68      85.00      80.56      94.64      82.00      76.16      87.50      92.31      89.67      85.12      97.73      95.19      96.97      91.67      90.38      80.77      82.93      91.67      95.19      90.48      88.75      91.83      96.55      86.98      88.75      97.79\n",
      "      8       83.91     85.62     52.17     64.38     50.96     70.73     69.15     74.04     74.39     66.49      93.18      76.92      85.56      89.42      78.57      88.46      86.96      76.79      85.56      81.11      94.64      83.00      76.74      87.50      92.31      89.67      86.90      97.73      97.12      96.21      93.75      92.31      78.85      79.88      94.17      95.19      91.07      86.88      91.35      96.55      86.46      87.50      97.79\n",
      "      9       84.12     85.62     53.26     66.88     48.08     72.56     71.81     75.00     71.34     68.09      91.36      75.96      85.00      91.35      82.14      87.50      88.04      75.89      85.56      80.56      94.64      84.00      77.91      87.50      91.35      90.76      85.71      97.73      98.08      94.70      95.83      91.35      78.85      82.93      93.33      95.19      91.07      88.12      89.90      95.69      85.94      88.75      97.79\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0019 0.0139 0.0699]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8412 \n",
      "Accuracies (test): [0.8562, 0.5326, 0.6688, 0.4808, 0.7256, 0.7181, 0.75, 0.7134, 0.6809, 0.9136, 0.7596, 0.85, 0.9135, 0.8214, 0.875, 0.8804, 0.7589, 0.8556, 0.8056, 0.9464, 0.84, 0.7791, 0.875, 0.9135, 0.9076, 0.8571, 0.9773, 0.9808, 0.947, 0.9583, 0.9135, 0.7885, 0.8293, 0.9333, 0.9519, 0.9107, 0.8812, 0.899, 0.9569, 0.8594, 0.8875, 0.9779]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 43\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc    t41 acc    t42 acc    t43 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.19     81.25     48.37     63.75     50.96     68.90     64.36     74.04     71.95     68.09      90.91      75.00      86.11      90.38      78.57      85.58      86.96      75.89      84.44      79.44      94.64      85.00      76.74      85.23      93.27      87.50      83.33      97.73      96.15      96.21      90.62      87.50      82.69      83.54      95.00      92.31      90.48      85.00      89.90      94.83      85.94      88.75      98.53      91.18\n",
      "      1       83.31     82.50     52.17     65.62     46.15     70.73     68.09     73.08     71.95     67.55      90.45      72.12      85.00      91.35      78.57      88.46      88.04      74.11      86.11      78.33      96.43      80.00      78.49      85.23      95.19      89.13      84.52      97.73      97.12      94.70      92.71      89.42      79.81      79.88      93.33      92.31      90.48      86.25      90.87      93.97      85.94      88.75      98.53      91.18\n",
      "      2       83.88     85.00     52.72     66.25     49.04     70.12     67.55     74.04     74.39     69.15      91.82      74.04      86.11      90.38      82.14      87.50      88.04      75.89      86.67      79.44      96.43      84.00      77.33      87.50      93.27      88.59      86.31      96.97      94.23      93.94      91.67      91.35      79.81      79.88      94.17      93.27      91.07      87.50      90.38      95.69      85.42      88.75      97.79      91.18\n",
      "      3       83.93     85.00     52.17     66.88     47.12     70.12     70.74     75.00     71.95     66.49      92.73      77.88      85.56      88.46      83.93      87.50      84.78      76.79      85.56      78.89      96.43      82.00      72.09      88.64      93.27      90.76      85.71      97.73      97.12      94.70      95.83      90.38      81.73      78.66      92.50      95.19      89.29      87.50      90.87      94.83      85.42      88.75      97.79      94.12\n",
      "      4       83.48     84.38     50.54     65.62     47.12     68.90     70.21     75.00     71.34     66.49      91.82      72.12      85.00      88.46      82.14      87.50      85.87      75.00      87.22      78.89      91.07      82.00      75.58      86.36      91.35      89.13      87.50      96.97      97.12      96.21      92.71      89.42      79.81      79.27      91.67      93.27      91.07      88.75      90.87      95.69      84.90      91.25      97.06      97.06\n",
      "      5       84.05     86.25     52.72     66.25     50.00     68.29     66.49     75.96     70.12     66.49      93.64      76.92      86.11      89.42      80.36      86.54      88.04      73.21      86.67      80.00      96.43      84.00      77.91      88.64      92.31      91.30      85.12      97.73      97.12      95.45      92.71      90.38      80.77      82.93      94.17      94.23      89.29      86.25      90.38      94.83      85.94      88.75      98.53      95.59\n",
      "      6       83.81     86.25     52.72     68.12     43.27     68.29     67.55     75.96     70.12     68.62      92.27      78.85      85.56      89.42      78.57      88.46      88.04      73.21      84.44      78.89      92.86      86.00      77.91      87.50      93.27      90.22      86.90      97.73      96.15      94.70      91.67      90.38      77.88      81.10      93.33      93.27      89.88      88.12      91.35      93.97      85.94      88.75      99.26      97.06\n",
      "      7       84.20     85.00     51.63     66.25     47.12     71.95     70.21     73.08     71.34     69.15      92.27      76.92      86.11      89.42      80.36      89.42      86.96      75.89      87.22      81.11      96.43      83.00      76.16      87.50      93.27      90.22      88.69      96.97      97.12      96.21      93.75      89.42      78.85      80.49      93.33      94.23      90.48      86.88      89.90      95.69      86.46      88.75      98.53      97.06\n",
      "      8       84.13     85.62     52.17     65.62     49.04     68.90     71.28     75.00     71.95     67.55      91.36      76.92      87.78      89.42      82.14      89.42      86.96      76.79      85.56      80.56      94.64      81.00      76.74      88.64      92.31      90.76      87.50      97.73      97.12      96.21      92.71      89.42      81.73      79.27      94.17      94.23      88.69      86.88      91.35      95.69      86.46      87.50      98.53      94.12\n",
      "      9       84.06     85.62     51.63     64.38     45.19     72.56     68.62     73.08     73.17     68.62      92.73      74.04      87.22      90.38      83.93      88.46      86.96      73.21      85.00      81.11      94.64      83.00      76.74      87.50      92.31      88.04      88.10      96.97      97.12      95.45      94.79      89.42      79.81      80.49      94.17      95.19      91.67      86.25      91.83      94.83      84.90      90.00      98.53      97.06\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0015 0.0142 0.0822]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8406 \n",
      "Accuracies (test): [0.8562, 0.5163, 0.6438, 0.4519, 0.7256, 0.6862, 0.7308, 0.7317, 0.6862, 0.9273, 0.7404, 0.8722, 0.9038, 0.8393, 0.8846, 0.8696, 0.7321, 0.85, 0.8111, 0.9464, 0.83, 0.7674, 0.875, 0.9231, 0.8804, 0.881, 0.9697, 0.9712, 0.9545, 0.9479, 0.8942, 0.7981, 0.8049, 0.9417, 0.9519, 0.9167, 0.8625, 0.9183, 0.9483, 0.849, 0.9, 0.9853, 0.9706]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 44\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc    t41 acc    t42 acc    t43 acc    t44 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       84.07     86.25     49.46     68.12     51.92     68.90     69.15     74.04     71.95     68.62      90.91      75.96      86.11      92.31      80.36      90.38      86.96      76.79      84.44      80.00      92.86      85.00      75.58      85.23      92.31      86.41      85.12      96.21      97.12      95.45      93.75      89.42      80.77      81.10      92.50      93.27      90.48      88.12      91.83      95.69      84.90      91.25      98.53      97.06      86.36\n",
      "      1       84.32     86.88     53.80     64.38     46.15     70.12     68.62     75.00     70.12     68.62      92.73      78.85      88.89      90.38      78.57      90.38      86.96      75.89      86.11      80.56      96.43      84.00      76.16      84.09      91.35      89.67      84.52      97.73      97.12      95.45      93.75      90.38      82.69      81.10      95.00      92.31      89.29      88.75      91.35      96.55      84.90      90.00      97.79      97.06      89.77\n",
      "      2       84.11     85.62     50.00     66.88     45.19     71.95     67.55     76.92     73.17     69.15      91.82      78.85      88.33      90.38      78.57      89.42      85.87      73.21      85.00      81.67      96.43      83.00      76.16      88.64      90.38      89.13      85.71      96.97      97.12      96.21      91.67      91.35      81.73      78.66      93.33      94.23      90.48      88.75      90.87      95.69      85.42      86.25      97.79      95.59      89.77\n",
      "      3       84.10     85.00     53.26     67.50     46.15     70.12     67.55     75.00     70.12     68.62      90.45      77.88      85.56      90.38      82.14      90.38      85.87      75.00      83.89      81.11      92.86      83.00      74.42      89.77      90.38      89.13      87.50      96.97      97.12      94.70      94.79      89.42      80.77      82.32      93.33      95.19      90.48      85.00      92.31      94.83      86.46      90.00      99.26      97.06      87.50\n",
      "      4       84.11     85.00     51.63     67.50     42.31     70.73     68.62     75.00     69.51     67.55      91.36      78.85      86.67      90.38      83.93      88.46      85.87      75.00      85.56      82.78      96.43      82.00      73.84      88.64      91.35      89.67      86.90      96.21      97.12      93.94      93.75      91.35      81.73      79.88      95.83      94.23      89.88      87.50      89.42      94.83      85.94      90.00      97.79      97.06      88.64\n",
      "      5       84.10     83.75     50.00     65.62     47.12     70.73     69.15     76.92     72.56     67.02      91.36      77.88      86.11      90.38      82.14      88.46      86.96      75.89      85.00      81.67      92.86      84.00      74.42      88.64      91.35      89.67      86.90      96.97      97.12      96.21      91.67      92.31      80.77      79.88      94.17      92.31      89.29      87.50      90.38      96.55      85.42      90.00      97.79      97.06      88.64\n",
      "      6       83.64     83.75     49.46     65.00     44.23     68.90     68.62     74.04     69.51     68.09      92.27      73.08      83.89      89.42      82.14      91.35      86.96      75.00      84.44      81.67      94.64      83.00      73.84      86.36      91.35      88.04      86.90      96.97      97.12      95.45      92.71      92.31      80.77      81.10      93.33      94.23      90.48      86.88      89.90      95.69      84.38      90.00      97.06      97.06      88.64\n",
      "      7       83.65     85.62     48.37     64.38     48.08     70.73     67.55     70.19     70.73     67.55      92.27      75.96      87.22      90.38      80.36      89.42      88.04      75.00      85.56      80.56      94.64      80.00      73.84      85.23      91.35      88.59      85.71      97.73      97.12      95.45      91.67      89.42      80.77      82.32      94.17      93.27      90.48      86.25      90.38      95.69      83.85      91.25      97.79      97.06      88.64\n",
      "      8       84.14     84.38     52.72     65.00     48.08     67.07     68.09     72.12     72.56     69.15      94.09      75.96      85.56      91.35      82.14      90.38      88.04      79.46      85.00      79.44      96.43      81.00      78.49      86.36      92.31      88.59      86.31      97.73      97.12      96.21      91.67      91.35      80.77      76.22      94.17      92.31      90.48      88.75      91.35      95.69      83.85      90.00      98.53      97.06      88.64\n",
      "      9       83.75     84.38     51.63     66.88     41.35     70.12     67.55     72.12     72.56     67.55      92.73      76.92      87.22      91.35      83.93      91.35      84.78      74.11      85.56      81.11      92.86      81.00      73.84      85.23      91.35      90.22      86.31      96.97      97.12      94.70      92.71      92.31      80.77      79.27      93.33      93.27      89.88      87.50      90.87      94.83      83.85      88.75      99.26      97.06      88.64\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0015 0.0166 0.186 ]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8375 \n",
      "Accuracies (test): [0.8438, 0.5163, 0.6688, 0.4135, 0.7012, 0.6755, 0.7212, 0.7256, 0.6755, 0.9273, 0.7692, 0.8722, 0.9135, 0.8393, 0.9135, 0.8478, 0.7411, 0.8556, 0.8111, 0.9286, 0.81, 0.7384, 0.8523, 0.9135, 0.9022, 0.8631, 0.9697, 0.9712, 0.947, 0.9271, 0.9231, 0.8077, 0.7927, 0.9333, 0.9327, 0.8988, 0.875, 0.9087, 0.9483, 0.8385, 0.8875, 0.9926, 0.9706, 0.8864]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 45\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc    t41 acc    t42 acc    t43 acc    t44 acc    t45 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.67     86.25     52.17     63.12     41.35     68.29     67.02     74.04     71.34     70.74      91.36      75.00      85.56      90.38      76.79      87.50      88.04      72.32      84.44      80.56      96.43      82.00      71.51      85.23      91.35      88.04      86.90      97.73      96.15      94.70      92.71      90.38      82.69      80.49      94.17      93.27      91.67      85.62      90.87      95.69      84.38      88.75      97.06      94.12      88.64      98.44\n",
      "      1       83.43     83.12     51.63     63.75     44.23     70.12     69.15     74.04     70.12     67.55      91.82      74.04      83.89      90.38      78.57      89.42      84.78      71.43      83.89      77.78      96.43      80.00      72.67      85.23      88.46      89.67      86.90      97.73      96.15      95.45      92.71      89.42      81.73      80.49      95.83      93.27      91.07      85.62      89.42      94.83      81.77      88.75      98.53      95.59      88.64      98.44\n",
      "      2       84.12     83.75     51.09     65.00     45.19     67.68     70.21     75.96     71.34     70.21      91.82      75.00      86.11      90.38      76.79      88.46      86.96      73.21      81.67      81.67      94.64      82.00      75.58      85.23      92.31      89.67      86.90      97.73      97.12      96.21      92.71      93.27      81.73      79.27      95.83      94.23      91.07      86.88      90.38      93.97      85.42      88.75      99.26      97.06      88.64      96.88\n",
      "      3       84.16     84.38     54.35     65.62     43.27     71.95     67.02     75.00     70.73     67.02      92.27      71.15      87.78      90.38      83.93      92.31      85.87      72.32      85.56      82.22      94.64      80.00      73.26      88.64      89.42      90.76      86.31      97.73      95.19      95.45      92.71      92.31      82.69      78.66      95.83      94.23      91.67      85.62      89.90      94.83      83.85      88.75      98.53      97.06      87.50      98.44\n",
      "      4       83.57     85.00     50.54     66.25     46.15     68.29     66.49     74.04     71.34     68.62      90.91      72.12      86.11      90.38      82.14      89.42      85.87      72.32      82.22      80.00      94.64      81.00      73.26      86.36      89.42      88.59      85.71      97.73      97.12      95.45      90.62      90.38      80.77      78.66      95.00      94.23      89.29      85.62      89.90      93.97      84.38      88.75      97.79      97.06      89.77      96.88\n",
      "      5       83.91     85.62     51.09     66.88     42.31     70.73     66.49     75.00     71.34     68.09      91.36      72.12      86.67      88.46      80.36      90.38      86.96      74.11      85.00      81.11      96.43      81.00      76.74      87.50      89.42      90.22      86.31      97.73      96.15      96.21      92.71      90.38      81.73      78.66      95.83      93.27      90.48      85.00      89.42      93.97      86.98      88.75      97.06      94.12      87.50      98.44\n",
      "      6       84.09     84.38     53.26     65.62     46.15     69.51     65.96     75.96     72.56     68.09      91.36      73.08      88.33      91.35      80.36      88.46      89.13      74.11      85.56      81.11      94.64      81.00      73.84      86.36      91.35      89.13      86.31      97.73      97.12      95.45      92.71      89.42      80.77      79.88      94.17      94.23      90.48      86.88      89.90      93.97      85.42      88.75      97.79      94.12      89.77      98.44\n",
      "      7       83.90     83.75     51.09     66.25     45.19     69.51     64.89     75.96     70.73     67.55      91.82      75.96      86.11      90.38      80.36      89.42      84.78      76.79      86.67      80.56      96.43      81.00      74.42      86.36      90.38      89.13      86.90      97.73      97.12      95.45      92.71      89.42      81.73      79.88      95.83      94.23      90.48      85.62      91.35      93.97      83.33      87.50      97.06      94.12      88.64      96.88\n",
      "      8       84.01     85.00     50.54     64.38     45.19     68.29     67.02     75.00     71.34     68.62      90.91      76.92      87.22      90.38      83.93      89.42      83.70      75.89      84.44      82.22      96.43      83.00      75.58      84.09      90.38      90.22      87.50      97.73      96.15      95.45      91.67      90.38      81.73      78.05      95.00      94.23      91.07      86.25      89.42      94.83      83.33      88.75      98.53      94.12      90.91      95.31\n",
      "      9       84.11     85.00     51.09     66.25     43.27     68.90     68.62     75.96     72.56     69.68      91.36      70.19      85.56      90.38      83.93      91.35      84.78      77.68      84.44      79.44      96.43      81.00      75.00      87.50      89.42      89.67      88.10      97.73      96.15      96.21      92.71      90.38      81.73      79.27      95.00      95.19      91.07      85.62      91.35      93.97      84.90      87.50      97.79      95.59      89.77      95.31\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0024 0.014  0.0697]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8411 \n",
      "Accuracies (test): [0.85, 0.5109, 0.6625, 0.4327, 0.689, 0.6862, 0.7596, 0.7256, 0.6968, 0.9136, 0.7019, 0.8556, 0.9038, 0.8393, 0.9135, 0.8478, 0.7768, 0.8444, 0.7944, 0.9643, 0.81, 0.75, 0.875, 0.8942, 0.8967, 0.881, 0.9773, 0.9615, 0.9621, 0.9271, 0.9038, 0.8173, 0.7927, 0.95, 0.9519, 0.9107, 0.8562, 0.9135, 0.9397, 0.849, 0.875, 0.9779, 0.9559, 0.8977, 0.9531]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 46\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc    t41 acc    t42 acc    t43 acc    t44 acc    t45 acc    t46 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.64     83.75     48.37     65.00     40.38     73.78     64.89     75.96     71.34     69.68      90.00      70.19      83.89      89.42      82.14      89.42      84.78      75.00      82.22      75.56      96.43      81.00      71.51      86.36      91.35      91.85      84.52      96.21      97.12      96.21      91.67      88.46      80.77      78.05      95.00      95.19      88.69      88.75      90.38      94.83      83.85      88.75      97.79      95.59      88.64      96.88      95.65\n",
      "      1       84.00     81.88     53.26     65.62     43.27     73.17     65.96     77.88     72.56     67.55      89.55      74.04      87.22      89.42      82.14      89.42      85.87      77.68      81.67      77.78      96.43      80.00      72.09      86.36      89.42      88.59      85.71      97.73      97.12      94.70      92.71      90.38      83.65      76.22      95.00      94.23      89.29      85.00      90.38      94.83      83.85      88.75      97.06      97.06      88.64      98.44      94.57\n",
      "      2       83.49     85.00     53.80     66.25     38.46     70.73     63.83     73.08     71.34     71.28      91.36      68.27      86.67      89.42      78.57      88.46      82.61      75.89      82.22      77.22      94.64      82.00      73.84      87.50      87.50      89.67      85.71      97.73      94.23      95.45      92.71      89.42      83.65      76.83      92.50      95.19      89.29      87.50      90.87      96.55      82.81      88.75      97.06      97.06      88.64      95.31      93.48\n",
      "      3       83.90     83.75     52.72     66.25     35.58     67.68     67.55     73.08     73.17     68.62      90.45      74.04      86.11      90.38      80.36      90.38      85.87      75.00      82.78      77.78      96.43      81.00      73.26      86.36      91.35      89.13      86.90      97.73      96.15      95.45      92.71      92.31      84.62      77.44      96.67      92.31      89.88      84.38      91.35      96.55      84.90      87.50      97.79      95.59      89.77      96.88      93.48\n",
      "      4       83.89     83.12     50.54     66.25     37.50     71.34     65.43     77.88     74.39     69.15      89.55      72.12      86.11      90.38      82.14      90.38      83.70      73.21      82.78      78.89      96.43      83.00      74.42      86.36      88.46      89.67      86.31      97.73      95.19      96.21      92.71      90.38      83.65      75.61      95.83      94.23      89.88      85.62      92.79      96.55      83.85      90.00      96.32      94.12      89.77      95.31      93.48\n",
      "      5       83.90     81.88     52.72     66.25     42.31     73.17     68.09     73.08     71.34     68.62      88.64      73.08      86.67      90.38      82.14      90.38      83.70      76.79      82.22      79.44      96.43      81.00      72.67      86.36      89.42      89.67      85.12      96.97      96.15      95.45      90.62      90.38      83.65      78.05      95.00      95.19      89.29      86.25      90.87      95.69      85.42      87.50      97.06      95.59      89.77      95.31      93.48\n",
      "      6       84.08     82.50     53.80     65.00     40.38     70.12     65.96     75.00     73.17     66.49      90.45      75.00      83.89      91.35      80.36      89.42      83.70      75.89      82.78      78.33      96.43      83.00      75.00      86.36      91.35      89.13      85.12      97.73      97.12      95.45      92.71      91.35      82.69      80.49      95.00      95.19      89.88      86.25      89.90      96.55      84.90      88.75      97.79      95.59      89.77      96.88      93.48\n",
      "      7       84.23     81.88     52.17     66.25     40.38     68.90     65.96     76.92     73.78     70.21      90.91      71.15      86.11      91.35      83.93      89.42      83.70      77.68      83.33      78.89      96.43      82.00      75.00      86.36      92.31      89.67      86.31      97.73      96.15      96.21      92.71      89.42      82.69      80.49      95.00      94.23      90.48      85.62      90.87      95.69      84.90      87.50      97.79      95.59      89.77      98.44      92.39\n",
      "      8       84.13     83.12     51.09     63.75     41.35     70.73     64.36     75.96     73.17     68.09      90.91      74.04      86.67      91.35      82.14      90.38      84.78      76.79      83.89      78.33      96.43      83.00      76.16      87.50      90.38      90.22      84.52      97.73      96.15      96.21      91.67      90.38      82.69      78.66      95.83      96.15      88.10      85.62      89.90      96.55      84.90      87.50      98.53      94.12      89.77      96.88      93.48\n",
      "      9       84.10     83.75     52.72     63.75     43.27     71.34     67.02     75.00     72.56     66.49      91.36      72.12      87.78      91.35      82.14      87.50      83.70      76.79      82.78      80.00      96.43      81.00      72.67      86.36      90.38      91.30      85.71      97.73      97.12      95.45      92.71      90.38      84.62      79.88      94.17      95.19      88.69      85.00      90.38      94.83      85.94      90.00      97.06      95.59      89.77      95.31      93.48\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0022 0.0223 0.1329]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8410 \n",
      "Accuracies (test): [0.8375, 0.5272, 0.6375, 0.4327, 0.7134, 0.6702, 0.75, 0.7256, 0.6649, 0.9136, 0.7212, 0.8778, 0.9135, 0.8214, 0.875, 0.837, 0.7679, 0.8278, 0.8, 0.9643, 0.81, 0.7267, 0.8636, 0.9038, 0.913, 0.8571, 0.9773, 0.9712, 0.9545, 0.9271, 0.9038, 0.8462, 0.7988, 0.9417, 0.9519, 0.8869, 0.85, 0.9038, 0.9483, 0.8594, 0.9, 0.9706, 0.9559, 0.8977, 0.9531, 0.9348]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 47\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc    t41 acc    t42 acc    t43 acc    t44 acc    t45 acc    t46 acc    t47 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       84.15     82.50     52.17     65.62     46.15     64.63     65.96     75.00     73.17     67.55      89.55      73.08      85.00      91.35      82.14      89.42      85.87      75.89      81.11      79.44      94.64      83.00      75.00      85.23      90.38      90.22      85.71      97.73      95.19      96.21      88.54      94.23      79.81      79.27      95.00      94.23      91.07      85.00      91.35      97.41      84.38      91.25      97.06      97.06      89.77      96.88      92.39      91.25\n",
      "      1       84.47     82.50     51.63     63.12     45.19     67.68     70.21     74.04     73.78     69.68      90.91      75.00      85.56      92.31      80.36      90.38      85.87      76.79      82.78      80.00      92.86      84.00      73.84      86.36      91.35      89.67      85.12      97.73      97.12      96.21      91.67      94.23      80.77      79.88      94.17      92.31      89.88      85.62      90.87      97.41      85.94      90.00      96.32      95.59      89.77      95.31      93.48      95.00\n",
      "      2       84.11     83.12     54.35     66.88     40.38     67.68     67.55     75.00     72.56     64.36      90.91      73.08      84.44      92.31      80.36      88.46      85.87      76.79      81.11      79.44      94.64      82.00      73.84      84.09      90.38      88.59      88.69      97.73      96.15      96.21      90.62      90.38      84.62      78.05      95.83      92.31      89.29      86.25      91.35      97.41      86.46      88.75      96.32      95.59      90.91      95.31      93.48      93.12\n",
      "      3       84.12     81.25     53.80     63.75     44.23     68.29     65.43     74.04     72.56     68.09      90.45      74.04      85.00      90.38      82.14      88.46      85.87      75.00      82.22      80.56      92.86      84.00      75.00      84.09      90.38      89.67      86.31      97.73      95.19      96.97      92.71      92.31      79.81      79.88      95.00      92.31      88.69      88.12      90.87      96.55      85.42      88.75      97.06      94.12      92.05      95.31      92.39      94.38\n",
      "      4       84.00     83.12     51.63     66.25     44.23     67.07     67.02     74.04     72.56     70.74      91.82      76.92      86.11      91.35      78.57      89.42      86.96      74.11      80.00      80.00      94.64      80.00      73.84      85.23      89.42      86.96      86.31      96.97      95.19      96.21      91.67      91.35      80.77      79.27      94.17      91.35      89.29      86.88      91.35      95.69      84.90      90.00      96.32      94.12      88.64      96.88      93.48      95.00\n",
      "      5       83.96     81.88     50.54     63.12     45.19     69.51     65.43     73.08     71.34     67.55      91.82      73.08      88.33      90.38      75.00      88.46      84.78      77.68      83.33      77.78      96.43      79.00      74.42      86.36      91.35      89.67      85.71      97.73      96.15      96.97      92.71      92.31      81.73      78.66      95.83      92.31      89.88      87.50      90.38      95.69      85.94      87.50      96.32      95.59      87.50      96.88      92.39      95.00\n",
      "      6       84.53     81.88     52.72     63.12     43.27     69.51     66.49     75.96     71.95     69.15      91.82      75.96      86.11      90.38      82.14      88.46      84.78      77.68      83.33      80.00      96.43      81.00      73.26      86.36      89.42      90.76      85.12      97.73      97.12      97.73      91.67      91.35      83.65      81.10      95.83      92.31      89.29      86.88      91.35      96.55      85.42      87.50      97.06      94.12      92.05      98.44      93.48      95.00\n",
      "      7       84.34     81.25     52.72     65.00     40.38     68.90     67.02     76.92     70.73     70.74      91.82      73.08      87.22      92.31      80.36      88.46      84.78      75.89      83.33      80.00      94.64      82.00      75.58      85.23      89.42      90.76      85.71      97.73      94.23      96.21      92.71      90.38      82.69      80.49      95.83      92.31      90.48      86.88      91.83      95.69      87.50      90.00      95.59      95.59      89.77      95.31      93.48      95.00\n",
      "      8       84.40     81.88     53.80     63.75     43.27     71.34     67.55     75.00     69.51     68.09      90.91      75.00      85.56      92.31      83.93      86.54      83.70      78.57      82.22      80.00      94.64      81.00      74.42      87.50      88.46      90.76      88.10      97.73      95.19      96.21      92.71      90.38      82.69      81.10      95.83      93.27      91.07      86.25      91.35      96.55      85.94      88.75      97.06      95.59      90.91      95.31      90.22      95.00\n",
      "      9       84.13     81.88     52.72     63.12     42.31     66.46     67.55     74.04     69.51     69.15      90.91      75.00      86.11      92.31      83.93      88.46      83.70      75.00      83.33      78.33      96.43      80.00      73.84      87.50      91.35      89.13      85.71      97.73      97.12      96.97      91.67      91.35      80.77      78.05      95.83      93.27      90.48      85.62      92.31      95.69      84.90      88.75      96.32      97.06      89.77      95.31      93.48      93.75\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0033 0.0198 0.0975]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8413 \n",
      "Accuracies (test): [0.8188, 0.5272, 0.6312, 0.4231, 0.6646, 0.6755, 0.7404, 0.6951, 0.6915, 0.9091, 0.75, 0.8611, 0.9231, 0.8393, 0.8846, 0.837, 0.75, 0.8333, 0.7833, 0.9643, 0.8, 0.7384, 0.875, 0.9135, 0.8913, 0.8571, 0.9773, 0.9712, 0.9697, 0.9167, 0.9135, 0.8077, 0.7805, 0.9583, 0.9327, 0.9048, 0.8562, 0.9231, 0.9569, 0.849, 0.8875, 0.9632, 0.9706, 0.8977, 0.9531, 0.9348, 0.9375]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 48\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc    t41 acc    t42 acc    t43 acc    t44 acc    t45 acc    t46 acc    t47 acc    t48 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       83.50     80.00     48.91     66.88     43.27     65.24     64.36     74.04     68.29     65.43      92.73      76.92      84.44      90.38      78.57      85.58      86.96      72.32      84.44      77.78      94.64      78.00      73.84      87.50      86.54      85.87      84.52      97.73      97.12      94.70      92.71      90.38      83.65      79.27      94.17      93.27      89.88      85.62      89.90      94.83      84.38      88.75      97.79      94.12      89.77      96.88      93.48      93.75      88.46\n",
      "      1       83.88     82.50     50.54     65.00     43.27     69.51     67.02     75.96     69.51     71.28      93.18      73.08      85.56      89.42      78.57      85.58      88.04      73.21      82.78      77.22      94.64      80.00      73.84      85.23      88.46      88.04      83.93      97.73      96.15      96.21      92.71      89.42      81.73      78.66      95.83      92.31      89.88      84.38      90.87      94.83      85.94      87.50      99.26      94.12      89.77      96.88      92.39      95.00      89.42\n",
      "      2       83.54     79.38     48.37     66.25     44.23     68.29     63.83     74.04     71.95     68.62      91.82      73.08      83.89      89.42      78.57      86.54      84.78      75.89      83.33      77.78      94.64      77.00      72.67      85.23      88.46      86.96      83.93      97.73      95.19      96.97      90.62      90.38      80.77      79.88      97.50      91.35      88.69      85.62      90.38      94.83      84.38      90.00      98.53      95.59      89.77      96.88      93.48      93.12      89.42\n",
      "      3       83.85     82.50     50.00     65.62     40.38     69.51     67.02     75.96     72.56     65.43      92.73      73.08      86.11      86.54      80.36      90.38      85.87      72.32      83.33      77.22      96.43      80.00      73.84      86.36      87.50      90.22      84.52      97.73      95.19      96.97      91.67      92.31      80.77      82.32      96.67      92.31      88.10      85.62      90.38      95.69      81.77      88.75      97.06      94.12      89.77      96.88      92.39      93.12      89.42\n",
      "      4       83.94     82.50     49.46     65.62     44.23     70.12     66.49     75.96     70.73     66.49      92.73      75.00      85.00      89.42      76.79      87.50      83.70      71.43      83.89      81.11      96.43      80.00      73.26      84.09      91.35      86.96      83.93      97.73      97.12      96.97      89.58      90.38      82.69      79.88      97.50      91.35      90.48      85.62      90.38      95.69      84.38      90.00      97.79      94.12      90.91      95.31      93.48      94.38      89.42\n",
      "      5       83.93     82.50     51.63     63.75     40.38     70.73     65.96     72.12     69.51     64.89      91.36      75.96      85.56      89.42      78.57      89.42      82.61      77.68      81.67      77.22      94.64      82.00      74.42      86.36      91.35      86.96      84.52      97.73      97.12      98.48      91.67      90.38      82.69      77.44      95.83      91.35      90.48      85.62      89.42      95.69      84.38      90.00      97.79      95.59      90.91      96.88      94.57      93.12      90.38\n",
      "      6       83.66     80.62     49.46     63.75     45.19     68.90     63.83     74.04     70.12     69.15      90.00      75.00      84.44      88.46      76.79      85.58      85.87      75.00      85.00      75.56      96.43      79.00      72.67      85.23      90.38      87.50      83.93      97.73      96.15      98.48      90.62      91.35      80.77      79.88      95.83      94.23      91.07      86.88      88.94      95.69      84.38      88.75      97.06      94.12      88.64      96.88      92.39      94.38      89.42\n",
      "      7       83.86     82.50     49.46     66.88     43.27     69.51     67.02     77.88     69.51     67.55      90.91      75.96      85.56      89.42      82.14      86.54      84.78      72.32      82.78      77.78      94.64      80.00      73.84      86.36      90.38      88.59      82.14      97.73      96.15      95.45      92.71      91.35      78.85      78.66      97.50      92.31      89.29      85.62      90.38      95.69      83.85      88.75      97.06      94.12      89.77      96.88      92.39      93.75      89.42\n",
      "      8       84.17     82.50     50.00     64.38     42.31     68.29     65.96     75.00     70.73     64.36      92.73      76.92      87.22      91.35      80.36      88.46      89.13      72.32      82.78      78.33      96.43      82.00      75.58      85.23      89.42      89.67      83.93      97.73      97.12      98.48      91.67      90.38      79.81      80.49      95.83      93.27      88.69      86.88      90.87      94.83      84.38      88.75      97.79      94.12      89.77      95.31      94.57      94.38      89.42\n",
      "      9       83.75     82.50     52.17     66.88     37.50     71.95     67.02     74.04     69.51     64.89      91.82      74.04      86.11      91.35      76.79      86.54      84.78      75.89      82.78      80.56      96.43      80.00      73.26      87.50      90.38      89.67      83.33      97.73      94.23      95.45      91.67      89.42      79.81      78.05      95.00      92.31      90.48      85.00      90.38      94.83      84.90      88.75      97.79      94.12      88.64      96.88      93.48      93.12      90.38\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0065 0.0427 0.2058]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8375 \n",
      "Accuracies (test): [0.825, 0.5217, 0.6688, 0.375, 0.7195, 0.6702, 0.7404, 0.6951, 0.6489, 0.9182, 0.7404, 0.8611, 0.9135, 0.7679, 0.8654, 0.8478, 0.7589, 0.8278, 0.8056, 0.9643, 0.8, 0.7326, 0.875, 0.9038, 0.8967, 0.8333, 0.9773, 0.9423, 0.9545, 0.9167, 0.8942, 0.7981, 0.7805, 0.95, 0.9231, 0.9048, 0.85, 0.9038, 0.9483, 0.849, 0.8875, 0.9779, 0.9412, 0.8864, 0.9688, 0.9348, 0.9312, 0.9038]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 49\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc    t41 acc    t42 acc    t43 acc    t44 acc    t45 acc    t46 acc    t47 acc    t48 acc    t49 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       82.65     83.12     49.46     63.75     40.38     65.85     65.96     75.00     68.29     62.23      91.36      75.96      86.67      92.31      78.57      86.54      84.78      72.32      82.22      73.89      92.86      81.00      68.02      86.36      88.46      89.13      83.93      96.21      91.35      95.45      90.62      90.38      84.62      75.00      96.67      92.31      87.50      83.12      89.42      93.97      79.17      88.75      94.85      94.12      89.77      96.88      95.65      94.38      89.42      71.88\n",
      "      1       82.88     82.50     45.11     68.12     42.31     67.68     64.36     74.04     65.24     62.23      91.36      79.81      85.00      89.42      82.14      85.58      86.96      73.21      80.56      75.56      92.86      81.00      70.93      86.36      89.42      86.96      85.71      96.21      95.19      96.21      90.62      89.42      78.85      79.27      95.00      92.31      88.69      85.00      88.94      93.97      81.25      88.75      96.32      94.12      88.64      96.88      94.57      91.25      89.42      76.04\n",
      "      2       82.93     82.50     48.37     65.62     43.27     68.90     64.89     73.08     67.68     62.23      91.82      75.96      86.67      90.38      78.57      88.46      88.04      71.43      80.56      75.56      92.86      79.00      71.51      86.36      90.38      86.41      85.12      97.73      92.31      94.70      89.58      87.50      78.85      78.05      95.83      92.31      87.50      85.62      88.46      94.83      81.77      88.75      97.79      94.12      88.64      96.88      94.57      93.75      90.38      78.12\n",
      "      3       82.43     83.75     45.11     65.00     37.50     69.51     63.30     71.15     68.90     64.89      90.91      73.08      84.44      89.42      78.57      86.54      88.04      74.11      78.89      75.56      96.43      79.00      70.35      85.23      86.54      86.41      84.52      96.21      93.27      95.45      92.71      86.54      82.69      75.61      95.00      90.38      88.69      85.62      87.50      93.97      80.21      87.50      97.79      94.12      88.64      96.88      93.48      93.12      90.38      76.04\n",
      "      4       83.26     83.12     50.00     66.88     40.38     69.51     63.30     75.00     68.29     60.64      90.91      78.85      87.22      88.46      80.36      88.46      86.96      69.64      81.11      77.22      94.64      80.00      72.67      86.36      92.31      87.50      85.12      96.21      93.27      94.70      93.75      91.35      82.69      75.00      95.83      91.35      88.69      84.38      88.46      94.83      81.77      88.75      98.53      94.12      89.77      96.88      94.57      92.50      89.42      78.12\n",
      "      5       83.11     83.75     47.28     64.38     44.23     68.29     63.83     72.12     67.07     67.02      91.82      75.96      83.33      90.38      78.57      86.54      86.96      72.32      83.89      77.22      96.43      80.00      69.77      84.09      90.38      86.96      86.90      96.97      91.35      96.97      92.71      87.50      80.77      76.83      96.67      92.31      88.10      85.62      89.90      94.83      82.81      87.50      96.32      94.12      89.77      96.88      94.57      93.75      90.38      76.04\n",
      "      6       82.99     80.00     48.37     63.12     39.42     70.12     64.89     75.00     68.90     66.49      89.09      74.04      85.56      87.50      80.36      88.46      86.96      72.32      82.22      76.11      96.43      80.00      70.35      87.50      86.54      86.41      85.12      96.97      94.23      96.97      92.71      88.46      82.69      74.39      95.83      91.35      88.69      85.00      89.42      94.83      82.81      87.50      97.06      94.12      89.77      98.44      94.57      93.12      90.38      76.04\n",
      "      7       83.13     85.62     48.37     64.38     40.38     73.78     66.49     70.19     67.68     63.83      90.91      76.92      85.56      90.38      80.36      87.50      88.04      73.21      81.11      77.22      96.43      78.00      70.35      86.36      89.42      87.50      82.74      96.97      92.31      96.97      94.79      88.46      80.77      74.39      95.00      91.35      88.10      85.00      88.94      94.83      81.77      88.75      97.79      94.12      87.50      95.31      94.57      92.50      91.35      79.17\n",
      "      8       82.93     86.25     45.65     64.38     37.50     71.95     64.89     73.08     66.46     63.83      91.36      72.12      86.11      89.42      80.36      90.38      88.04      71.43      81.67      76.11      94.64      80.00      71.51      86.36      92.31      86.96      84.52      96.97      94.23      95.45      94.79      85.58      77.88      75.61      95.00      91.35      89.29      85.62      89.42      94.83      84.38      87.50      97.06      94.12      88.64      95.31      94.57      92.50      90.38      76.04\n",
      "      9       83.02     83.75     47.28     65.62     43.27     68.29     67.02     70.19     66.46     64.89      90.00      76.92      85.00      88.46      80.36      85.58      89.13      72.32      81.11      76.67      96.43      77.00      72.09      86.36      88.46      88.59      85.12      96.97      94.23      96.97      92.71      88.46      80.77      75.00      95.83      91.35      91.07      86.25      89.42      94.83      82.81      87.50      95.59      94.12      87.50      95.31      94.57      93.75      87.50      79.17\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0153 0.0956 0.3228]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8302 \n",
      "Accuracies (test): [0.8375, 0.4728, 0.6562, 0.4327, 0.6829, 0.6702, 0.7019, 0.6646, 0.6489, 0.9, 0.7692, 0.85, 0.8846, 0.8036, 0.8558, 0.8913, 0.7232, 0.8111, 0.7667, 0.9643, 0.77, 0.7209, 0.8636, 0.8846, 0.8859, 0.8512, 0.9697, 0.9423, 0.9697, 0.9271, 0.8846, 0.8077, 0.75, 0.9583, 0.9135, 0.9107, 0.8625, 0.8942, 0.9483, 0.8281, 0.875, 0.9559, 0.9412, 0.875, 0.9531, 0.9457, 0.9375, 0.875, 0.7917]\n",
      "---\n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "init_logvar_lin_minval: -10.0\n",
      "init_logvar_lin_maxval: -8.0\n",
      "init_logvar_conv_minval: -10.0\n",
      "init_logvar_conv_maxval: -8.0\n",
      "\n",
      "Learning task 50\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc    t7 acc    t8 acc    t9 acc    t10 acc    t11 acc    t12 acc    t13 acc    t14 acc    t15 acc    t16 acc    t17 acc    t18 acc    t19 acc    t20 acc    t21 acc    t22 acc    t23 acc    t24 acc    t25 acc    t26 acc    t27 acc    t28 acc    t29 acc    t30 acc    t31 acc    t32 acc    t33 acc    t34 acc    t35 acc    t36 acc    t37 acc    t38 acc    t39 acc    t40 acc    t41 acc    t42 acc    t43 acc    t44 acc    t45 acc    t46 acc    t47 acc    t48 acc    t49 acc    t50 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------  --------  --------  --------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "      0       82.54     83.75     50.54     66.25     40.38     68.90     68.62     75.00     62.80     62.77      87.27      71.15      83.89      90.38      76.79      88.46      85.87      66.96      77.22      75.56      96.43      77.00      71.51      79.55      88.46      88.59      82.14      95.45      94.23      95.45      94.79      88.46      78.85      77.44      95.83      92.31      88.10      86.88      87.02      93.10      82.29      88.75      95.59      92.65      86.36      96.88      94.57      93.12      89.42      77.08      96.25\n",
      "      1       82.93     84.38     52.72     64.38     40.38     70.12     66.49     73.08     65.24     62.23      90.45      75.00      86.67      87.50      82.14      85.58      84.78      70.54      81.11      72.78      96.43      79.00      73.26      84.09      89.42      88.59      83.33      96.97      94.23      95.45      94.79      88.46      79.81      75.61      95.00      91.35      86.31      84.38      87.02      93.97      81.77      88.75      95.59      94.12      86.36      96.88      93.48      92.50      89.42      78.12      96.25\n",
      "      2       83.33     83.12     49.46     66.88     42.31     68.29     68.62     73.08     67.68     62.77      89.09      75.00      85.56      88.46      82.14      89.42      86.96      71.43      82.22      75.56      96.43      79.00      72.67      86.36      91.35      87.50      84.52      94.70      92.31      94.70      94.79      86.54      81.73      75.00      94.17      92.31      88.69      84.38      88.94      95.69      82.81      88.75      94.85      94.12      87.50      96.88      94.57      93.12      89.42      77.08      97.50\n",
      "      3       83.46     85.00     49.46     64.38     44.23     69.51     66.49     74.04     69.51     64.36      89.55      76.92      85.00      88.46      80.36      86.54      83.70      72.32      82.22      75.00      96.43      78.00      73.84      86.36      91.35      90.76      82.74      96.97      93.27      96.97      93.75      89.42      82.69      75.61      95.83      92.31      89.88      85.00      88.46      94.83      81.77      88.75      94.85      94.12      87.50      96.88      93.48      91.25      89.42      76.04      97.50\n",
      "      4       83.29     82.50     48.91     62.50     44.23     69.51     67.55     75.00     67.07     63.83      90.45      73.08      86.11      90.38      83.93      84.62      86.96      71.43      81.11      75.56      96.43      76.00      73.26      86.36      89.42      88.04      84.52      96.21      93.27      96.97      92.71      89.42      81.73      75.61      95.83      91.35      89.29      83.75      88.46      93.97      82.81      90.00      94.12      94.12      89.77      96.88      94.57      93.12      88.46      77.08      96.25\n",
      "      5       83.42     86.25     50.54     65.62     40.38     69.51     65.96     73.08     67.07     63.83      90.45      75.00      86.67      86.54      80.36      85.58      84.78      72.32      82.78      76.67      96.43      79.00      75.58      86.36      89.42      89.67      82.74      96.97      93.27      96.21      94.79      88.46      84.62      78.05      95.00      90.38      87.50      88.12      88.46      93.10      81.77      87.50      95.59      94.12      88.64      96.88      93.48      92.50      88.46      78.12      96.25\n",
      "      6       83.20     85.62     51.09     66.88     39.42     69.51     67.02     72.12     66.46     63.30      88.18      75.00      83.33      89.42      78.57      87.50      85.87      72.32      82.78      74.44      96.43      80.00      71.51      86.36      87.50      88.59      82.74      96.21      94.23      95.45      94.79      87.50      82.69      77.44      95.00      91.35      90.48      84.38      88.46      93.10      82.29      90.00      95.59      94.12      89.77      96.88      93.48      91.88      89.42      77.08      96.25\n",
      "      7       83.26     83.75     47.28     63.12     44.23     71.34     66.49     73.08     67.68     65.96      89.09      75.96      86.11      89.42      78.57      85.58      85.87      69.64      82.22      73.89      96.43      80.00      72.09      86.36      88.46      87.50      83.33      96.97      92.31      96.97      94.79      88.46      79.81      78.05      93.33      91.35      89.29      83.12      89.90      93.97      81.77      88.75      95.59      94.12      90.91      96.88      94.57      93.75      89.42      78.12      97.50\n",
      "      8       83.31     85.00     50.00     65.62     41.35     70.12     67.02     71.15     67.07     64.89      89.55      77.88      82.78      89.42      78.57      85.58      84.78      70.54      81.67      74.44      96.43      80.00      72.67      86.36      90.38      88.04      82.74      96.21      92.31      96.97      94.79      88.46      81.73      76.83      95.83      90.38      90.48      85.00      89.90      93.10      82.81      88.75      95.59      94.12      89.77      96.88      94.57      93.12      88.46      78.12      97.50\n",
      "      9       83.60     87.50     49.46     65.62     41.35     67.07     67.02     74.04     68.29     65.96      91.36      76.92      85.00      87.50      80.36      86.54      84.78      75.00      81.11      77.22      94.64      78.00      73.26      86.36      90.38      88.04      82.74      96.21      95.19      95.45      93.75      89.42      82.69      78.05      95.00      90.38      91.07      84.38      88.94      93.97      82.81      87.50      96.32      94.12      90.91      96.88      94.57      93.12      89.42      79.17      95.00\n",
      "Adding inducing inputs to the coreset based on predictive entropy\n",
      "Options: {'mode': 'soft_lowest', 'offset': 0.0, 'n_mixed': 1}\n",
      "\n",
      "Entropy quartiles: [0.0028 0.0172 0.0897]\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8360 \n",
      "Accuracies (test): [0.875, 0.4946, 0.6562, 0.4135, 0.6707, 0.6702, 0.7404, 0.6829, 0.6596, 0.9136, 0.7692, 0.85, 0.875, 0.8036, 0.8654, 0.8478, 0.75, 0.8111, 0.7722, 0.9464, 0.78, 0.7326, 0.8636, 0.9038, 0.8804, 0.8274, 0.9621, 0.9519, 0.9545, 0.9375, 0.8942, 0.8269, 0.7805, 0.95, 0.9038, 0.9107, 0.8438, 0.8894, 0.9397, 0.8281, 0.875, 0.9632, 0.9412, 0.9091, 0.9688, 0.9457, 0.9312, 0.8942, 0.7917, 0.95]\n",
      "---\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n",
      "\n",
      "0.835968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fsvi_omniglot.pkl\", task_sequence)\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsvi",
   "language": "python",
   "name": "fsvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}